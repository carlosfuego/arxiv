[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v1",
                "updated": "2025-09-03T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v1",
                "updated": "2025-09-01T19:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v1",
                "updated": "2025-09-01T03:16:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.03518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03518v1",
                "updated": "2025-09-03T17:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    59,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    59,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Can LLMs Lie? Investigation beyond Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Lie? Investigation beyond Hallucination"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\na variety of tasks, but their increasing autonomy in real-world applications\nraises concerns about their trustworthiness. While hallucinations-unintentional\nfalsehoods-have been widely studied, the phenomenon of lying, where an LLM\nknowingly generates falsehoods to achieve an ulterior objective, remains\nunderexplored. In this work, we systematically investigate the lying behavior\nof LLMs, differentiating it from hallucinations and testing it in practical\nscenarios. Through mechanistic interpretability techniques, we uncover the\nneural mechanisms underlying deception, employing logit lens analysis, causal\ninterventions, and contrastive activation steering to identify and control\ndeceptive behavior. We study real-world lying scenarios and introduce\nbehavioral steering vectors that enable fine-grained manipulation of lying\ntendencies. Further, we explore the trade-offs between lying and end-task\nperformance, establishing a Pareto frontier where dishonesty can enhance goal\noptimization. Our findings contribute to the broader discourse on AI ethics,\nshedding light on the risks and potential safeguards for deploying LLMs in\nhigh-stakes environments. Code and more illustrations are available at\nhttps://llm-liar.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities across\na variety of tasks, but their increasing autonomy in real-world applications\nraises concerns about their trustworthiness. While hallucinations-unintentional\nfalsehoods-have been widely studied, the phenomenon of lying, where an LLM\nknowingly generates falsehoods to achieve an ulterior objective, remains\nunderexplored. In this work, we systematically investigate the lying behavior\nof LLMs, differentiating it from hallucinations and testing it in practical\nscenarios. Through mechanistic interpretability techniques, we uncover the\nneural mechanisms underlying deception, employing logit lens analysis, causal\ninterventions, and contrastive activation steering to identify and control\ndeceptive behavior. We study real-world lying scenarios and introduce\nbehavioral steering vectors that enable fine-grained manipulation of lying\ntendencies. Further, we explore the trade-offs between lying and end-task\nperformance, establishing a Pareto frontier where dishonesty can enhance goal\noptimization. Our findings contribute to the broader discourse on AI ethics,\nshedding light on the risks and potential safeguards for deploying LLMs in\nhigh-stakes environments. Code and more illustrations are available at\nhttps://llm-liar.github.io/"
                },
                "authors": [
                    {
                        "name": "Haoran Huan"
                    },
                    {
                        "name": "Mihir Prabhudesai"
                    },
                    {
                        "name": "Mengning Wu"
                    },
                    {
                        "name": "Shantanu Jaiswal"
                    },
                    {
                        "name": "Deepak Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Pathak"
                },
                "author": "Deepak Pathak",
                "arxiv_comment": "Website at https://llm-liar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03516v1",
                "updated": "2025-09-03T17:58:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    58,
                    12,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:58:12Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    58,
                    12,
                    2,
                    246,
                    0
                ],
                "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?"
                },
                "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/."
                },
                "authors": [
                    {
                        "name": "Ouxiang Li"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Huijuan Huang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Jiarong Ou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Project Page: https://t2i-corebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03512v1",
                "updated": "2025-09-03T17:52:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    52,
                    16,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:52:16Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    52,
                    16,
                    2,
                    246,
                    0
                ],
                "title": "Bayesian Multivariate Sparse Functional PCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Multivariate Sparse Functional PCA"
                },
                "summary": "Functional Principal Components Analysis (FPCA) provides a parsimonious,\nsemi-parametric model for multivariate, sparsely-observed functional data.\nFrequentist FPCA approaches estimate principal components (PCs) from the data,\nthen condition on these estimates in subsequent analyses. As an alternative, we\npropose a fully Bayesian inferential framework for multivariate, sparse\nfunctional data (MSFAST) which explicitly models the PCs and incorporates their\nuncertainty. MSFAST builds upon the FAST approach to FPCA for univariate,\ndensely-observed functional data. Like FAST, MSFAST represents PCs using\northonormal splines, samples the orthonormal spline coefficients using\nparameter expansion, and enforces eigenvalue ordering during model fit. MSFAST\nextends FAST to multivariate, sparsely-observed data by (1) standardizing each\nfunctional covariate to mitigate poor posterior conditioning due to disparate\nscales; (2) using a better-suited orthogonal spline basis; (3) parallelizing\nlikelihood calculations over covariates; (4) updating parameterizations and\npriors for computational stability; (5) using a Procrustes-based posterior\nalignment procedure; and (6) providing efficient prediction routines. We\nevaluated MSFAST alongside existing implementations using simulations. MSFAST\nproduces uniquely valid inferences and accurate estimates, particularly for\nsmaller signals. MSFAST is motivated by and applied to a study of child growth,\nwith an accompanying vignette illustrating the implementation step-by-step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Principal Components Analysis (FPCA) provides a parsimonious,\nsemi-parametric model for multivariate, sparsely-observed functional data.\nFrequentist FPCA approaches estimate principal components (PCs) from the data,\nthen condition on these estimates in subsequent analyses. As an alternative, we\npropose a fully Bayesian inferential framework for multivariate, sparse\nfunctional data (MSFAST) which explicitly models the PCs and incorporates their\nuncertainty. MSFAST builds upon the FAST approach to FPCA for univariate,\ndensely-observed functional data. Like FAST, MSFAST represents PCs using\northonormal splines, samples the orthonormal spline coefficients using\nparameter expansion, and enforces eigenvalue ordering during model fit. MSFAST\nextends FAST to multivariate, sparsely-observed data by (1) standardizing each\nfunctional covariate to mitigate poor posterior conditioning due to disparate\nscales; (2) using a better-suited orthogonal spline basis; (3) parallelizing\nlikelihood calculations over covariates; (4) updating parameterizations and\npriors for computational stability; (5) using a Procrustes-based posterior\nalignment procedure; and (6) providing efficient prediction routines. We\nevaluated MSFAST alongside existing implementations using simulations. MSFAST\nproduces uniquely valid inferences and accurate estimates, particularly for\nsmaller signals. MSFAST is motivated by and applied to a study of child growth,\nwith an accompanying vignette illustrating the implementation step-by-step."
                },
                "authors": [
                    {
                        "name": "Joseph Sartini"
                    },
                    {
                        "name": "Scott Zeger"
                    },
                    {
                        "name": "Ciprian Crainiceanu"
                    }
                ],
                "author_detail": {
                    "name": "Ciprian Crainiceanu"
                },
                "author": "Ciprian Crainiceanu",
                "arxiv_comment": "24 pages, 7 figures for main text. Appendix contains supplemental\n  material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05657v2",
                "updated": "2025-09-03T17:47:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    47,
                    46,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-06T01:17:52Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    17,
                    52,
                    4,
                    157,
                    0
                ],
                "title": "Emulating compact binary population synthesis simulations with\n  uncertainty quantification and model comparison using Bayesian normalizing\n  flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emulating compact binary population synthesis simulations with\n  uncertainty quantification and model comparison using Bayesian normalizing\n  flows"
                },
                "summary": "Population synthesis simulations of compact binary coalescences~(CBCs) play a\ncrucial role in extracting astrophysical insights from an ensemble of\ngravitational wave~(GW) observations. However, realistic simulations can be\ncostly to implement for a dense grid of initial conditions. Normalizing flows\ncan emulate population synthesis runs to enable simulation-based inference from\nobserved catalogs and data augmentation for feature prediction in rarely\nsynthesizable sub-populations. However, flow predictions can be wrought with\nuncertainties, especially for sparse training sets. In this work, we develop a\nmethod for quantifying and marginalizing uncertainties in the emulators by\nimplementing the Bayesian Normalizing flow, a conditional density estimator\nconstructed from Bayesian neural networks. Using the exact likelihood function\nnaturally associated with density estimators, we sample the posterior\ndistribution of flow parameters with suitably chosen priors to quantify and\nmarginalize over flow uncertainties. We demonstrate the accuracy, calibration,\ninference, and data-augmentation impacts of the estimated uncertainties for\nsimulations of binary black hole populations formed through common envelope\nevolution. We outline the applications of the proposed methodology in the\ncontext of simulation-based inference from growing GW catalogs and feature\nprediction, with state-of-the-art binary evolution simulators, now marginalized\nover model and data uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population synthesis simulations of compact binary coalescences~(CBCs) play a\ncrucial role in extracting astrophysical insights from an ensemble of\ngravitational wave~(GW) observations. However, realistic simulations can be\ncostly to implement for a dense grid of initial conditions. Normalizing flows\ncan emulate population synthesis runs to enable simulation-based inference from\nobserved catalogs and data augmentation for feature prediction in rarely\nsynthesizable sub-populations. However, flow predictions can be wrought with\nuncertainties, especially for sparse training sets. In this work, we develop a\nmethod for quantifying and marginalizing uncertainties in the emulators by\nimplementing the Bayesian Normalizing flow, a conditional density estimator\nconstructed from Bayesian neural networks. Using the exact likelihood function\nnaturally associated with density estimators, we sample the posterior\ndistribution of flow parameters with suitably chosen priors to quantify and\nmarginalize over flow uncertainties. We demonstrate the accuracy, calibration,\ninference, and data-augmentation impacts of the estimated uncertainties for\nsimulations of binary black hole populations formed through common envelope\nevolution. We outline the applications of the proposed methodology in the\ncontext of simulation-based inference from growing GW catalogs and feature\nprediction, with state-of-the-art binary evolution simulators, now marginalized\nover model and data uncertainties."
                },
                "authors": [
                    {
                        "name": "Anarya Ray"
                    }
                ],
                "author_detail": {
                    "name": "Anarya Ray"
                },
                "author": "Anarya Ray",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00191v2",
                "updated": "2025-09-03T17:45:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    45,
                    56,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-31T20:20:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    20,
                    20,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Sharp Bounds on the Variance of General Regression Adjustment in\n  Randomized Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Bounds on the Variance of General Regression Adjustment in\n  Randomized Experiments"
                },
                "summary": "A growing statistical literature focuses on causal inference in the context\nof experiments where the target of inference is the average treatment effect in\na finite population and random assignment determines which subjects are\nallocated to one of the experimental conditions. In this framework, variances\nof average treatment effect estimators remain unidentified because they depend\non the covariance between treated and untreated potential outcomes, which are\nnever jointly observed. Conventional variance estimators are upwardly biased.\nAronow, Green and Lee [Ann. Statist. 42(3): 850-871 (June 2014)] provide an\nestimator for the variance of the difference-in-means estimator that is\nasymptotically sharp. In practice, researchers often use some form of covariate\nadjustment, such as linear regression, when estimating the average treatment\neffect. Adapting propositions from empirical process theory, we extend the\nresult in (Aronow et al., 2014), providing asymptotically sharp variance bounds\nfor general regression adjustment. We apply these results to linear regression\nadjustment and show benefits both in a simulation and in three empirical\napplications drawn from different disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing statistical literature focuses on causal inference in the context\nof experiments where the target of inference is the average treatment effect in\na finite population and random assignment determines which subjects are\nallocated to one of the experimental conditions. In this framework, variances\nof average treatment effect estimators remain unidentified because they depend\non the covariance between treated and untreated potential outcomes, which are\nnever jointly observed. Conventional variance estimators are upwardly biased.\nAronow, Green and Lee [Ann. Statist. 42(3): 850-871 (June 2014)] provide an\nestimator for the variance of the difference-in-means estimator that is\nasymptotically sharp. In practice, researchers often use some form of covariate\nadjustment, such as linear regression, when estimating the average treatment\neffect. Adapting propositions from empirical process theory, we extend the\nresult in (Aronow et al., 2014), providing asymptotically sharp variance bounds\nfor general regression adjustment. We apply these results to linear regression\nadjustment and show benefits both in a simulation and in three empirical\napplications drawn from different disciplines."
                },
                "authors": [
                    {
                        "name": "Jonas M. Mikhaeil"
                    },
                    {
                        "name": "Donald P. Green"
                    }
                ],
                "author_detail": {
                    "name": "Donald P. Green"
                },
                "author": "Donald P. Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03505v1",
                "updated": "2025-09-03T17:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    39,
                    8,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    39,
                    8,
                    2,
                    246,
                    0
                ],
                "title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence"
                },
                "summary": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX, the first installment of our large\nstructured-data models (LDMs). LimiX treats structured data as a joint\ndistribution over variables and missingness, thus capable of addressing a wide\nrange of tabular tasks through query-based conditional prediction via a single\nmodel. LimiX is pretrained using masked joint-distribution modeling with an\nepisodic, context-conditional objective, where the model predicts for query\nsubsets conditioned on dataset-specific contexts, supporting rapid,\ntraining-free adaptation at inference. We evaluate LimiX across 10 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. With a single model and a unified\ninterface, LimiX consistently surpasses strong baselines including\ngradient-boosting trees, deep tabular networks, recent tabular foundation\nmodels, and automated ensembles, as shown in Figure 1 and Figure 2. The\nsuperiority holds across a wide range of tasks, such as classification,\nregression, missing value imputation, and data generation, often by substantial\nmargins, while avoiding task-specific architectures or bespoke training per\ntask. All LimiX models are publicly accessible under Apache 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX, the first installment of our large\nstructured-data models (LDMs). LimiX treats structured data as a joint\ndistribution over variables and missingness, thus capable of addressing a wide\nrange of tabular tasks through query-based conditional prediction via a single\nmodel. LimiX is pretrained using masked joint-distribution modeling with an\nepisodic, context-conditional objective, where the model predicts for query\nsubsets conditioned on dataset-specific contexts, supporting rapid,\ntraining-free adaptation at inference. We evaluate LimiX across 10 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. With a single model and a unified\ninterface, LimiX consistently surpasses strong baselines including\ngradient-boosting trees, deep tabular networks, recent tabular foundation\nmodels, and automated ensembles, as shown in Figure 1 and Figure 2. The\nsuperiority holds across a wide range of tasks, such as classification,\nregression, missing value imputation, and data generation, often by substantial\nmargins, while avoiding task-specific architectures or bespoke training per\ntask. All LimiX models are publicly accessible under Apache 2.0."
                },
                "authors": [
                    {
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "name": "Gang Ren"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiansheng Li"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Lang Mo"
                    },
                    {
                        "name": "Li Mao"
                    },
                    {
                        "name": "Mingchao Hao"
                    },
                    {
                        "name": "Ningbo Dai"
                    },
                    {
                        "name": "Renzhe Xu"
                    },
                    {
                        "name": "Shuyang Li"
                    },
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Yue He"
                    },
                    {
                        "name": "Yuanrui Wang"
                    },
                    {
                        "name": "Yunjia Zhang"
                    },
                    {
                        "name": "Zijing Xu"
                    },
                    {
                        "name": "Dongzhe Li"
                    },
                    {
                        "name": "Fang Gao"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Jiandong Liu"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Kaijie Cheng"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Linjun Zhou"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Shaohua Fan"
                    },
                    {
                        "name": "Xiaoyu Lin"
                    },
                    {
                        "name": "Xinyan Han"
                    },
                    {
                        "name": "Xuanyue Li"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Yuan Xue"
                    },
                    {
                        "name": "Yuanyuan Jiang"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Zhenlei Wang"
                    },
                    {
                        "name": "Peng Cui"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cui"
                },
                "author": "Peng Cui",
                "arxiv_comment": "56 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03501v1",
                "updated": "2025-09-03T17:33:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    33,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:33:20Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    33,
                    20,
                    2,
                    246,
                    0
                ],
                "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data"
                },
                "summary": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs."
                },
                "authors": [
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Xiangyu Peng"
                    },
                    {
                        "name": "Shrikant Kendre"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles",
                "arxiv_comment": "This technical report serves as the archival version of our paper\n  accepted at the ICCV 2025 Workshop. For more information, please visit our\n  project website: https://strefer.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v1",
                "updated": "2025-09-03T17:29:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03494v1",
                "updated": "2025-09-03T17:23:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    24,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:23:24Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    24,
                    2,
                    246,
                    0
                ],
                "title": "Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual\n  Prompts for NR-IQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual\n  Prompts for NR-IQA"
                },
                "summary": "In this paper, we propose a novel parameter-efficient adaptation method for\nNo- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized\nin pixel-space. Unlike full fine-tuning of Multimodal Large Language Models\n(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base\nmodel), while keeping the underlying model fully frozen. During inference,\nthese visual prompts are combined with images via addition and processed by\nmPLUG-Owl2 with the textual query \"Rate the technical quality of the image.\"\nEvaluations across distortion types (synthetic, realistic, AI-generated) on\nKADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against\nfull finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on\nKADID-10k. To our knowledge, this is the first work to leverage pixel-space\nvisual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level\nvision tasks. The source code is publicly available at https: // github. com/\nyahya-ben/ mplug2-vp-for-nriqa .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel parameter-efficient adaptation method for\nNo- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized\nin pixel-space. Unlike full fine-tuning of Multimodal Large Language Models\n(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base\nmodel), while keeping the underlying model fully frozen. During inference,\nthese visual prompts are combined with images via addition and processed by\nmPLUG-Owl2 with the textual query \"Rate the technical quality of the image.\"\nEvaluations across distortion types (synthetic, realistic, AI-generated) on\nKADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against\nfull finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on\nKADID-10k. To our knowledge, this is the first work to leverage pixel-space\nvisual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level\nvision tasks. The source code is publicly available at https: // github. com/\nyahya-ben/ mplug2-vp-for-nriqa ."
                },
                "authors": [
                    {
                        "name": "Yahya Benmahane"
                    },
                    {
                        "name": "Mohammed El Hassouni"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed El Hassouni"
                },
                "author": "Mohammed El Hassouni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03493v1",
                "updated": "2025-09-03T17:23:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    19,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:23:19Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    19,
                    2,
                    246,
                    0
                ],
                "title": "On Entropy Control in LLM-RL Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Entropy Control in LLM-RL Algorithms"
                },
                "summary": "For RL algorithms, appropriate entropy control is crucial to their\neffectiveness. To control the policy entropy, a commonly used method is entropy\nregularization, which is adopted in various popular RL algorithms including\nPPO, SAC and A3C. Although entropy regularization proves effective in robotic\nand games RL conventionally, studies found that it gives weak to no gains in\nLLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL\nsetting. Specifically, we first argue that the conventional entropy\nregularization suffers from the LLM's extremely large response space and the\nsparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy\ncontrol method that utilizes a new clamped entropy bonus with an automatically\nadjusted coefficient. The clamped entropy is evaluated with the re-normalized\npolicy defined on certain smaller token space, which encourages exploration\nwithin a more compact response set. In addition, the algorithm automatically\nadjusts entropy coefficient according to the clamped entropy value, effectively\ncontrolling the entropy-induced bias while leveraging the entropy's benefits.\nAEnt is tested in math-reasoning tasks under different base models and\ndatasets, and it is observed that AEnt outperforms the baselines consistently\nacross multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For RL algorithms, appropriate entropy control is crucial to their\neffectiveness. To control the policy entropy, a commonly used method is entropy\nregularization, which is adopted in various popular RL algorithms including\nPPO, SAC and A3C. Although entropy regularization proves effective in robotic\nand games RL conventionally, studies found that it gives weak to no gains in\nLLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL\nsetting. Specifically, we first argue that the conventional entropy\nregularization suffers from the LLM's extremely large response space and the\nsparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy\ncontrol method that utilizes a new clamped entropy bonus with an automatically\nadjusted coefficient. The clamped entropy is evaluated with the re-normalized\npolicy defined on certain smaller token space, which encourages exploration\nwithin a more compact response set. In addition, the algorithm automatically\nadjusts entropy coefficient according to the clamped entropy value, effectively\ncontrolling the entropy-induced bias while leveraging the entropy's benefits.\nAEnt is tested in math-reasoning tasks under different base models and\ndatasets, and it is observed that AEnt outperforms the baselines consistently\nacross multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Han Shen"
                    }
                ],
                "author_detail": {
                    "name": "Han Shen"
                },
                "author": "Han Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12100v2",
                "updated": "2025-09-03T17:18:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    18,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-12T21:20:10Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    20,
                    10,
                    3,
                    163,
                    0
                ],
                "title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions\n  to Generative Model's Response for Vulnerability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions\n  to Generative Model's Response for Vulnerability Analysis"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for cybersecurity threat\nanalysis, but their deployment in security-sensitive environments raises trust\nand safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual\nanalysis is infeasible, making scalable and verifiable AI support critical.\nWhen querying LLMs, dealing with emerging vulnerabilities is challenging as\nthey have a training cut-off date. While Retrieval-Augmented Generation (RAG)\ncan inject up-to-date context to alleviate the cut-off date limitation, it\nremains unclear how much LLMs rely on retrieved evidence versus the model's\ninternal knowledge, and whether the retrieved information is meaningful or even\ncorrect. This uncertainty could mislead security analysts, mis-prioritize\npatches, and increase security risks. Therefore, this work proposes LLM\nEmbedding-based Attribution (LEA) to analyze the generated responses for\nvulnerability exploitation analysis. More specifically, LEA quantifies the\nrelative contribution of internal knowledge vs. retrieved content in the\ngenerated responses. We evaluate LEA on 500 critical vulnerabilities disclosed\nbetween 2016 and 2025, across three RAG settings -- valid, generic, and\nincorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's\nability to detect clear distinctions between non-retrieval, generic-retrieval,\nand valid-retrieval scenarios with over 95% accuracy on larger models. Finally,\nwe demonstrate the limitations posed by incorrect retrieval of vulnerability\ninformation and raise a cautionary note to the cybersecurity community\nregarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA\noffers security analysts with a metric to audit RAG-enhanced workflows,\nimproving the transparent and trustworthy deployment of AI in cybersecurity\nthreat analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for cybersecurity threat\nanalysis, but their deployment in security-sensitive environments raises trust\nand safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual\nanalysis is infeasible, making scalable and verifiable AI support critical.\nWhen querying LLMs, dealing with emerging vulnerabilities is challenging as\nthey have a training cut-off date. While Retrieval-Augmented Generation (RAG)\ncan inject up-to-date context to alleviate the cut-off date limitation, it\nremains unclear how much LLMs rely on retrieved evidence versus the model's\ninternal knowledge, and whether the retrieved information is meaningful or even\ncorrect. This uncertainty could mislead security analysts, mis-prioritize\npatches, and increase security risks. Therefore, this work proposes LLM\nEmbedding-based Attribution (LEA) to analyze the generated responses for\nvulnerability exploitation analysis. More specifically, LEA quantifies the\nrelative contribution of internal knowledge vs. retrieved content in the\ngenerated responses. We evaluate LEA on 500 critical vulnerabilities disclosed\nbetween 2016 and 2025, across three RAG settings -- valid, generic, and\nincorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's\nability to detect clear distinctions between non-retrieval, generic-retrieval,\nand valid-retrieval scenarios with over 95% accuracy on larger models. Finally,\nwe demonstrate the limitations posed by incorrect retrieval of vulnerability\ninformation and raise a cautionary note to the cybersecurity community\nregarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA\noffers security analysts with a metric to audit RAG-enhanced workflows,\nimproving the transparent and trustworthy deployment of AI in cybersecurity\nthreat analysis."
                },
                "authors": [
                    {
                        "name": "Reza Fayyazi"
                    },
                    {
                        "name": "Michael Zuzak"
                    },
                    {
                        "name": "Shanchieh Jay Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shanchieh Jay Yang"
                },
                "author": "Shanchieh Jay Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02479v2",
                "updated": "2025-09-03T17:06:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    6,
                    42,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T16:30:19Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    19,
                    1,
                    245,
                    0
                ],
                "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning"
                },
                "summary": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation."
                },
                "authors": [
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Longtao Zheng"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Yingru Li"
                    },
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00634v2",
                "updated": "2025-09-03T17:06:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    6,
                    38,
                    2,
                    246,
                    0
                ],
                "published": "2025-03-01T22:00:03Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    22,
                    0,
                    3,
                    5,
                    60,
                    0
                ],
                "title": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts"
                },
                "summary": "Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts."
                },
                "authors": [
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    }
                ],
                "author_detail": {
                    "name": "Hany Hassan Awadalla"
                },
                "author": "Hany Hassan Awadalla",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11133v2",
                "updated": "2025-09-03T17:03:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    3,
                    15,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-15T00:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    0,
                    58,
                    10,
                    4,
                    227,
                    0
                ],
                "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens\n  of Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens\n  of Documents"
                },
                "summary": "Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco"
                },
                "authors": [
                    {
                        "name": "Tomer Wolfson"
                    },
                    {
                        "name": "Harsh Trivedi"
                    },
                    {
                        "name": "Mor Geva"
                    },
                    {
                        "name": "Yoav Goldberg"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Tushar Khot"
                    },
                    {
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    }
                ],
                "author_detail": {
                    "name": "Reut Tsarfaty"
                },
                "author": "Reut Tsarfaty",
                "arxiv_comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03477v1",
                "updated": "2025-09-03T16:56:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    56,
                    27,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:56:27Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    56,
                    27,
                    2,
                    246,
                    0
                ],
                "title": "Robult: Leveraging Redundancy and Modality Specific Features for Robust\n  Multimodal Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robult: Leveraging Redundancy and Modality Specific Features for Robust\n  Multimodal Learning"
                },
                "summary": "Addressing missing modalities and limited labeled data is crucial for\nadvancing robust multimodal learning. We propose Robult, a scalable framework\ndesigned to mitigate these challenges by preserving modality-specific\ninformation and leveraging redundancy through a novel information-theoretic\napproach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled\n(PU) contrastive loss that maximizes task-relevant feature alignment while\neffectively utilizing limited labeled data in semi-supervised settings, and (2)\na latent reconstruction loss that ensures unique modality-specific information\nis retained. These strategies, embedded within a modular design, enhance\nperformance across various downstream tasks and ensure resilience to incomplete\nmodalities during inference. Experimental results across diverse datasets\nvalidate that Robult achieves superior performance over existing approaches in\nboth semi-supervised learning and missing modality contexts. Furthermore, its\nlightweight design promotes scalability and seamless integration with existing\narchitectures, making it suitable for real-world multimodal applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing missing modalities and limited labeled data is crucial for\nadvancing robust multimodal learning. We propose Robult, a scalable framework\ndesigned to mitigate these challenges by preserving modality-specific\ninformation and leveraging redundancy through a novel information-theoretic\napproach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled\n(PU) contrastive loss that maximizes task-relevant feature alignment while\neffectively utilizing limited labeled data in semi-supervised settings, and (2)\na latent reconstruction loss that ensures unique modality-specific information\nis retained. These strategies, embedded within a modular design, enhance\nperformance across various downstream tasks and ensure resilience to incomplete\nmodalities during inference. Experimental results across diverse datasets\nvalidate that Robult achieves superior performance over existing approaches in\nboth semi-supervised learning and missing modality contexts. Furthermore, its\nlightweight design promotes scalability and seamless integration with existing\narchitectures, making it suitable for real-world multimodal applications."
                },
                "authors": [
                    {
                        "name": "Duy A. Nguyen"
                    },
                    {
                        "name": "Abhi Kamboj"
                    },
                    {
                        "name": "Minh N. Do"
                    }
                ],
                "author_detail": {
                    "name": "Minh N. Do"
                },
                "author": "Minh N. Do",
                "arxiv_comment": "Accepted and presented at IJCAI 2025 in Montreal, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03463v1",
                "updated": "2025-09-03T16:39:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    39,
                    25,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:39:25Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    39,
                    25,
                    2,
                    246,
                    0
                ],
                "title": "The Impact of Critique on LLM-Based Model Generation from Natural\n  Language: The Case of Activity Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Critique on LLM-Based Model Generation from Natural\n  Language: The Case of Activity Diagrams"
                },
                "summary": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average."
                },
                "authors": [
                    {
                        "name": "Parham Khamsepour"
                    },
                    {
                        "name": "Mark Cole"
                    },
                    {
                        "name": "Ish Ashraf"
                    },
                    {
                        "name": "Sandeep Puri"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Shiva Nejati"
                    }
                ],
                "author_detail": {
                    "name": "Shiva Nejati"
                },
                "author": "Shiva Nejati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03462v1",
                "updated": "2025-09-03T16:37:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    37,
                    49,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:37:49Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    37,
                    49,
                    2,
                    246,
                    0
                ],
                "title": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning"
                },
                "summary": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Zhuo Cao"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v3",
                "updated": "2025-09-03T16:36:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    36,
                    17,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03458v1",
                "updated": "2025-09-03T16:33:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    33,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:33:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    33,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "Comparison of Halo Model and Simulation Predictions for Projected-Field\n  Kinematic Sunyaev-Zel'dovich Cross-Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Halo Model and Simulation Predictions for Projected-Field\n  Kinematic Sunyaev-Zel'dovich Cross-Correlations"
                },
                "summary": "The kinematic Sunyaev-Zel'dovich (kSZ) effect in the cosmic microwave\nbackground (CMB) is a powerful probe of gas physics and large-scale structure\n(LSS) in our universe. We consider the \"projected-field\" kSZ estimator, which\ninvolves cross-correlating a foreground-cleaned, filtered, squared CMB\ntemperature map with an LSS tracer, and requires no individual tracer\nredshifts. We compare $\\verb|class_sz|$ halo model calculations of\nprojected-field kSZ cross-correlations with measurements of these signals from\nthe Websky numerical simulations. We cross-correlate halo density maps from\nWebsky with various CMB secondary signals. We first validate our halo model by\ncomparing its predictions for thermal SZ (tSZ) and patchy screening ($\\tau$)\ncross-correlations to measurements of these signals from Websky. We consider\nthree different halo redshift ranges in our comparisons. We also construct our\nown kSZ, tSZ, and $\\tau$ maps to validate the form of the relevant profiles.\nFollowing the tSZ and $\\tau$ validation, we compare projected-field kSZ\ncalculations between the halo model and the simulations. We use filters\nconstructed for $\\textit{Planck}$ and the Simons Observatory (SO) to assess the\naccuracy of the halo-model kSZ predictions for experiments of differing\nsensitivity. Overall, we find good agreement, particularly at $\\textit{Planck}$\nsensitivity. However, we find an $\\approx$ 20$\\%$ difference between our halo\nmodel and the simulations for SO, which significantly exceeds the predicted\nerror bars. We note that our halo model includes only the dominant expected\nterm in the projected-field kSZ signal; the magnitude of the difference between\nour model and the simulations is consistent with previous predictions for terms\narising from other contractions in the theory calculation. These terms will\nneed to be included to obtain unbiased inference from upcoming projected-field\nkSZ measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kinematic Sunyaev-Zel'dovich (kSZ) effect in the cosmic microwave\nbackground (CMB) is a powerful probe of gas physics and large-scale structure\n(LSS) in our universe. We consider the \"projected-field\" kSZ estimator, which\ninvolves cross-correlating a foreground-cleaned, filtered, squared CMB\ntemperature map with an LSS tracer, and requires no individual tracer\nredshifts. We compare $\\verb|class_sz|$ halo model calculations of\nprojected-field kSZ cross-correlations with measurements of these signals from\nthe Websky numerical simulations. We cross-correlate halo density maps from\nWebsky with various CMB secondary signals. We first validate our halo model by\ncomparing its predictions for thermal SZ (tSZ) and patchy screening ($\\tau$)\ncross-correlations to measurements of these signals from Websky. We consider\nthree different halo redshift ranges in our comparisons. We also construct our\nown kSZ, tSZ, and $\\tau$ maps to validate the form of the relevant profiles.\nFollowing the tSZ and $\\tau$ validation, we compare projected-field kSZ\ncalculations between the halo model and the simulations. We use filters\nconstructed for $\\textit{Planck}$ and the Simons Observatory (SO) to assess the\naccuracy of the halo-model kSZ predictions for experiments of differing\nsensitivity. Overall, we find good agreement, particularly at $\\textit{Planck}$\nsensitivity. However, we find an $\\approx$ 20$\\%$ difference between our halo\nmodel and the simulations for SO, which significantly exceeds the predicted\nerror bars. We note that our halo model includes only the dominant expected\nterm in the projected-field kSZ signal; the magnitude of the difference between\nour model and the simulations is consistent with previous predictions for terms\narising from other contractions in the theory calculation. These terms will\nneed to be included to obtain unbiased inference from upcoming projected-field\nkSZ measurements."
                },
                "authors": [
                    {
                        "name": "Michael Jacob Rodriguez"
                    },
                    {
                        "name": "Aleksandra Kusiak"
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "J. Colin Hill"
                    }
                ],
                "author_detail": {
                    "name": "J. Colin Hill"
                },
                "author": "J. Colin Hill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04394v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04394v4",
                "updated": "2025-09-04T12:13:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    13,
                    47,
                    3,
                    247,
                    0
                ],
                "published": "2024-12-05T18:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bayesian Quantum Amplitude Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantum Amplitude Estimation"
                },
                "summary": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail."
                },
                "authors": [
                    {
                        "name": "Alexandra Ramôa"
                    },
                    {
                        "name": "Luis Paulo Santos"
                    }
                ],
                "author_detail": {
                    "name": "Luis Paulo Santos"
                },
                "author": "Luis Paulo Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04394v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04394v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08848v2",
                "updated": "2025-09-03T16:19:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    19,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-01-15T15:00:11Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    0,
                    11,
                    2,
                    15,
                    0
                ],
                "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning"
                },
                "summary": "Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators."
                },
                "authors": [
                    {
                        "name": "Carlos Güemes-Palau"
                    },
                    {
                        "name": "Miquel Ferriol-Galmés"
                    },
                    {
                        "name": "Jordi Paillisse-Vilanova"
                    },
                    {
                        "name": "Albert López-Brescó"
                    },
                    {
                        "name": "Pere Barlet-Ros"
                    },
                    {
                        "name": "Albert Cabellos-Aparicio"
                    }
                ],
                "author_detail": {
                    "name": "Albert Cabellos-Aparicio"
                },
                "author": "Albert Cabellos-Aparicio",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v3",
                "updated": "2025-09-03T16:15:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    15,
                    16,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v3",
                "updated": "2025-09-03T16:14:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    14,
                    12,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10978v2",
                "updated": "2025-09-03T16:13:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    13,
                    41,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-16T08:26:59Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    26,
                    59,
                    4,
                    136,
                    0
                ],
                "title": "Group-in-Group Policy Optimization for LLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-in-Group Policy Optimization for LLM Agent Training"
                },
                "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost."
                },
                "authors": [
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Tingcong Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03427v1",
                "updated": "2025-09-03T15:58:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    58,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:58:22Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    58,
                    22,
                    2,
                    246,
                    0
                ],
                "title": "Federated Learning: An approach with Hybrid Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning: An approach with Hybrid Homomorphic Encryption"
                },
                "summary": "Federated Learning (FL) is a distributed machine learning approach that\npromises privacy by keeping the data on the device. However, gradient\nreconstruction and membership-inference attacks show that model updates still\nleak information. Fully Homomorphic Encryption (FHE) can address those privacy\nconcerns but it suffers from ciphertext expansion and requires prohibitive\noverhead on resource-constrained devices. We propose the first Hybrid\nHomomorphic Encryption (HHE) framework for FL that pairs the PASTA symmetric\ncipher with the BFV FHE scheme. Clients encrypt local model updates with PASTA\nand send both the lightweight ciphertexts and the PASTA key (itself\nBFV-encrypted) to the server, which performs a homomorphic evaluation of the\ndecryption circuit of PASTA and aggregates the resulting BFV ciphertexts. A\nprototype implementation, developed on top of the Flower FL framework, shows\nthat on independently and identically distributed MNIST dataset with 12 clients\nand 10 training rounds, the proposed HHE system achieves 97.6% accuracy, just\n1.3% below plaintext, while reducing client upload bandwidth by over 2,000x and\ncutting client runtime by 30% compared to a system based solely on the BFV FHE\nscheme. However, server computational cost increases by roughly 15621x for each\nclient participating in the training phase, a challenge to be addressed in\nfuture work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a distributed machine learning approach that\npromises privacy by keeping the data on the device. However, gradient\nreconstruction and membership-inference attacks show that model updates still\nleak information. Fully Homomorphic Encryption (FHE) can address those privacy\nconcerns but it suffers from ciphertext expansion and requires prohibitive\noverhead on resource-constrained devices. We propose the first Hybrid\nHomomorphic Encryption (HHE) framework for FL that pairs the PASTA symmetric\ncipher with the BFV FHE scheme. Clients encrypt local model updates with PASTA\nand send both the lightweight ciphertexts and the PASTA key (itself\nBFV-encrypted) to the server, which performs a homomorphic evaluation of the\ndecryption circuit of PASTA and aggregates the resulting BFV ciphertexts. A\nprototype implementation, developed on top of the Flower FL framework, shows\nthat on independently and identically distributed MNIST dataset with 12 clients\nand 10 training rounds, the proposed HHE system achieves 97.6% accuracy, just\n1.3% below plaintext, while reducing client upload bandwidth by over 2,000x and\ncutting client runtime by 30% compared to a system based solely on the BFV FHE\nscheme. However, server computational cost increases by roughly 15621x for each\nclient participating in the training phase, a challenge to be addressed in\nfuture work."
                },
                "authors": [
                    {
                        "name": "Pedro Correia"
                    },
                    {
                        "name": "Ivan Silva"
                    },
                    {
                        "name": "Ivone Amorim"
                    },
                    {
                        "name": "Eva Maia"
                    },
                    {
                        "name": "Isabel Praça"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Praça"
                },
                "author": "Isabel Praça",
                "arxiv_comment": "19 pages, 8 figures, To be published in the conference Security and\n  Trust Management(STM), ESORICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3; C.2.0; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16513v2",
                "updated": "2025-09-03T15:54:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    54,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-19T18:00:02Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    18,
                    0,
                    2,
                    3,
                    170,
                    0
                ],
                "title": "10,000 Resolved Triples from Gaia: Empirical Constraints on Triple Star\n  Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "10,000 Resolved Triples from Gaia: Empirical Constraints on Triple Star\n  Populations"
                },
                "summary": "We present a catalog of $\\sim 10,000$ resolved triple star systems within 500\npc of the Sun, constructed using Gaia data. The triples include main-sequence,\nred giant, and white dwarf components spanning separations of 10 to 50,000 au.\nA well-characterized selection function allows us to constrain intrinsic\ndemographics of the triple star population. We find that (a) all systems are\ncompatible with being hierarchical and dynamically stable; (b) mutual orbital\ninclinations are isotropic for wide triples but show modest alignment as the\nsystems become more compact; (c) primary masses follow a Kroupa initial mass\nfunction weighted by the triple fraction; (d) inner binary orbital periods,\neccentricities, and mass ratios mirror those of isolated binaries, including a\npronounced twin excess (mass ratios greater than 0.95) out to separations of\n1000+ au, suggesting a common formation pathway; (e) tertiary mass ratios\nfollow a power-law distribution with slope $-1.4$; (f) tertiary orbits are\nconsistent with a log-normal period distribution and thermal eccentricities,\nsubject to dynamical stability. Informed by these observations, we develop a\npublicly available prescription for generating mock triple star populations.\nFinally, we estimate the catalog's completeness and infer the intrinsic triple\nfraction, which rises steadily with primary mass: from $5\\%$ at $\\lesssim\n0.5\\,{\\rm M_\\odot}$ to $35\\%$ at $2\\,{\\rm M_\\odot}$. The public catalog\nprovides a robust testbed for models of triple star formation and evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a catalog of $\\sim 10,000$ resolved triple star systems within 500\npc of the Sun, constructed using Gaia data. The triples include main-sequence,\nred giant, and white dwarf components spanning separations of 10 to 50,000 au.\nA well-characterized selection function allows us to constrain intrinsic\ndemographics of the triple star population. We find that (a) all systems are\ncompatible with being hierarchical and dynamically stable; (b) mutual orbital\ninclinations are isotropic for wide triples but show modest alignment as the\nsystems become more compact; (c) primary masses follow a Kroupa initial mass\nfunction weighted by the triple fraction; (d) inner binary orbital periods,\neccentricities, and mass ratios mirror those of isolated binaries, including a\npronounced twin excess (mass ratios greater than 0.95) out to separations of\n1000+ au, suggesting a common formation pathway; (e) tertiary mass ratios\nfollow a power-law distribution with slope $-1.4$; (f) tertiary orbits are\nconsistent with a log-normal period distribution and thermal eccentricities,\nsubject to dynamical stability. Informed by these observations, we develop a\npublicly available prescription for generating mock triple star populations.\nFinally, we estimate the catalog's completeness and infer the intrinsic triple\nfraction, which rises steadily with primary mass: from $5\\%$ at $\\lesssim\n0.5\\,{\\rm M_\\odot}$ to $35\\%$ at $2\\,{\\rm M_\\odot}$. The public catalog\nprovides a robust testbed for models of triple star formation and evolution."
                },
                "authors": [
                    {
                        "name": "Cheyanne Shariat"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Smadar Naoz"
                    }
                ],
                "author_detail": {
                    "name": "Smadar Naoz"
                },
                "author": "Smadar Naoz",
                "arxiv_doi": "10.1088/1538-3873/adfb30",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1538-3873/adfb30",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.16513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to PASP. Relevant code and data can be found at\n  https://github.com/cheyanneshariat/gaia_triples",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17523v2",
                "updated": "2025-09-03T15:54:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    54,
                    9,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-24T21:08:29Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    21,
                    8,
                    29,
                    6,
                    236,
                    0
                ],
                "title": "Learning Reaction-Diffusion Kinetics from Mechanical Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Reaction-Diffusion Kinetics from Mechanical Information"
                },
                "summary": "A central challenge in materials science is characterizing chemical processes\nthat are elusive to direct measurement, particularly in functional materials\noperating under realistic conditions. Here, we demonstrate that mechanical\nstrain fields contain sufficient information to reconstruct hidden chemical\nkinetics in coupled chemomechanical systems. Our partial differential\nequation-constrained learning framework decodes concentration-dependent\ndiffusion kinetics, thermodynamic driving forces, and spatially heterogeneous\nreaction rates solely from mechanical observations. Using battery electrode\nmaterials as a model system, we demonstrate that the framework can accurately\nidentify complex constitutive laws governing three distinct scenarios:\nclassical Fickian diffusion, spinodal decomposition with pattern formation, and\nheterogeneous electrochemical reactions with spatial rate variations. The\napproach demonstrates robustness while maintaining accuracy with limited\nspatial data and reasonable experimental noise levels. Most significantly, the\nframework simultaneously infers multiple fundamental processes and properties,\nincluding diffusivity, reaction kinetics, chemical potential, and spatial\nheterogeneity maps, all from mechanical information alone. This method\nestablishes a paradigm for materials characterization, enabling accurate\nlearning of chemical processes in energy storage systems, catalysts, and\nphase-change materials where conventional diagnostics prove difficult. By\nrevealing that mechanical deformation patterns serve as information-rich\nfingerprints of the underlying chemical processes, this work follows the\npathway of inversely learning constitutive laws, with broad implications in\nmaterials science and engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central challenge in materials science is characterizing chemical processes\nthat are elusive to direct measurement, particularly in functional materials\noperating under realistic conditions. Here, we demonstrate that mechanical\nstrain fields contain sufficient information to reconstruct hidden chemical\nkinetics in coupled chemomechanical systems. Our partial differential\nequation-constrained learning framework decodes concentration-dependent\ndiffusion kinetics, thermodynamic driving forces, and spatially heterogeneous\nreaction rates solely from mechanical observations. Using battery electrode\nmaterials as a model system, we demonstrate that the framework can accurately\nidentify complex constitutive laws governing three distinct scenarios:\nclassical Fickian diffusion, spinodal decomposition with pattern formation, and\nheterogeneous electrochemical reactions with spatial rate variations. The\napproach demonstrates robustness while maintaining accuracy with limited\nspatial data and reasonable experimental noise levels. Most significantly, the\nframework simultaneously infers multiple fundamental processes and properties,\nincluding diffusivity, reaction kinetics, chemical potential, and spatial\nheterogeneity maps, all from mechanical information alone. This method\nestablishes a paradigm for materials characterization, enabling accurate\nlearning of chemical processes in energy storage systems, catalysts, and\nphase-change materials where conventional diagnostics prove difficult. By\nrevealing that mechanical deformation patterns serve as information-rich\nfingerprints of the underlying chemical processes, this work follows the\npathway of inversely learning constitutive laws, with broad implications in\nmaterials science and engineering."
                },
                "authors": [
                    {
                        "name": "Royal C. Ihuaenyi"
                    },
                    {
                        "name": "Hongbo Zhao"
                    },
                    {
                        "name": "Ruqing Fang"
                    },
                    {
                        "name": "Ruobing Bai"
                    },
                    {
                        "name": "Martin Z. Bazant"
                    },
                    {
                        "name": "Juner Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Juner Zhu"
                },
                "author": "Juner Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03425v1",
                "updated": "2025-09-03T15:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    53,
                    56,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    53,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "LINKER: Learning Interactions Between Functional Groups and Residues\n  With Chemical Knowledge-Enhanced Reasoning and Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINKER: Learning Interactions Between Functional Groups and Residues\n  With Chemical Knowledge-Enhanced Reasoning and Explainability"
                },
                "summary": "Accurate identification of interactions between protein residues and ligand\nfunctional groups is essential to understand molecular recognition and guide\nrational drug design. Existing deep learning approaches for protein-ligand\ninterpretability often rely on 3D structural input or use distance-based\ncontact labels, limiting both their applicability and biological relevance. We\nintroduce LINKER, the first sequence-based model to predict residue-functional\ngroup interactions in terms of biologically defined interaction types, using\nonly protein sequences and the ligand SMILES as input. LINKER is trained with\nstructure-supervised attention, where interaction labels are derived from 3D\nprotein-ligand complexes via functional group-based motif extraction. By\nabstracting ligand structures into functional groups, the model focuses on\nchemically meaningful substructures while predicting interaction types rather\nthan mere spatial proximity. Crucially, LINKER requires only sequence-level\ninput at inference time, enabling large-scale application in settings where\nstructural data is unavailable. Experiments on the LP-PDBBind benchmark\ndemonstrate that structure-informed supervision over functional group\nabstractions yields interaction predictions closely aligned with ground-truth\nbiochemical annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of interactions between protein residues and ligand\nfunctional groups is essential to understand molecular recognition and guide\nrational drug design. Existing deep learning approaches for protein-ligand\ninterpretability often rely on 3D structural input or use distance-based\ncontact labels, limiting both their applicability and biological relevance. We\nintroduce LINKER, the first sequence-based model to predict residue-functional\ngroup interactions in terms of biologically defined interaction types, using\nonly protein sequences and the ligand SMILES as input. LINKER is trained with\nstructure-supervised attention, where interaction labels are derived from 3D\nprotein-ligand complexes via functional group-based motif extraction. By\nabstracting ligand structures into functional groups, the model focuses on\nchemically meaningful substructures while predicting interaction types rather\nthan mere spatial proximity. Crucially, LINKER requires only sequence-level\ninput at inference time, enabling large-scale application in settings where\nstructural data is unavailable. Experiments on the LP-PDBBind benchmark\ndemonstrate that structure-informed supervision over functional group\nabstractions yields interaction predictions closely aligned with ground-truth\nbiochemical annotations."
                },
                "authors": [
                    {
                        "name": "Phuc Pham"
                    },
                    {
                        "name": "Viet Thanh Duy Nguyen"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18179v2",
                "updated": "2025-09-03T15:53:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    53,
                    18,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-25T13:11:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs"
                },
                "summary": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "arxiv_comment": "accepted at EMNLP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03419v1",
                "updated": "2025-09-03T15:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:48:33Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "title": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges"
                },
                "summary": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels."
                },
                "authors": [
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Qingqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "8 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11088v2",
                "updated": "2025-09-03T15:43:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    43,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-14T21:57:10Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    21,
                    57,
                    10,
                    3,
                    226,
                    0
                ],
                "title": "Pulsations change the structures of massive stars before they explode:\n  interpreting the nearby supernova SN 2023ixf",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulsations change the structures of massive stars before they explode:\n  interpreting the nearby supernova SN 2023ixf"
                },
                "summary": "Massive red supergiants (RSGs) are known to become hydrodynamically unstable\nbefore they explode. Still, the vast majority of supernova (SN) models assume\nRSG progenitors in hydrostatic equilibrium. Here, we self-consistently follow\nthe hydrodynamic evolution of RSGs with different masses and the development of\nradial envelope pulsations. Pulsations significantly alter the observable pre-\nand post-SN properties, and their importance increases substantially as a\nfunction of initial mass. We demonstrate that inferring core masses, let alone\ninitial masses, from a single pre-SN luminosity and effective temperature of\nhigh-mass RSGs is inadvisable, as these can vary by an order of magnitude\nduring the pulsation. We find that pulsations can naturally lead to\n``early-excess\" emission in SN light curves and to variations in early\nphotospheric velocities, which can help break degeneracies in type-II SNe. We\ncompare to SN~2023ixf, for which a pulsating RSG progenitor was discovered. We\ndemonstrate that its pre- and post-SN characteristics agree very well with our\nexploding pulsating RSG model, whereas hydrostatic stellar models are not\nwell-suited. The data coverage at early times is insufficient to break all\ndegeneracies, but we find constraints on the explosion phase. We find no\nevidence for the claimed pulsation period of the SN~2024ggi progenitor, as it\nmatches Spitzer's orbital period. This study underscores the importance of\nhydrodynamical pre-SN stellar models. It implies an important shift in our\nunderstanding of the last stages of massive star evolution, the interpretation\nof pre-SN properties, the connection between SNe and their progenitors, and the\nmissing RSG problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive red supergiants (RSGs) are known to become hydrodynamically unstable\nbefore they explode. Still, the vast majority of supernova (SN) models assume\nRSG progenitors in hydrostatic equilibrium. Here, we self-consistently follow\nthe hydrodynamic evolution of RSGs with different masses and the development of\nradial envelope pulsations. Pulsations significantly alter the observable pre-\nand post-SN properties, and their importance increases substantially as a\nfunction of initial mass. We demonstrate that inferring core masses, let alone\ninitial masses, from a single pre-SN luminosity and effective temperature of\nhigh-mass RSGs is inadvisable, as these can vary by an order of magnitude\nduring the pulsation. We find that pulsations can naturally lead to\n``early-excess\" emission in SN light curves and to variations in early\nphotospheric velocities, which can help break degeneracies in type-II SNe. We\ncompare to SN~2023ixf, for which a pulsating RSG progenitor was discovered. We\ndemonstrate that its pre- and post-SN characteristics agree very well with our\nexploding pulsating RSG model, whereas hydrostatic stellar models are not\nwell-suited. The data coverage at early times is insufficient to break all\ndegeneracies, but we find constraints on the explosion phase. We find no\nevidence for the claimed pulsation period of the SN~2024ggi progenitor, as it\nmatches Spitzer's orbital period. This study underscores the importance of\nhydrodynamical pre-SN stellar models. It implies an important shift in our\nunderstanding of the last stages of massive star evolution, the interpretation\nof pre-SN properties, the connection between SNe and their progenitors, and the\nmissing RSG problem."
                },
                "authors": [
                    {
                        "name": "Eva Laplace"
                    },
                    {
                        "name": "Vincent A. Bronner"
                    },
                    {
                        "name": "Fabian R. N. Schneider"
                    },
                    {
                        "name": "Philipp Podsiadlowski"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Podsiadlowski"
                },
                "author": "Philipp Podsiadlowski",
                "arxiv_comment": "Submitted to AAS journals. This work is the follow-up of a companion\n  paper on arXiv (Bronner, Laplace et al. 2025, arXiv:2508.11077)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01527v2",
                "updated": "2025-09-03T15:43:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    43,
                    1,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-01T15:02:00Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    2,
                    0,
                    0,
                    244,
                    0
                ],
                "title": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model"
                },
                "summary": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling."
                },
                "authors": [
                    {
                        "name": "Amirreza Nayyeri"
                    },
                    {
                        "name": "Abbas Rasoolzadegan"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rasoolzadegan"
                },
                "author": "Abbas Rasoolzadegan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03407v1",
                "updated": "2025-09-03T15:32:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    32,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:32:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    32,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning"
                },
                "summary": "Natural language processing (NLP) enables the understanding and generation of\nmeaningful human language, typically using a pre-trained complex architecture\non a large dataset to learn the language and next fine-tune its weights to\nimplement a specific task. Twofold goals are examined; to understand the\nmechanism underlying successful pre-training and to determine the interplay\nbetween the pre-training accuracy and the fine-tuning of classification tasks.\nThe following main results were obtained; the accuracy per token (APT)\nincreased with its appearance frequency in the dataset, and its average over\nall tokens served as an order parameter to quantify pre-training success, which\nincreased along the transformer blocks. Pre-training broke the symmetry among\ntokens and grouped them into finite, small, strong match token clusters, as\ninferred from the presented token confusion matrix. This feature was sharpened\nalong the transformer blocks toward the output layer, enhancing its performance\nconsiderably compared with that of the embedding layer. Consequently,\nhigher-order language structures were generated by pre-training, even though\nthe learning cost function was directed solely at identifying a single token.\nThese pre-training findings were reflected by the improved fine-tuning accuracy\nalong the transformer blocks. Additionally, the output label prediction\nconfidence was found to be independent of the average input APT, as the input\nmeaning was preserved since the tokens are replaced primarily by strong match\ntokens. Finally, although pre-training is commonly absent in image\nclassification tasks, its underlying mechanism is similar to that used in\nfine-tuning NLP classification tasks, hinting at its universality. The results\nwere based on the BERT-6 architecture pre-trained on the Wikipedia dataset and\nfine-tuned on the FewRel and DBpedia classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) enables the understanding and generation of\nmeaningful human language, typically using a pre-trained complex architecture\non a large dataset to learn the language and next fine-tune its weights to\nimplement a specific task. Twofold goals are examined; to understand the\nmechanism underlying successful pre-training and to determine the interplay\nbetween the pre-training accuracy and the fine-tuning of classification tasks.\nThe following main results were obtained; the accuracy per token (APT)\nincreased with its appearance frequency in the dataset, and its average over\nall tokens served as an order parameter to quantify pre-training success, which\nincreased along the transformer blocks. Pre-training broke the symmetry among\ntokens and grouped them into finite, small, strong match token clusters, as\ninferred from the presented token confusion matrix. This feature was sharpened\nalong the transformer blocks toward the output layer, enhancing its performance\nconsiderably compared with that of the embedding layer. Consequently,\nhigher-order language structures were generated by pre-training, even though\nthe learning cost function was directed solely at identifying a single token.\nThese pre-training findings were reflected by the improved fine-tuning accuracy\nalong the transformer blocks. Additionally, the output label prediction\nconfidence was found to be independent of the average input APT, as the input\nmeaning was preserved since the tokens are replaced primarily by strong match\ntokens. Finally, although pre-training is commonly absent in image\nclassification tasks, its underlying mechanism is similar to that used in\nfine-tuning NLP classification tasks, hinting at its universality. The results\nwere based on the BERT-6 architecture pre-trained on the Wikipedia dataset and\nfine-tuned on the FewRel and DBpedia classification tasks."
                },
                "authors": [
                    {
                        "name": "Yarden Tzach"
                    },
                    {
                        "name": "Ronit D. Gross"
                    },
                    {
                        "name": "Ella Koresh"
                    },
                    {
                        "name": "Shalom Rosner"
                    },
                    {
                        "name": "Or Shpringer"
                    },
                    {
                        "name": "Tal Halevi"
                    },
                    {
                        "name": "Ido Kanter"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kanter"
                },
                "author": "Ido Kanter",
                "arxiv_comment": "46 pages, 18 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18096v2",
                "updated": "2025-09-03T15:32:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    32,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-22T16:52:48Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    16,
                    52,
                    48,
                    6,
                    173,
                    0
                ],
                "title": "Deep Research Agents: A Systematic Examination And Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research Agents: A Systematic Examination And Roadmap"
                },
                "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Haozheng Zhang"
                    },
                    {
                        "name": "Kang Li"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Songcen Xu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03391v1",
                "updated": "2025-09-03T15:15:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    31,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:31Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    31,
                    2,
                    246,
                    0
                ],
                "title": "More Parameters Than Populations: A Systematic Literature Review of\n  Large Language Models within Survey Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Parameters Than Populations: A Systematic Literature Review of\n  Large Language Models within Survey Research"
                },
                "summary": "Survey research has a long-standing history of being a human-powered field,\nbut one that embraces various technologies for the collection, processing, and\nanalysis of various behavioral, political, and social outcomes of interest,\namong others. At the same time, Large Language Models (LLMs) bring new\ntechnological challenges and prerequisites in order to fully harness their\npotential. In this paper, we report work-in-progress on a systematic literature\nreview based on keyword searches from multiple large-scale databases as well as\ncitation networks that assesses how LLMs are currently being applied within the\nsurvey research process. We synthesize and organize our findings according to\nthe survey research process to include examples of LLM usage across three broad\nphases: pre-data collection, data collection, and post-data collection. We\ndiscuss selected examples of potential use cases for LLMs as well as its\npitfalls based on examples from existing literature. Considering survey\nresearch has rich experience and history regarding data quality, we discuss\nsome opportunities and describe future outlooks for survey research to\ncontribute to the continued development and refinement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey research has a long-standing history of being a human-powered field,\nbut one that embraces various technologies for the collection, processing, and\nanalysis of various behavioral, political, and social outcomes of interest,\namong others. At the same time, Large Language Models (LLMs) bring new\ntechnological challenges and prerequisites in order to fully harness their\npotential. In this paper, we report work-in-progress on a systematic literature\nreview based on keyword searches from multiple large-scale databases as well as\ncitation networks that assesses how LLMs are currently being applied within the\nsurvey research process. We synthesize and organize our findings according to\nthe survey research process to include examples of LLM usage across three broad\nphases: pre-data collection, data collection, and post-data collection. We\ndiscuss selected examples of potential use cases for LLMs as well as its\npitfalls based on examples from existing literature. Considering survey\nresearch has rich experience and history regarding data quality, we discuss\nsome opportunities and describe future outlooks for survey research to\ncontribute to the continued development and refinement of LLMs."
                },
                "authors": [
                    {
                        "name": "Trent D. Buskirk"
                    },
                    {
                        "name": "Florian Keusch"
                    },
                    {
                        "name": "Leah von der Heyde"
                    },
                    {
                        "name": "Adam Eck"
                    }
                ],
                "author_detail": {
                    "name": "Adam Eck"
                },
                "author": "Adam Eck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01503v2",
                "updated": "2025-09-03T15:07:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    7,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    21,
                    45,
                    0,
                    244,
                    0
                ],
                "title": "Using Aggregate Relational Data to Infer Social Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Aggregate Relational Data to Infer Social Networks"
                },
                "summary": "This study introduces a novel approach for inferring social network\nstructures using Aggregate Relational Data (ARD), addressing the challenge of\nlimited detailed network data availability. By integrating ARD with variational\napproximation methods, we provide a computationally efficient and\ncost-effective solution for network analysis. Our methodology demonstrates the\npotential of ARD to offer insightful approximations of network dynamics, as\nevidenced by Monte Carlo Simulations. This paper not only showcases the utility\nof ARD in social network inference but also opens avenues for future research\nin enhancing estimation precision and exploring diverse network datasets.\nThrough this work, we contribute to the field of network analysis by offering\nan alternative strategy for understanding complex social networks with\nconstrained data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a novel approach for inferring social network\nstructures using Aggregate Relational Data (ARD), addressing the challenge of\nlimited detailed network data availability. By integrating ARD with variational\napproximation methods, we provide a computationally efficient and\ncost-effective solution for network analysis. Our methodology demonstrates the\npotential of ARD to offer insightful approximations of network dynamics, as\nevidenced by Monte Carlo Simulations. This paper not only showcases the utility\nof ARD in social network inference but also opens avenues for future research\nin enhancing estimation precision and exploring diverse network datasets.\nThrough this work, we contribute to the field of network analysis by offering\nan alternative strategy for understanding complex social networks with\nconstrained data."
                },
                "authors": [
                    {
                        "name": "Xunkang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Xunkang Tian"
                },
                "author": "Xunkang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01028v2",
                "updated": "2025-09-03T15:01:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    1,
                    47,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-31T23:36:44Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    23,
                    36,
                    44,
                    6,
                    243,
                    0
                ],
                "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute\n  Image Generation"
                },
                "summary": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation."
                },
                "authors": [
                    {
                        "name": "Zixin Zhu"
                    },
                    {
                        "name": "Kevin Duarte"
                    },
                    {
                        "name": "Mamshad Nayeem Rizve"
                    },
                    {
                        "name": "Chengyuan Xu"
                    },
                    {
                        "name": "Ratheesh Kalarot"
                    },
                    {
                        "name": "Junsong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Junsong Yuan"
                },
                "author": "Junsong Yuan",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02654v2",
                "updated": "2025-09-03T14:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    58,
                    7,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-03T14:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure"
                },
                "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Sanchari Sen"
                    },
                    {
                        "name": "Swagath Venkataramani"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00096v2",
                "updated": "2025-09-03T14:58:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    58,
                    3,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T15:48:18Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    15,
                    48,
                    18,
                    2,
                    239,
                    0
                ],
                "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning\n  LLMs"
                },
                "summary": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Accepted to EMNLP2025 findings (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03380v1",
                "updated": "2025-09-03T14:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems"
                },
                "summary": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "7th International Workshop on Agent-Based Modelling of Human\n  Behaviour (ABMHuB'25), ALife 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93A16",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12121v2",
                "updated": "2025-09-03T14:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    55,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-13T18:00:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    18,
                    0,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "The population of NuSTAR Black Hole X-ray Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population of NuSTAR Black Hole X-ray Binaries"
                },
                "summary": "The spin of a black hole (BH) encodes information about its formation and\nevolution history. Yet the understanding of the distribution of BH spins in\nX-ray binaries (XBs), of the models used to measure spin, and of their impact\non systematic uncertainties remains incomplete. In this work, we expand on\nprevious analyses of the entire NuSTAR archive of accreting BH XBs. Prior work\ncompiled a sample of 245 spectral fits using the relativistic reflection method\nfor NuSTAR observations of 36 BH systems. Here, we aim to probe two aspects:\nthe connection between BH spin and binary system properties, and the\nrelationships between parameters in the spectral fits. We identify moderate\nnegative correlations between spin uncertainty and both BH mass and system\ninclination, and a moderate positive correlation with distance. We also point\nout tentative multidimensional degeneracies between inclination, disk density,\nFe abundance, ionization, and the presence or absence of absorption features\nfrom ionized outflows linked to disk winds. Lastly, we provide a comprehensive\nview of the observed distribution of BH spins in XBs, in comparison to spins\ninferred from gravitational waves. We find that the distribution of BH spins in\nXBs can be described by a beta distribution with $\\alpha=5.66$ and\n$\\beta=1.09$. This data set is highly complex, and the analysis presented here\ndoes not fully explore all potential parameter correlations. We make the full\ndata set available in Zenodo to the community to encourage further exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spin of a black hole (BH) encodes information about its formation and\nevolution history. Yet the understanding of the distribution of BH spins in\nX-ray binaries (XBs), of the models used to measure spin, and of their impact\non systematic uncertainties remains incomplete. In this work, we expand on\nprevious analyses of the entire NuSTAR archive of accreting BH XBs. Prior work\ncompiled a sample of 245 spectral fits using the relativistic reflection method\nfor NuSTAR observations of 36 BH systems. Here, we aim to probe two aspects:\nthe connection between BH spin and binary system properties, and the\nrelationships between parameters in the spectral fits. We identify moderate\nnegative correlations between spin uncertainty and both BH mass and system\ninclination, and a moderate positive correlation with distance. We also point\nout tentative multidimensional degeneracies between inclination, disk density,\nFe abundance, ionization, and the presence or absence of absorption features\nfrom ionized outflows linked to disk winds. Lastly, we provide a comprehensive\nview of the observed distribution of BH spins in XBs, in comparison to spins\ninferred from gravitational waves. We find that the distribution of BH spins in\nXBs can be described by a beta distribution with $\\alpha=5.66$ and\n$\\beta=1.09$. This data set is highly complex, and the analysis presented here\ndoes not fully explore all potential parameter correlations. We make the full\ndata set available in Zenodo to the community to encourage further exploration."
                },
                "authors": [
                    {
                        "name": "Paul A. Draghis"
                    },
                    {
                        "name": "Jon M. Miller"
                    },
                    {
                        "name": "Laura Brenneman"
                    },
                    {
                        "name": "Elisa Costantini"
                    },
                    {
                        "name": "Luigi C. Gallo"
                    },
                    {
                        "name": "Mark Reynolds"
                    },
                    {
                        "name": "John A. Tomsick"
                    },
                    {
                        "name": "Abderahmen Zoghbi"
                    }
                ],
                "author_detail": {
                    "name": "Abderahmen Zoghbi"
                },
                "author": "Abderahmen Zoghbi",
                "arxiv_doi": "10.3847/1538-4357/adec85",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adec85",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.12121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages + appendix & references, 13 figures. Published in ApJ.\n  Updated to match journal version",
                "arxiv_journal_ref": "ApJ 989 227 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01550v2",
                "updated": "2025-09-03T14:56:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    47,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-03T02:34:16Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    2,
                    34,
                    16,
                    6,
                    215,
                    0
                ],
                "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End\n  Data Curation Pipeline Synergizing SFT and RL at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End\n  Data Curation Pipeline Synergizing SFT and RL at Scale"
                },
                "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks."
                },
                "authors": [
                    {
                        "name": "Zhilong Chen"
                    },
                    {
                        "name": "Chengzong Zhao"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Gustavo A. Oliva"
                    },
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Aaditya Bhatia"
                    },
                    {
                        "name": "Chong Chun Yong"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03379v1",
                "updated": "2025-09-03T14:55:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    55,
                    49,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:55:49Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    55,
                    49,
                    2,
                    246,
                    0
                ],
                "title": "TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers"
                },
                "summary": "Vision Transformers (ViTs) achieve strong performance in image classification\nbut incur high computational costs from processing all image tokens. To reduce\ninference costs in large ViTs without compromising accuracy, we propose\nTinyDrop, a training-free token dropping framework guided by a lightweight\nvision model. The guidance model estimates the importance of tokens while\nperforming inference, thereby selectively discarding low-importance tokens if\nlarge vit models need to perform attention calculations. The framework operates\nplug-and-play, requires no architectural modifications, and is compatible with\ndiverse ViT architectures. Evaluations on standard image classification\nbenchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs\nwith minimal accuracy degradation, highlighting its generalization capability\nand practical utility for efficient ViT-based classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) achieve strong performance in image classification\nbut incur high computational costs from processing all image tokens. To reduce\ninference costs in large ViTs without compromising accuracy, we propose\nTinyDrop, a training-free token dropping framework guided by a lightweight\nvision model. The guidance model estimates the importance of tokens while\nperforming inference, thereby selectively discarding low-importance tokens if\nlarge vit models need to perform attention calculations. The framework operates\nplug-and-play, requires no architectural modifications, and is compatible with\ndiverse ViT architectures. Evaluations on standard image classification\nbenchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs\nwith minimal accuracy degradation, highlighting its generalization capability\nand practical utility for efficient ViT-based classification."
                },
                "authors": [
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Qingyuan Wang"
                    },
                    {
                        "name": "Binhua Huang"
                    },
                    {
                        "name": "Shaowu Chen"
                    },
                    {
                        "name": "Deepu John"
                    }
                ],
                "author_detail": {
                    "name": "Deepu John"
                },
                "author": "Deepu John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v1",
                "updated": "2025-09-03T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03361v1",
                "updated": "2025-09-03T14:46:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    46,
                    5,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:46:05Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    46,
                    5,
                    2,
                    246,
                    0
                ],
                "title": "Search for Past Stellar Encounters and the Origin of 3I/ATLAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for Past Stellar Encounters and the Origin of 3I/ATLAS"
                },
                "summary": "3I/ATLAS, the third discovered interstellar object, has a heliocentric speed\nof 58 km/s and exhibits cometary activity. To constrain the origin of 3I/ATLAS\nand its past dynamical evolution, we propagate the orbits of 3I/ATLAS and\nnearby stars to search for stellar encounters. Integrating orbits in the\nGalactic potential and propagating the astrometric and radial-velocity\nuncertainties of 30 million Gaia stars, we identify 25 encounters with median\nencounter distances less than 1 pc. However, because the encounter speeds\nbetween 3I/ATLAS and each encounter exceed 20 km/s, none is a plausible host\nunder common ejection mechanisms. We infer stellar masses for most stars and\nquantify the gravitational perturbations exerted by each individual star or\neach binary system on 3I/ATLAS. The strongest gravitational scattering\nperturber is a wide M-dwarf binary. Among all past encounters, the binary's\nbarycenter and 3I/ATLAS reach the small encounter distance of 0.242 pc and the\nencounter speed of 28.39 km/s,1.64 Myr ago. We further demonstrate that the\ncumulative influence of the stellar encounters on both the speed and direction\nof 3I/ATLAS is weak. Based on the present kinematics of 3I/ATLAS to assess its\norigin, we find that a thin-disk origin is strongly favored, because the thin\ndisk both exhibits a velocity distribution closely matching that of 3I/ATLAS\nand provides the dominant local number density of stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3I/ATLAS, the third discovered interstellar object, has a heliocentric speed\nof 58 km/s and exhibits cometary activity. To constrain the origin of 3I/ATLAS\nand its past dynamical evolution, we propagate the orbits of 3I/ATLAS and\nnearby stars to search for stellar encounters. Integrating orbits in the\nGalactic potential and propagating the astrometric and radial-velocity\nuncertainties of 30 million Gaia stars, we identify 25 encounters with median\nencounter distances less than 1 pc. However, because the encounter speeds\nbetween 3I/ATLAS and each encounter exceed 20 km/s, none is a plausible host\nunder common ejection mechanisms. We infer stellar masses for most stars and\nquantify the gravitational perturbations exerted by each individual star or\neach binary system on 3I/ATLAS. The strongest gravitational scattering\nperturber is a wide M-dwarf binary. Among all past encounters, the binary's\nbarycenter and 3I/ATLAS reach the small encounter distance of 0.242 pc and the\nencounter speed of 28.39 km/s,1.64 Myr ago. We further demonstrate that the\ncumulative influence of the stellar encounters on both the speed and direction\nof 3I/ATLAS is weak. Based on the present kinematics of 3I/ATLAS to assess its\norigin, we find that a thin-disk origin is strongly favored, because the thin\ndisk both exhibits a velocity distribution closely matching that of 3I/ATLAS\nand provides the dominant local number density of stars."
                },
                "authors": [
                    {
                        "name": "Yiyang Guo"
                    },
                    {
                        "name": "Luyao Zhang"
                    },
                    {
                        "name": "Fabo Feng"
                    },
                    {
                        "name": "Zhao-Yu Li"
                    },
                    {
                        "name": "Anton Pomazan"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "11 pages, 5 figures, submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23751v2",
                "updated": "2025-09-03T14:36:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    36,
                    0,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-31T17:38:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks"
                },
                "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22467v2",
                "updated": "2025-09-03T14:35:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    35,
                    19,
                    2,
                    246,
                    0
                ],
                "published": "2025-03-28T14:27:54Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    27,
                    54,
                    4,
                    87,
                    0
                ],
                "title": "An integrated method for clustering and association network inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An integrated method for clustering and association network inference"
                },
                "summary": "High dimensional Gaussian graphical models provide a rigorous framework to\ndescribe a network of statistical dependencies between entities, such as genes\nin genomic regulation studies or species in ecology. Penalized methods,\nincluding the standard Graphical-Lasso, are well-known approaches to infer the\nparameters of these models. As the number of variables in the model (of\nentities in the network) grow, the network inference and interpretation become\nmore complex. The Normal-Block model is introduced, a new model that clusters\nvariables and consider a network at the cluster level. Normal-Block both adds\nstructure to the network and reduces its size. The approach builds on\nGraphical-Lasso to add a penalty on the network's edges and limit the detection\nof spurious dependencies. A zero-inflated version of the model is also proposed\nto account for real-world data properties. For the inference procedure, two\napproaches are introduced, a straightforward method based on state-of-the-art\napproaches and an original, more rigorous method that simultaneously infers the\nclustering of variables and the association network between clusters, using a\npenalized variational Expectation-Maximization approach. An implementation of\nthe model in R, in a package called \\textbf{normalblockr}, is available on\ngithub\\footnote{https://github.com/jeannetous/normalblockr}. The results of the\nmodels in terms of clustering and network inference are presented, using both\nsimulated data and various types of real-world data (proteomics and words\noccurrences on webpages).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High dimensional Gaussian graphical models provide a rigorous framework to\ndescribe a network of statistical dependencies between entities, such as genes\nin genomic regulation studies or species in ecology. Penalized methods,\nincluding the standard Graphical-Lasso, are well-known approaches to infer the\nparameters of these models. As the number of variables in the model (of\nentities in the network) grow, the network inference and interpretation become\nmore complex. The Normal-Block model is introduced, a new model that clusters\nvariables and consider a network at the cluster level. Normal-Block both adds\nstructure to the network and reduces its size. The approach builds on\nGraphical-Lasso to add a penalty on the network's edges and limit the detection\nof spurious dependencies. A zero-inflated version of the model is also proposed\nto account for real-world data properties. For the inference procedure, two\napproaches are introduced, a straightforward method based on state-of-the-art\napproaches and an original, more rigorous method that simultaneously infers the\nclustering of variables and the association network between clusters, using a\npenalized variational Expectation-Maximization approach. An implementation of\nthe model in R, in a package called \\textbf{normalblockr}, is available on\ngithub\\footnote{https://github.com/jeannetous/normalblockr}. The results of the\nmodels in terms of clustering and network inference are presented, using both\nsimulated data and various types of real-world data (proteomics and words\noccurrences on webpages)."
                },
                "authors": [
                    {
                        "name": "Jeanne Tous"
                    },
                    {
                        "name": "Julien Chiquet"
                    }
                ],
                "author_detail": {
                    "name": "Julien Chiquet"
                },
                "author": "Julien Chiquet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18721v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18721v5",
                "updated": "2025-09-03T14:34:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    34,
                    30,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-26T06:38:38Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    6,
                    38,
                    38,
                    1,
                    238,
                    0
                ],
                "title": "LLM as an Execution Estimator: Recovering Missing Dependency for\n  Practical Time-travelling Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as an Execution Estimator: Recovering Missing Dependency for\n  Practical Time-travelling Debugging"
                },
                "summary": "Determining the dynamic data dependency of a step that reads a variable $v$\nis challenging. It typically requires either exhaustive instrumentation, which\nbecomes prohibitively expensive when $v$ is defined within library calls, or\nrepeated executions, which are impractical for non-deterministic programs. In\nthis work, we propose RecovSlicing for computing dynamic data dependency in a\nsingle run, with only partial instrumentation. We explore the intuition that\nLLM can potentially infer program dynamics based on a partially recorded trace\nand relevant code as its context. Given (1) a partially recorded trace of a\nprogram $P$ and (2) the slicing criteria consisting of a query step $s$ and a\nquery variable $v$ read by $s$, RecovSlicing computes the runtime definition of\n$v$ on the trace by estimating the miss-recorded execution of $P$. In this\nwork, we allow the user to specify implicit query variable. Technically, built\nupon non-deterministic LLM, we address the challenges of (1) precise recovery\nof runtime variable value and structure from the recorded execution and (2)\naligning the memory address of recovered variables and the recorded variables\nfor definition analysis. We evaluate RecovSlicing on 8300 data dependencies\nacross three slicing benchmarks, comparing it with Slicer4J, ND-Slicer, LLM\nSlicer, and re-execution Slicer. RecovSlicing achieves significantly higher\naccuracy (80.3%, 91.1%, 98.3%) and recall (up to 98.3%) than the best baseline\n(accuracy: 39.0%, 82.0%, 59.9%; recall: 53.4%, 79.1%, 87.1%). Integrated into a\ndual-slicing regression bug localizer, it identifies 16% more regressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the dynamic data dependency of a step that reads a variable $v$\nis challenging. It typically requires either exhaustive instrumentation, which\nbecomes prohibitively expensive when $v$ is defined within library calls, or\nrepeated executions, which are impractical for non-deterministic programs. In\nthis work, we propose RecovSlicing for computing dynamic data dependency in a\nsingle run, with only partial instrumentation. We explore the intuition that\nLLM can potentially infer program dynamics based on a partially recorded trace\nand relevant code as its context. Given (1) a partially recorded trace of a\nprogram $P$ and (2) the slicing criteria consisting of a query step $s$ and a\nquery variable $v$ read by $s$, RecovSlicing computes the runtime definition of\n$v$ on the trace by estimating the miss-recorded execution of $P$. In this\nwork, we allow the user to specify implicit query variable. Technically, built\nupon non-deterministic LLM, we address the challenges of (1) precise recovery\nof runtime variable value and structure from the recorded execution and (2)\naligning the memory address of recovered variables and the recorded variables\nfor definition analysis. We evaluate RecovSlicing on 8300 data dependencies\nacross three slicing benchmarks, comparing it with Slicer4J, ND-Slicer, LLM\nSlicer, and re-execution Slicer. RecovSlicing achieves significantly higher\naccuracy (80.3%, 91.1%, 98.3%) and recall (up to 98.3%) than the best baseline\n(accuracy: 39.0%, 82.0%, 59.9%; recall: 53.4%, 79.1%, 87.1%). Integrated into a\ndual-slicing regression bug localizer, it identifies 16% more regressions."
                },
                "authors": [
                    {
                        "name": "Yunrui Pei"
                    },
                    {
                        "name": "Hongshu Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Weiyu Kong"
                    },
                    {
                        "name": "Jin song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin song Dong"
                },
                "author": "Jin song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18721v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18721v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14427v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14427v3",
                "updated": "2025-09-03T14:28:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-20T15:50:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    15,
                    50,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Frugal inference for control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frugal inference for control"
                },
                "summary": "A key challenge in advancing artificial intelligence is achieving the right\nbalance between utility maximization and resource use by both external movement\nand internal computation. While this trade-off has been studied in fully\nobservable settings, our understanding of resource efficiency in partially\nobservable environments remains limited. Motivated by this challenge, we\ndevelop a version of the POMDP framework where the information gained through\ninference is treated as a resource that must be optimized alongside task\nperformance and motion effort. By solving this problem in environments\ndescribed by linear-Gaussian dynamics, we uncover fundamental principles of\nresource efficiency. Our study reveals a phase transition in the inference,\nswitching from a Bayes-optimal approach to one that strategically leaves some\nuncertainty unresolved. This frugal behavior gives rise to a structured family\nof equally effective strategies, facilitating adaptation to later objectives\nand constraints overlooked during the original optimization. We illustrate the\napplicability of our framework and the generality of the principles we derived\nusing two nonlinear tasks. Overall, this work provides a foundation for a new\ntype of rational computation that both brains and machines could use for\neffective but resource-efficient control under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in advancing artificial intelligence is achieving the right\nbalance between utility maximization and resource use by both external movement\nand internal computation. While this trade-off has been studied in fully\nobservable settings, our understanding of resource efficiency in partially\nobservable environments remains limited. Motivated by this challenge, we\ndevelop a version of the POMDP framework where the information gained through\ninference is treated as a resource that must be optimized alongside task\nperformance and motion effort. By solving this problem in environments\ndescribed by linear-Gaussian dynamics, we uncover fundamental principles of\nresource efficiency. Our study reveals a phase transition in the inference,\nswitching from a Bayes-optimal approach to one that strategically leaves some\nuncertainty unresolved. This frugal behavior gives rise to a structured family\nof equally effective strategies, facilitating adaptation to later objectives\nand constraints overlooked during the original optimization. We illustrate the\napplicability of our framework and the generality of the principles we derived\nusing two nonlinear tasks. Overall, this work provides a foundation for a new\ntype of rational computation that both brains and machines could use for\neffective but resource-efficient control under uncertainty."
                },
                "authors": [
                    {
                        "name": "Itzel Olivos-Castillo"
                    },
                    {
                        "name": "Paul Schrater"
                    },
                    {
                        "name": "Xaq Pitkow"
                    }
                ],
                "author_detail": {
                    "name": "Xaq Pitkow"
                },
                "author": "Xaq Pitkow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14427v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14427v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03345v1",
                "updated": "2025-09-03T14:22:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    22,
                    42,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:22:42Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    22,
                    42,
                    2,
                    246,
                    0
                ],
                "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning"
                },
                "summary": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR."
                },
                "authors": [
                    {
                        "name": "Yunxin Sun"
                    },
                    {
                        "name": "Abulhair Saparov"
                    }
                ],
                "author_detail": {
                    "name": "Abulhair Saparov"
                },
                "author": "Abulhair Saparov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03341v1",
                "updated": "2025-09-03T14:18:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    18,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:18:22Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    18,
                    22,
                    2,
                    246,
                    0
                ],
                "title": "On the MIA Vulnerability Gap Between Private GANs and Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the MIA Vulnerability Gap Between Private GANs and Diffusion Models"
                },
                "summary": "Generative Adversarial Networks (GANs) and diffusion models have emerged as\nleading approaches for high-quality image synthesis. While both can be trained\nunder differential privacy (DP) to protect sensitive data, their sensitivity to\nmembership inference attacks (MIAs), a key threat to data confidentiality,\nremains poorly understood. In this work, we present the first unified\ntheoretical and empirical analysis of the privacy risks faced by differentially\nprivate generative models. We begin by showing, through a stability-based\nanalysis, that GANs exhibit fundamentally lower sensitivity to data\nperturbations than diffusion models, suggesting a structural advantage in\nresisting MIAs. We then validate this insight with a comprehensive empirical\nstudy using a standardized MIA pipeline to evaluate privacy leakage across\ndatasets and privacy budgets. Our results consistently reveal a marked privacy\nrobustness gap in favor of GANs, even in strong DP regimes, highlighting that\nmodel type alone can critically shape privacy leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Adversarial Networks (GANs) and diffusion models have emerged as\nleading approaches for high-quality image synthesis. While both can be trained\nunder differential privacy (DP) to protect sensitive data, their sensitivity to\nmembership inference attacks (MIAs), a key threat to data confidentiality,\nremains poorly understood. In this work, we present the first unified\ntheoretical and empirical analysis of the privacy risks faced by differentially\nprivate generative models. We begin by showing, through a stability-based\nanalysis, that GANs exhibit fundamentally lower sensitivity to data\nperturbations than diffusion models, suggesting a structural advantage in\nresisting MIAs. We then validate this insight with a comprehensive empirical\nstudy using a standardized MIA pipeline to evaluate privacy leakage across\ndatasets and privacy budgets. Our results consistently reveal a marked privacy\nrobustness gap in favor of GANs, even in strong DP regimes, highlighting that\nmodel type alone can critically shape privacy leakage."
                },
                "authors": [
                    {
                        "name": "Ilana Sebag"
                    },
                    {
                        "name": "Jean-Yves Franceschi"
                    },
                    {
                        "name": "Alain Rakotomamonjy"
                    },
                    {
                        "name": "Alexandre Allauzen"
                    },
                    {
                        "name": "Jamal Atif"
                    }
                ],
                "author_detail": {
                    "name": "Jamal Atif"
                },
                "author": "Jamal Atif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03336v1",
                "updated": "2025-09-03T14:12:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    12,
                    32,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:12:32Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    12,
                    32,
                    2,
                    246,
                    0
                ],
                "title": "AI-Driven Drug Repurposing through miRNA-mRNA Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Drug Repurposing through miRNA-mRNA Relation"
                },
                "summary": "miRNA mRNA relations are closely linked to several biological processes and\ndisease mechanisms In a recent study we tested the performance of large\nlanguage models LLMs on extracting miRNA mRNA relations from PubMed PubMedBERT\nachieved the best performance of 0.783 F1 score for miRNA mRNA Interaction\nCorpus MMIC Here we first applied the finetuned PubMedBERT model to extract\nmiRNA mRNA relations from PubMed for chronic obstructive pulmonary disease COPD\nAlzheimers disease AD stroke type 2 diabetes mellitus T2DM chronic liver\ndisease and cancer Next we retrieved miRNA drug relations using KinderMiner a\nliterature mining tool for relation extraction Then we constructed three\ninteraction networks 1 disease centric network 2 drug centric network and 3\nmiRNA centric network comprising 3497 nodes and 16417 edges organized as a\ndirected graph to capture complex biological relationships Finally we validated\nthe drugs using MIMIC IV Our integrative approach revealed both established and\nnovel candidate drugs for diseases under study through 595 miRNA drug relations\nextracted from PubMed To the best of our knowledge this is the first study to\nsystematically extract and visualize relationships among four distinct\nbiomedical entities miRNA mRNA drug and disease",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "miRNA mRNA relations are closely linked to several biological processes and\ndisease mechanisms In a recent study we tested the performance of large\nlanguage models LLMs on extracting miRNA mRNA relations from PubMed PubMedBERT\nachieved the best performance of 0.783 F1 score for miRNA mRNA Interaction\nCorpus MMIC Here we first applied the finetuned PubMedBERT model to extract\nmiRNA mRNA relations from PubMed for chronic obstructive pulmonary disease COPD\nAlzheimers disease AD stroke type 2 diabetes mellitus T2DM chronic liver\ndisease and cancer Next we retrieved miRNA drug relations using KinderMiner a\nliterature mining tool for relation extraction Then we constructed three\ninteraction networks 1 disease centric network 2 drug centric network and 3\nmiRNA centric network comprising 3497 nodes and 16417 edges organized as a\ndirected graph to capture complex biological relationships Finally we validated\nthe drugs using MIMIC IV Our integrative approach revealed both established and\nnovel candidate drugs for diseases under study through 595 miRNA drug relations\nextracted from PubMed To the best of our knowledge this is the first study to\nsystematically extract and visualize relationships among four distinct\nbiomedical entities miRNA mRNA drug and disease"
                },
                "authors": [
                    {
                        "name": "Sharanya Manoharan"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Oviya Ramalakshmi Iyyappan"
                    },
                    {
                        "name": "Mohamed Saleem Abdul Shukkoor"
                    },
                    {
                        "name": "Malathi Sellapan"
                    },
                    {
                        "name": "Kalpana Raja"
                    }
                ],
                "author_detail": {
                    "name": "Kalpana Raja"
                },
                "author": "Kalpana Raja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03335v2",
                "updated": "2025-09-04T09:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    25,
                    5,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T14:10:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    10,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms"
                },
                "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03331v1",
                "updated": "2025-09-03T14:06:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    6,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:06:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    6,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing\n  Large Language Model Vulnerability Repair Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing\n  Large Language Model Vulnerability Repair Capabilities"
                },
                "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios."
                },
                "authors": [
                    {
                        "name": "Weizhe Wang"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Jianfei Sun"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Guangquan Xu"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03329v1",
                "updated": "2025-09-03T14:04:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    4,
                    51,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:04:51Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    4,
                    51,
                    2,
                    246,
                    0
                ],
                "title": "SESGO: Spanish Evaluation of Stereotypical Generative Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SESGO: Spanish Evaluation of Stereotypical Generative Outputs"
                },
                "summary": "This paper addresses the critical gap in evaluating bias in multilingual\nLarge Language Models (LLMs), with a specific focus on Spanish language within\nculturally-aware Latin American contexts. Despite widespread global deployment,\ncurrent evaluations remain predominantly US-English-centric, leaving potential\nharms in other linguistic and cultural contexts largely underexamined. We\nintroduce a novel, culturally-grounded framework for detecting social biases in\ninstruction-tuned LLMs. Our approach adapts the underspecified question\nmethodology from the BBQ dataset by incorporating culturally-specific\nexpressions and sayings that encode regional stereotypes across four social\ncategories: gender, race, socioeconomic class, and national origin. Using more\nthan 4,000 prompts, we propose a new metric that combines accuracy with the\ndirection of error to effectively balance model performance and bias alignment\nin both ambiguous and disambiguated contexts. To our knowledge, our work\npresents the first systematic evaluation examining how leading commercial LLMs\nrespond to culturally specific bias in the Spanish language, revealing varying\npatterns of bias manifestation across state-of-the-art models. We also\ncontribute evidence that bias mitigation techniques optimized for English do\nnot effectively transfer to Spanish tasks, and that bias patterns remain\nlargely consistent across different sampling temperatures. Our modular\nframework offers a natural extension to new stereotypes, bias categories, or\nlanguages and cultural contexts, representing a significant step toward more\nequitable and culturally-aware evaluation of AI systems in the diverse\nlinguistic environments where they operate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical gap in evaluating bias in multilingual\nLarge Language Models (LLMs), with a specific focus on Spanish language within\nculturally-aware Latin American contexts. Despite widespread global deployment,\ncurrent evaluations remain predominantly US-English-centric, leaving potential\nharms in other linguistic and cultural contexts largely underexamined. We\nintroduce a novel, culturally-grounded framework for detecting social biases in\ninstruction-tuned LLMs. Our approach adapts the underspecified question\nmethodology from the BBQ dataset by incorporating culturally-specific\nexpressions and sayings that encode regional stereotypes across four social\ncategories: gender, race, socioeconomic class, and national origin. Using more\nthan 4,000 prompts, we propose a new metric that combines accuracy with the\ndirection of error to effectively balance model performance and bias alignment\nin both ambiguous and disambiguated contexts. To our knowledge, our work\npresents the first systematic evaluation examining how leading commercial LLMs\nrespond to culturally specific bias in the Spanish language, revealing varying\npatterns of bias manifestation across state-of-the-art models. We also\ncontribute evidence that bias mitigation techniques optimized for English do\nnot effectively transfer to Spanish tasks, and that bias patterns remain\nlargely consistent across different sampling temperatures. Our modular\nframework offers a natural extension to new stereotypes, bias categories, or\nlanguages and cultural contexts, representing a significant step toward more\nequitable and culturally-aware evaluation of AI systems in the diverse\nlinguistic environments where they operate."
                },
                "authors": [
                    {
                        "name": "Melissa Robles"
                    },
                    {
                        "name": "Catalina Bernal"
                    },
                    {
                        "name": "Denniss Raigoso"
                    },
                    {
                        "name": "Mateo Dulce Rubio"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Dulce Rubio"
                },
                "author": "Mateo Dulce Rubio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03321v1",
                "updated": "2025-09-03T13:53:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    53,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:53:29Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    53,
                    29,
                    2,
                    246,
                    0
                ],
                "title": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT"
                },
                "summary": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Linyu Ou"
                    }
                ],
                "author_detail": {
                    "name": "Linyu Ou"
                },
                "author": "Linyu Ou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03317v2",
                "updated": "2025-09-04T12:40:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    40,
                    40,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T13:50:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    50,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Bayesian Additive Regression Trees for functional ANOVA model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Additive Regression Trees for functional ANOVA model"
                },
                "summary": "Bayesian Additive Regression Trees (BART) is a powerful statistical model\nthat leverages the strengths of Bayesian inference and regression trees. It has\nreceived significant attention for capturing complex non-linear relationships\nand interactions among predictors. However, the accuracy of BART often comes at\nthe cost of interpretability. To address this limitation, we propose ANOVA\nBayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART\nbased on the functional ANOVA decomposition, which is used to decompose the\nvariability of a function into different interactions, each representing the\ncontribution of a different set of covariates or factors. Our proposed\nANOVA-BART enhances interpretability, preserves and extends the theoretical\nguarantees of BART, and achieves superior predictive performance. Specifically,\nwe establish that the posterior concentration rate of ANOVA-BART is nearly\nminimax optimal, and further provides the same convergence rates for each\ninteraction that are not available for BART. Moreover, comprehensive\nexperiments confirm that ANOVA-BART surpasses BART in both accuracy and\nuncertainty quantification, while also demonstrating its effectiveness in\ncomponent selection. These results suggest that ANOVA-BART offers a compelling\nalternative to BART by balancing predictive accuracy, interpretability, and\ntheoretical consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Additive Regression Trees (BART) is a powerful statistical model\nthat leverages the strengths of Bayesian inference and regression trees. It has\nreceived significant attention for capturing complex non-linear relationships\nand interactions among predictors. However, the accuracy of BART often comes at\nthe cost of interpretability. To address this limitation, we propose ANOVA\nBayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART\nbased on the functional ANOVA decomposition, which is used to decompose the\nvariability of a function into different interactions, each representing the\ncontribution of a different set of covariates or factors. Our proposed\nANOVA-BART enhances interpretability, preserves and extends the theoretical\nguarantees of BART, and achieves superior predictive performance. Specifically,\nwe establish that the posterior concentration rate of ANOVA-BART is nearly\nminimax optimal, and further provides the same convergence rates for each\ninteraction that are not available for BART. Moreover, comprehensive\nexperiments confirm that ANOVA-BART surpasses BART in both accuracy and\nuncertainty quantification, while also demonstrating its effectiveness in\ncomponent selection. These results suggest that ANOVA-BART offers a compelling\nalternative to BART by balancing predictive accuracy, interpretability, and\ntheoretical consistency."
                },
                "authors": [
                    {
                        "name": "Seokhun Park"
                    },
                    {
                        "name": "Insung Kong"
                    },
                    {
                        "name": "Yongdai Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yongdai Kim"
                },
                "author": "Yongdai Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04387v2",
                "updated": "2025-09-03T13:46:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    46,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-05T21:36:21Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    36,
                    21,
                    2,
                    36,
                    0
                ],
                "title": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual\n  LLMs"
                },
                "summary": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft."
                },
                "authors": [
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Minyoung Kim"
                    },
                    {
                        "name": "Fady Rezk"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03315v1",
                "updated": "2025-09-03T13:44:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    44,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:44:53Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    44,
                    53,
                    2,
                    246,
                    0
                ],
                "title": "The super learner for time-to-event outcomes: A tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The super learner for time-to-event outcomes: A tutorial"
                },
                "summary": "Estimating risks or survival probabilities conditional on individual\ncharacteristics based on censored time-to-event data is a commonly faced task.\nThis may be for the purpose of developing a prediction model or may be part of\na wider estimation procedure, such as in causal inference. A challenge is that\nit is impossible to know at the outset which of a set of candidate models will\nprovide the best predictions. The super learner is a powerful approach for\nfinding the best model or combination of models ('ensemble') among a\npre-specified set of candidate models or 'learners', which can include\nparametric and machine learning models. Super learners for time-to-event\noutcomes have been developed, but the literature is technical and a reader may\nfind it challenging to gather together the full details of how these methods\nwork and can be implemented. In this paper we provide a practical tutorial on\nsuper learner methods for time-to-event outcomes. An overview of the general\nsteps involved in the super learner is given, followed by details of three\nspecific implementations for time-to-event outcomes. We cover discrete-time and\ncontinuous-time versions of the super learner, as described by Polley and van\nder Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare\nthe properties of the methods and provide information on how they can be\nimplemented in R. The methods are illustrated using an open access data set and\nR code is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating risks or survival probabilities conditional on individual\ncharacteristics based on censored time-to-event data is a commonly faced task.\nThis may be for the purpose of developing a prediction model or may be part of\na wider estimation procedure, such as in causal inference. A challenge is that\nit is impossible to know at the outset which of a set of candidate models will\nprovide the best predictions. The super learner is a powerful approach for\nfinding the best model or combination of models ('ensemble') among a\npre-specified set of candidate models or 'learners', which can include\nparametric and machine learning models. Super learners for time-to-event\noutcomes have been developed, but the literature is technical and a reader may\nfind it challenging to gather together the full details of how these methods\nwork and can be implemented. In this paper we provide a practical tutorial on\nsuper learner methods for time-to-event outcomes. An overview of the general\nsteps involved in the super learner is given, followed by details of three\nspecific implementations for time-to-event outcomes. We cover discrete-time and\ncontinuous-time versions of the super learner, as described by Polley and van\nder Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare\nthe properties of the methods and provide information on how they can be\nimplemented in R. The methods are illustrated using an open access data set and\nR code is provided."
                },
                "authors": [
                    {
                        "name": "Ruth H. Keogh"
                    },
                    {
                        "name": "Karla Diaz-Ordaz"
                    },
                    {
                        "name": "Nan van Geloven"
                    },
                    {
                        "name": "Jon Michael Gran"
                    },
                    {
                        "name": "Kamaryn T. Tanner"
                    }
                ],
                "author_detail": {
                    "name": "Kamaryn T. Tanner"
                },
                "author": "Kamaryn T. Tanner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03312v1",
                "updated": "2025-09-03T13:42:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"
                },
                "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03310v1",
                "updated": "2025-09-03T13:41:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    41,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:41:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    41,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding"
                },
                "summary": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems."
                },
                "authors": [
                    {
                        "name": "Evgenii Kniazev"
                    },
                    {
                        "name": "Arseny Kravchenko"
                    },
                    {
                        "name": "Igor Rekun"
                    },
                    {
                        "name": "James Broadhead"
                    },
                    {
                        "name": "Nikita Shamgunov"
                    },
                    {
                        "name": "Pranav Sah"
                    },
                    {
                        "name": "Pratik Nichite"
                    },
                    {
                        "name": "Ivan Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Yamshchikov"
                },
                "author": "Ivan Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11951v2",
                "updated": "2025-09-03T13:41:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    41,
                    36,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-17T16:04:04Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    4,
                    4,
                    0,
                    48,
                    0
                ],
                "title": "Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid\n  Quantum Classical Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid\n  Quantum Classical Machine Learning"
                },
                "summary": "The development of quantum computers has been the stimulus that enables the\nrealization of Quantum Machine Learning (QML), an area that integrates the\ncalculational framework of quantum mechanics with the adaptive properties of\nclassical machine learning. This article suggests a broad architecture that\nallows the connection between classical data pipelines and quantum algorithms,\nhybrid quantum-classical models emerge as a promising route to scalable and\nnear-term quantum benefit. At the core of this paradigm lies the\nClassical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional\nclassical data are encoded using sophisticated classical encoding strategies\nwhich encode the data in terms of amplitude and angle of rotation, along with\nsuperposition mapping. These techniques allow compression of information\nexponentially into Hilbert space representations, which, together with reduced\nsample complexity, allows greater feature expressivity. We also examine\nvariational quantum circuits, quantum gates expressed as trainable variables\nthat run with classical optimizers to overcome decoherence, noise, and\ngate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ)\ndevices. Experimental comparisons with a Quantum Naive Bayes classifier prove\nthat even small quantum circuits can approximate probabilistic inference with\ncompetitive accuracy compared to classical benchmarks, and have much better\nrobustness to noisy data distributionsThis model does not only explain the\nalgorithmic and architectural design of QML, it also offers a roadmap to the\nimplementation of quantum kernels, variational algorithms, and hybrid feedback\nloops into practice, including optimization, computer vision, and medical\ndiagnostics. The results support the idea that hybrid architectures with strong\ndata encoding and adaptive error protection are key to moving QML out of theory\nto practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of quantum computers has been the stimulus that enables the\nrealization of Quantum Machine Learning (QML), an area that integrates the\ncalculational framework of quantum mechanics with the adaptive properties of\nclassical machine learning. This article suggests a broad architecture that\nallows the connection between classical data pipelines and quantum algorithms,\nhybrid quantum-classical models emerge as a promising route to scalable and\nnear-term quantum benefit. At the core of this paradigm lies the\nClassical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional\nclassical data are encoded using sophisticated classical encoding strategies\nwhich encode the data in terms of amplitude and angle of rotation, along with\nsuperposition mapping. These techniques allow compression of information\nexponentially into Hilbert space representations, which, together with reduced\nsample complexity, allows greater feature expressivity. We also examine\nvariational quantum circuits, quantum gates expressed as trainable variables\nthat run with classical optimizers to overcome decoherence, noise, and\ngate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ)\ndevices. Experimental comparisons with a Quantum Naive Bayes classifier prove\nthat even small quantum circuits can approximate probabilistic inference with\ncompetitive accuracy compared to classical benchmarks, and have much better\nrobustness to noisy data distributionsThis model does not only explain the\nalgorithmic and architectural design of QML, it also offers a roadmap to the\nimplementation of quantum kernels, variational algorithms, and hybrid feedback\nloops into practice, including optimization, computer vision, and medical\ndiagnostics. The results support the idea that hybrid architectures with strong\ndata encoding and adaptive error protection are key to moving QML out of theory\nto practice."
                },
                "authors": [
                    {
                        "name": "Bhavna Bose"
                    },
                    {
                        "name": "Saurav Verma"
                    }
                ],
                "author_detail": {
                    "name": "Saurav Verma"
                },
                "author": "Saurav Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03307v1",
                "updated": "2025-09-03T13:39:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    39,
                    11,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    39,
                    11,
                    2,
                    246,
                    0
                ],
                "title": "Searching for HWW Anomalous Couplings with Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for HWW Anomalous Couplings with Simulation-Based Inference"
                },
                "summary": "Understanding the source of the universe's asymmetry between matter and\nantimatter is one of the major open questions in particle physics. In this\nwork, the sensitivity of novel machine-learning-based inference techniques to\nCP-odd and CP-even $HWW$ anomalous couplings is studied in the $\\\\WH\n\\rightarrow \\ell \\nu b\\bar{b}$ channel ($\\ell = e, \\mu$), within the Standard\nModel Effective Field Theory (SMEFT) framework. Two machine-learning\nsimulation-based inference (SBI) methods are explored: a per-event\nlikelihood-ratio estimator, which directly approximates the ratio of\nprobability densities between competing hypotheses, is benchmarked against a\nper-event optimal-observable estimator optimized for sensitivity to the\nparameters of interest. Both approaches are also compared to traditional\nsummary statistics, in this case histograms of kinematic and angular\nobservables, as commonly used in experimental analyses. SBI methods provide\ntighter constraints than one-dimensional summary statistics, though their\nperformance is comparable to two-dimensional histogram analysis. The\noptimal-observable approach remains promising for its ability to probe multiple\ncouplings simultaneously. Restricting the analysis to a region of high $S/B$\nalso enhances sensitivity to CP-odd operators while preserving sensitivity to\nCP-even operators, which histogram analyses often lose. Although the\nlikelihood-ratio estimator sometimes struggles with likelihood minima and\nshapes, optimisations that target its robustness could make it more sensitive\nthan both the optimal-observable estimator and the histogram method. These\nresults underscore the potential of advanced simulation-based inference\ntechniques, encouraging further exploration with LHC Run 3 data to surpass\ncurrent ATLAS and CMS sensitivities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the source of the universe's asymmetry between matter and\nantimatter is one of the major open questions in particle physics. In this\nwork, the sensitivity of novel machine-learning-based inference techniques to\nCP-odd and CP-even $HWW$ anomalous couplings is studied in the $\\\\WH\n\\rightarrow \\ell \\nu b\\bar{b}$ channel ($\\ell = e, \\mu$), within the Standard\nModel Effective Field Theory (SMEFT) framework. Two machine-learning\nsimulation-based inference (SBI) methods are explored: a per-event\nlikelihood-ratio estimator, which directly approximates the ratio of\nprobability densities between competing hypotheses, is benchmarked against a\nper-event optimal-observable estimator optimized for sensitivity to the\nparameters of interest. Both approaches are also compared to traditional\nsummary statistics, in this case histograms of kinematic and angular\nobservables, as commonly used in experimental analyses. SBI methods provide\ntighter constraints than one-dimensional summary statistics, though their\nperformance is comparable to two-dimensional histogram analysis. The\noptimal-observable approach remains promising for its ability to probe multiple\ncouplings simultaneously. Restricting the analysis to a region of high $S/B$\nalso enhances sensitivity to CP-odd operators while preserving sensitivity to\nCP-even operators, which histogram analyses often lose. Although the\nlikelihood-ratio estimator sometimes struggles with likelihood minima and\nshapes, optimisations that target its robustness could make it more sensitive\nthan both the optimal-observable estimator and the histogram method. These\nresults underscore the potential of advanced simulation-based inference\ntechniques, encouraging further exploration with LHC Run 3 data to surpass\ncurrent ATLAS and CMS sensitivities."
                },
                "authors": [
                    {
                        "name": "Marta Silva"
                    },
                    {
                        "name": "Ricardo Barrué"
                    },
                    {
                        "name": "Inês Ochoa"
                    },
                    {
                        "name": "Patricia Conde Muíño"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Conde Muíño"
                },
                "author": "Patricia Conde Muíño",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05668v4",
                "updated": "2025-09-03T13:30:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    30,
                    36,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-06T01:32:14Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    32,
                    14,
                    4,
                    157,
                    0
                ],
                "title": "RNE: plug-and-play diffusion inference-time control and energy-based\n  training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RNE: plug-and-play diffusion inference-time control and energy-based\n  training"
                },
                "summary": "Diffusion models generate data by removing noise gradually, which corresponds\nto the time-reversal of a noising process. However, access to only the\ndenoising kernels is often insufficient. In many applications, we need the\nknowledge of the marginal densities along the generation trajectory, which\nenables tasks such as inference-time control. To address this gap, in this\npaper, we introduce the Radon-Nikodym Estimator (RNE). Based on the concept of\nthe density ratio between path distributions, it reveals a fundamental\nconnection between marginal densities and transition kernels, providing a\nflexible plug-and-play framework that unifies diffusion density estimation,\ninference-time control, and energy-based diffusion training under a single\nperspective. Experiments demonstrated that RNE delivers strong results in\ninference-time control applications, such as annealing and model composition,\nwith promising inference-time scaling performance. Moreover, RNE provides a\nsimple yet efficient regularisation for training energy-based diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models generate data by removing noise gradually, which corresponds\nto the time-reversal of a noising process. However, access to only the\ndenoising kernels is often insufficient. In many applications, we need the\nknowledge of the marginal densities along the generation trajectory, which\nenables tasks such as inference-time control. To address this gap, in this\npaper, we introduce the Radon-Nikodym Estimator (RNE). Based on the concept of\nthe density ratio between path distributions, it reveals a fundamental\nconnection between marginal densities and transition kernels, providing a\nflexible plug-and-play framework that unifies diffusion density estimation,\ninference-time control, and energy-based diffusion training under a single\nperspective. Experiments demonstrated that RNE delivers strong results in\ninference-time control applications, such as annealing and model composition,\nwith promising inference-time scaling performance. Moreover, RNE provides a\nsimple yet efficient regularisation for training energy-based diffusion."
                },
                "authors": [
                    {
                        "name": "Jiajun He"
                    },
                    {
                        "name": "José Miguel Hernández-Lobato"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Francisco Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Vargas"
                },
                "author": "Francisco Vargas",
                "arxiv_comment": "48 pages; 15 figures; Add more experiments on energy-based training,\n  fix several typos and an error in RNC-TDS paragraph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03303v1",
                "updated": "2025-09-03T13:28:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    28,
                    33,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:28:33Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    28,
                    33,
                    2,
                    246,
                    0
                ],
                "title": "Automatic Differentiation of Agent-Based Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Differentiation of Agent-Based Models"
                },
                "summary": "Agent-based models (ABMs) simulate complex systems by capturing the bottom-up\ninteractions of individual agents comprising the system. Many complex systems\nof interest, such as epidemics or financial markets, involve thousands or even\nmillions of agents. Consequently, ABMs often become computationally demanding\nand rely on the calibration of numerous free parameters, which has\nsignificantly hindered their widespread adoption. In this paper, we demonstrate\nthat automatic differentiation (AD) techniques can effectively alleviate these\ncomputational burdens. By applying AD to ABMs, the gradients of the simulator\nbecome readily available, greatly facilitating essential tasks such as\ncalibration and sensitivity analysis. Specifically, we show how AD enables\nvariational inference (VI) techniques for efficient parameter calibration. Our\nexperiments demonstrate substantial performance improvements and computational\nsavings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;\nand the SIR epidemiological model. Our approach thus significantly enhances the\npracticality and scalability of ABMs for studying complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based models (ABMs) simulate complex systems by capturing the bottom-up\ninteractions of individual agents comprising the system. Many complex systems\nof interest, such as epidemics or financial markets, involve thousands or even\nmillions of agents. Consequently, ABMs often become computationally demanding\nand rely on the calibration of numerous free parameters, which has\nsignificantly hindered their widespread adoption. In this paper, we demonstrate\nthat automatic differentiation (AD) techniques can effectively alleviate these\ncomputational burdens. By applying AD to ABMs, the gradients of the simulator\nbecome readily available, greatly facilitating essential tasks such as\ncalibration and sensitivity analysis. Specifically, we show how AD enables\nvariational inference (VI) techniques for efficient parameter calibration. Our\nexperiments demonstrate substantial performance improvements and computational\nsavings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;\nand the SIR epidemiological model. Our approach thus significantly enhances the\npracticality and scalability of ABMs for studying complex systems."
                },
                "authors": [
                    {
                        "name": "Arnau Quera-Bofarull"
                    },
                    {
                        "name": "Nicholas Bishop"
                    },
                    {
                        "name": "Joel Dyer"
                    },
                    {
                        "name": "Daniel Jarne Ornia"
                    },
                    {
                        "name": "Anisoara Calinescu"
                    },
                    {
                        "name": "Doyne Farmer"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01326v2",
                "updated": "2025-09-03T12:09:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    12,
                    9,
                    34,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-02T05:11:21Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    11,
                    21,
                    0,
                    153,
                    0
                ],
                "title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for\n  Operations Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for\n  Operations Research"
                },
                "summary": "Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Bokui Chen"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Qingxing Cao"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Accepted by Annual Meetings of the Association for Computational\n  Linguistics 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03249v2",
                "updated": "2025-09-04T08:55:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    55,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T12:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    12,
                    7,
                    23,
                    2,
                    246,
                    0
                ],
                "title": "Structure Transfer: an Inference-Based Calculus for the Transformation\n  of Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure Transfer: an Inference-Based Calculus for the Transformation\n  of Representations"
                },
                "summary": "Representation choice is of fundamental importance to our ability to\ncommunicate and reason effectively. A major unsolved problem, addressed in this\npaper, is how to devise representational-system (RS) agnostic techniques that\ndrive representation transformation and choice. We present a novel calculus,\ncalled structure transfer, that enables representation transformation across\ndiverse RSs. Specifically, given a source representation drawn from a source\nRS, the rules of structure transfer allow us to generate a target\nrepresentation for a target RS. The generality of structure transfer comes in\npart from its ability to ensure that the source representation and the\ngenerated target representation satisfy any specified relation (such as\nsemantic equivalence). This is done by exploiting schemas, which encode\nknowledge about RSs. Specifically, schemas can express preservation of\ninformation across relations between any pair of RSs, and this knowledge is\nused by structure transfer to derive a structure for the target representation\nwhich ensures that the desired relation holds. We formalise this using\nRepresentational Systems Theory, building on the key concept of a construction\nspace. The abstract nature of construction spaces grants them the generality to\nmodel RSs of diverse kinds, including formal languages, geometric figures and\ndiagrams, as well as informal notations. Consequently, structure transfer is a\nsystem-agnostic calculus that can be used to identify alternative\nrepresentations in a wide range of practical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation choice is of fundamental importance to our ability to\ncommunicate and reason effectively. A major unsolved problem, addressed in this\npaper, is how to devise representational-system (RS) agnostic techniques that\ndrive representation transformation and choice. We present a novel calculus,\ncalled structure transfer, that enables representation transformation across\ndiverse RSs. Specifically, given a source representation drawn from a source\nRS, the rules of structure transfer allow us to generate a target\nrepresentation for a target RS. The generality of structure transfer comes in\npart from its ability to ensure that the source representation and the\ngenerated target representation satisfy any specified relation (such as\nsemantic equivalence). This is done by exploiting schemas, which encode\nknowledge about RSs. Specifically, schemas can express preservation of\ninformation across relations between any pair of RSs, and this knowledge is\nused by structure transfer to derive a structure for the target representation\nwhich ensures that the desired relation holds. We formalise this using\nRepresentational Systems Theory, building on the key concept of a construction\nspace. The abstract nature of construction spaces grants them the generality to\nmodel RSs of diverse kinds, including formal languages, geometric figures and\ndiagrams, as well as informal notations. Consequently, structure transfer is a\nsystem-agnostic calculus that can be used to identify alternative\nrepresentations in a wide range of practical settings."
                },
                "authors": [
                    {
                        "name": "Daniel Raggi"
                    },
                    {
                        "name": "Gem Stapleton"
                    },
                    {
                        "name": "Mateja Jamnik"
                    },
                    {
                        "name": "Aaron Stockdill"
                    },
                    {
                        "name": "Grecia Garcia Garcia"
                    },
                    {
                        "name": "Peter C-H. Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peter C-H. Cheng"
                },
                "author": "Peter C-H. Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T27, 03B35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.3; F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03234v1",
                "updated": "2025-09-03T11:46:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    46,
                    24,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:46:24Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    46,
                    24,
                    2,
                    246,
                    0
                ],
                "title": "TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of\n  Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have significantly reduced the number of trainable parameters needed in\nfine-tuning large language models (LLMs). Subsequent developments of LoRA-style\nadapters have diverged into two main directions: (1) enhancing model\nexpressivity with high-rank adapters, and (2) pushing for further parameter\nreduction, as exemplified by vector-based methods. However, these approaches\npresent a trade-off, as achieving the expressivity of high-rank weight updates\ntypically comes at the cost of sacrificing the extreme parameter efficiency\noffered by vector-based techniques. To address this issue, we propose a\nvector-based random \\underline{\\textbf{Te}}nsor network for\nhigh-\\underline{\\textbf{R}}ank \\underline{\\textbf{A}}daptation (TeRA), a novel\nPEFT method that achieves high-rank weight updates while retaining the\nparameter efficiency of vector-based PEFT adapters. This is achieved by\nparameterizing the tensorized weight update matrix as a Tucker-like tensor\nnetwork (TN), in which large randomly initialized factors are frozen and shared\nacross layers, while only small layer-specific scaling vectors, formed by\nentries in diagonal factor matrices, are trained. This design effectively\ndecouples the rank of the weight update matrix from the number of trainable\nparameters. Comprehensive experiments demonstrate that TeRA matches or even\noutperforms high-rank adapters, while requiring a trainable parameter count\nsimilar to vector-based methods. Theoretical analysis and ablation studies\nfurther validate the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have significantly reduced the number of trainable parameters needed in\nfine-tuning large language models (LLMs). Subsequent developments of LoRA-style\nadapters have diverged into two main directions: (1) enhancing model\nexpressivity with high-rank adapters, and (2) pushing for further parameter\nreduction, as exemplified by vector-based methods. However, these approaches\npresent a trade-off, as achieving the expressivity of high-rank weight updates\ntypically comes at the cost of sacrificing the extreme parameter efficiency\noffered by vector-based techniques. To address this issue, we propose a\nvector-based random \\underline{\\textbf{Te}}nsor network for\nhigh-\\underline{\\textbf{R}}ank \\underline{\\textbf{A}}daptation (TeRA), a novel\nPEFT method that achieves high-rank weight updates while retaining the\nparameter efficiency of vector-based PEFT adapters. This is achieved by\nparameterizing the tensorized weight update matrix as a Tucker-like tensor\nnetwork (TN), in which large randomly initialized factors are frozen and shared\nacross layers, while only small layer-specific scaling vectors, formed by\nentries in diagonal factor matrices, are trained. This design effectively\ndecouples the rank of the weight update matrix from the number of trainable\nparameters. Comprehensive experiments demonstrate that TeRA matches or even\noutperforms high-rank adapters, while requiring a trainable parameter count\nsimilar to vector-based methods. Theoretical analysis and ablation studies\nfurther validate the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13257v3",
                "updated": "2025-09-03T11:11:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    11,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-11-20T12:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    14,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Inference by Multiple Identical Observers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference by Multiple Identical Observers"
                },
                "summary": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely."
                },
                "authors": [
                    {
                        "name": "Martin T. Barlow"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Barlow"
                },
                "author": "Martin T. Barlow",
                "arxiv_comment": "This version (v3) has a new title, and is shorter and more\n  mathematical than v2. Most of sections 4,5,6,7 of v2 have been omitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62A01 Secondary 60A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05225v2",
                "updated": "2025-09-03T11:11:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    11,
                    28,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-08T13:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation"
                },
                "summary": "The rapid advancement of Chinese LLMs underscores the need for\nvertical-domain evaluations to ensure reliable applications. However, existing\nbenchmarks often lack domain coverage and provide limited insights into the\nChinese working context. Leveraging qualification exams as a unified framework\nfor expertise evaluation, we introduce QualBench, the first multi-domain\nChinese QA benchmark dedicated to localized assessment of Chinese LLMs. The\ndataset includes over 17,000 questions across six vertical domains, drawn from\n24 Chinese qualifications to align with national policies and professional\nstandards. Results reveal an interesting pattern of Chinese LLMs consistently\nsurpassing non-Chinese models, with the Qwen2.5 model outperforming the more\nadvanced GPT-4o, emphasizing the value of localized domain knowledge in meeting\nqualification requirements. The average accuracy of 53.98% reveals the current\ngaps in domain coverage within model capabilities. Furthermore, we identify\nperformance degradation caused by LLM crowdsourcing, assess data contamination,\nand illustrate the effectiveness of prompt engineering and model fine-tuning,\nsuggesting opportunities for future improvements through multi-domain RAG and\nFederated Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Chinese LLMs underscores the need for\nvertical-domain evaluations to ensure reliable applications. However, existing\nbenchmarks often lack domain coverage and provide limited insights into the\nChinese working context. Leveraging qualification exams as a unified framework\nfor expertise evaluation, we introduce QualBench, the first multi-domain\nChinese QA benchmark dedicated to localized assessment of Chinese LLMs. The\ndataset includes over 17,000 questions across six vertical domains, drawn from\n24 Chinese qualifications to align with national policies and professional\nstandards. Results reveal an interesting pattern of Chinese LLMs consistently\nsurpassing non-Chinese models, with the Qwen2.5 model outperforming the more\nadvanced GPT-4o, emphasizing the value of localized domain knowledge in meeting\nqualification requirements. The average accuracy of 53.98% reveals the current\ngaps in domain coverage within model capabilities. Furthermore, we identify\nperformance degradation caused by LLM crowdsourcing, assess data contamination,\nand illustrate the effectiveness of prompt engineering and model fine-tuning,\nsuggesting opportunities for future improvements through multi-domain RAG and\nFederated Learning."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "arxiv_comment": "Accepted by EMNLP 2025 Main Conference. Homepage:\n  https://github.com/mengze-hong/QualBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09049v4",
                "updated": "2025-09-03T11:09:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    9,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2024-12-12T08:19:01Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    19,
                    1,
                    3,
                    347,
                    0
                ],
                "title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues"
                },
                "summary": "Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "arxiv_comment": "Accepted by EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03212v1",
                "updated": "2025-09-03T11:00:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    0,
                    46,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:00:46Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    0,
                    46,
                    2,
                    246,
                    0
                ],
                "title": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have significantly improved\nnatural language understanding and generation, enhancing Human-Computer\nInteraction (HCI). However, LLMs are limited to unimodal text processing and\nlack the ability to interpret emotional cues from non-verbal signals, hindering\nmore immersive and empathetic interactions. This work explores integrating\nmultimodal sentiment perception into LLMs to create emotion-aware agents. We\npropose \\ours, an AI-based virtual companion that captures multimodal sentiment\ncues, enabling emotionally aligned and animated HCI. \\ours introduces a\nMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusion\ntransformer and supervised contrastive learning to provide emotional cues.\nAdditionally, we develop an emotion-aware prompt engineering strategy for\ngenerating empathetic responses and integrate a Text-to-Speech (TTS) system and\nanimated avatar module for expressive interactions. \\ours provides a framework\nfor emotion-aware agents with applications in companion robotics, social care,\nmental health, and human-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have significantly improved\nnatural language understanding and generation, enhancing Human-Computer\nInteraction (HCI). However, LLMs are limited to unimodal text processing and\nlack the ability to interpret emotional cues from non-verbal signals, hindering\nmore immersive and empathetic interactions. This work explores integrating\nmultimodal sentiment perception into LLMs to create emotion-aware agents. We\npropose \\ours, an AI-based virtual companion that captures multimodal sentiment\ncues, enabling emotionally aligned and animated HCI. \\ours introduces a\nMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusion\ntransformer and supervised contrastive learning to provide emotional cues.\nAdditionally, we develop an emotion-aware prompt engineering strategy for\ngenerating empathetic responses and integrate a Text-to-Speech (TTS) system and\nanimated avatar module for expressive interactions. \\ours provides a framework\nfor emotion-aware agents with applications in companion robotics, social care,\nmental health, and human-centered AI."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Li"
                },
                "author": "Chenxi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02327v2",
                "updated": "2025-09-03T10:56:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    56,
                    13,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T13:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    53,
                    9,
                    1,
                    245,
                    0
                ],
                "title": "Variational Uncertainty Decomposition for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Uncertainty Decomposition for In-Context Learning"
                },
                "summary": "As large language models (LLMs) gain popularity in conducting prediction\ntasks in-context, understanding the sources of uncertainty in in-context\nlearning becomes essential to ensuring reliability. The recent hypothesis of\nin-context learning performing predictive Bayesian inference opens the avenue\nfor Bayesian uncertainty estimation, particularly for decomposing uncertainty\ninto epistemic uncertainty due to lack of in-context data and aleatoric\nuncertainty inherent in the in-context prediction task. However, the\ndecomposition idea remains under-explored due to the intractability of the\nlatent parameter posterior from the underlying Bayesian model. In this work, we\nintroduce a variational uncertainty decomposition framework for in-context\nlearning without explicitly sampling from the latent parameter posterior, by\noptimising auxiliary queries as probes to obtain an upper bound to the\naleatoric uncertainty of an LLM's in-context learning procedure, which also\ninduces a lower bound to the epistemic uncertainty. Through experiments on\nsynthetic and real-world tasks, we show quantitatively and qualitatively that\nthe decomposed uncertainties obtained from our method exhibit desirable\nproperties of epistemic and aleatoric uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain popularity in conducting prediction\ntasks in-context, understanding the sources of uncertainty in in-context\nlearning becomes essential to ensuring reliability. The recent hypothesis of\nin-context learning performing predictive Bayesian inference opens the avenue\nfor Bayesian uncertainty estimation, particularly for decomposing uncertainty\ninto epistemic uncertainty due to lack of in-context data and aleatoric\nuncertainty inherent in the in-context prediction task. However, the\ndecomposition idea remains under-explored due to the intractability of the\nlatent parameter posterior from the underlying Bayesian model. In this work, we\nintroduce a variational uncertainty decomposition framework for in-context\nlearning without explicitly sampling from the latent parameter posterior, by\noptimising auxiliary queries as probes to obtain an upper bound to the\naleatoric uncertainty of an LLM's in-context learning procedure, which also\ninduces a lower bound to the epistemic uncertainty. Through experiments on\nsynthetic and real-world tasks, we show quantitatively and qualitatively that\nthe decomposed uncertainties obtained from our method exhibit desirable\nproperties of epistemic and aleatoric uncertainty."
                },
                "authors": [
                    {
                        "name": "I. Shavindra Jayasekera"
                    },
                    {
                        "name": "Jacob Si"
                    },
                    {
                        "name": "Filippo Valdettaro"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "Fixing author order; typo p.20",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11622v2",
                "updated": "2025-09-03T10:52:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    52,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-15T17:48:36Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    48,
                    36,
                    4,
                    227,
                    0
                ],
                "title": "Deconfounding via Profiled Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconfounding via Profiled Transfer Learning"
                },
                "summary": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Ziyuan Chen"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Fang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yao"
                },
                "author": "Fang Yao",
                "arxiv_comment": "We need to further refine this paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09997v3",
                "updated": "2025-09-03T10:48:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    48,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-01-17T07:30:01Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models"
                },
                "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02170v2",
                "updated": "2025-09-03T10:39:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    39,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T10:22:46Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    10,
                    22,
                    46,
                    1,
                    245,
                    0
                ],
                "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoidance Decoding for Diverse Multi-Branch Story Generation"
                },
                "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity."
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Nakyeong Yang"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16822v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16822v5",
                "updated": "2025-09-03T10:35:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    35,
                    17,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-22T08:48:52Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    48,
                    52,
                    1,
                    296,
                    0
                ],
                "title": "Can Large Language Models Act as Ensembler for Multi-GNNs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Act as Ensembler for Multi-GNNs?"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://github.com/AquariusAQ/LensGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://github.com/AquariusAQ/LensGNN."
                },
                "authors": [
                    {
                        "name": "Hanqi Duan"
                    },
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16822v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16822v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03194v1",
                "updated": "2025-09-03T10:28:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    28,
                    25,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T10:28:25Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    28,
                    25,
                    2,
                    246,
                    0
                ],
                "title": "Bayesian Network Propensity Score to Evaluate Treatment Effects in\n  Observational Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Network Propensity Score to Evaluate Treatment Effects in\n  Observational Studies"
                },
                "summary": "This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel\napproach for estimating treatment effects in observational studies\ncharacterized by unknown (and likely unbalanced) designs and complex dependency\nstructures among covariates. Traditional methods, such as logistic regression,\noften impose rigid parametric assumptions that may lead to misspecification\nerrors, compromising causal inference. Recent classical and machine learning\nalternatives, such as boosted CART, random forests, and Stable Balancing\nWeights, seem to be attractive in a predictive perspective, but they typically\nlack asymptotic properties, such as consistency, efficiency, and valid variance\nestimation. In contrast, the recently proposed BNPS to estimate propensity\nscores uses Bayesian Networks to flexibly model conditional dependencies while\npreserving essential statistical properties such as consistency, asymptotic\nnormality and asymptotic efficiency. Combined with the H\\'ajek estimator, BNPS\nenables robust estimation of the Average Treatment Effect (ATE) in scenarios\nwith strong covariate interactions and unknown data-generating mechanisms.\nThrough extensive simulations across fifteen realistic scenarios and varying\nsample sizes, BNPS consistently outperforms benchmark methods in both empirical\nrejection rates and coverage accuracy. Finally, an application to a real-world\ndataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan,\nItaly) demonstrates BNPS's practical value in assessing the impact of pelvic\nlymph node dissection on hospitalization duration and biochemical recurrence.\nThe findings support BNPS as a statistically robust, interpretable and\ntransparent alternative for causal inference in complex observational settings,\nenhancing the reliability of evidence from real-world biomedical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel\napproach for estimating treatment effects in observational studies\ncharacterized by unknown (and likely unbalanced) designs and complex dependency\nstructures among covariates. Traditional methods, such as logistic regression,\noften impose rigid parametric assumptions that may lead to misspecification\nerrors, compromising causal inference. Recent classical and machine learning\nalternatives, such as boosted CART, random forests, and Stable Balancing\nWeights, seem to be attractive in a predictive perspective, but they typically\nlack asymptotic properties, such as consistency, efficiency, and valid variance\nestimation. In contrast, the recently proposed BNPS to estimate propensity\nscores uses Bayesian Networks to flexibly model conditional dependencies while\npreserving essential statistical properties such as consistency, asymptotic\nnormality and asymptotic efficiency. Combined with the H\\'ajek estimator, BNPS\nenables robust estimation of the Average Treatment Effect (ATE) in scenarios\nwith strong covariate interactions and unknown data-generating mechanisms.\nThrough extensive simulations across fifteen realistic scenarios and varying\nsample sizes, BNPS consistently outperforms benchmark methods in both empirical\nrejection rates and coverage accuracy. Finally, an application to a real-world\ndataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan,\nItaly) demonstrates BNPS's practical value in assessing the impact of pelvic\nlymph node dissection on hospitalization duration and biochemical recurrence.\nThe findings support BNPS as a statistically robust, interpretable and\ntransparent alternative for causal inference in complex observational settings,\nenhancing the reliability of evidence from real-world biomedical data."
                },
                "authors": [
                    {
                        "name": "Clelia Di Serio"
                    },
                    {
                        "name": "Federica Cugnata"
                    },
                    {
                        "name": "Pier Luigi Conti"
                    },
                    {
                        "name": "Alberto Briganti"
                    },
                    {
                        "name": "Fulvia Mecatti"
                    },
                    {
                        "name": "Paola Vicard"
                    },
                    {
                        "name": "Paola Maria Vittoria Rancoita"
                    }
                ],
                "author_detail": {
                    "name": "Paola Maria Vittoria Rancoita"
                },
                "author": "Paola Maria Vittoria Rancoita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03191v1",
                "updated": "2025-09-03T10:21:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    21,
                    18,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T10:21:18Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    21,
                    18,
                    2,
                    246,
                    0
                ],
                "title": "Tabular foundation model for GEOAI benchmark problems\n  BM/AirportSoilProperties/2/2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation model for GEOAI benchmark problems\n  BM/AirportSoilProperties/2/2025"
                },
                "summary": "This paper presents a novel application of the Tabular Prior-Data Fitted\nNetwork (TabPFN) - a transformer-based foundation model for tabular data - to\ngeotechnical site characterization problems defined in the GEOAI benchmark\nBM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the\nspatial variation of undrained shear strength (su) across borehole depth\nprofiles, and (2) imputing missing mechanical parameters in a dense-site\ndataset. We apply TabPFN in a zero-training, few-shot, in-context learning\nsetting - without hyper-parameter tuning - and provide it with additional\ncontext from the big indirect database (BID). The study demonstrates that\nTabPFN, as a general-purpose foundation model, achieved superior accuracy and\nwell-calibrated predictive distributions compared to a conventional\nhierarchical Bayesian model (HBM) baseline, while also offering significant\ngains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),\nTabPFN outperformed the HBM in prediction accuracy and delivered an\norder-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical\nparameter imputation), TabPFN likewise achieved lower RMSE for all target\nparameters with well-quantified uncertainties, though its cumulative\ncomputation cost was higher than HBM's due to its one-variable-at-a-time\ninference. These results mark the first successful use of a tabular foundation\nmodel in geotechnical modeling, suggesting a potential paradigm shift in\nprobabilistic site characterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel application of the Tabular Prior-Data Fitted\nNetwork (TabPFN) - a transformer-based foundation model for tabular data - to\ngeotechnical site characterization problems defined in the GEOAI benchmark\nBM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the\nspatial variation of undrained shear strength (su) across borehole depth\nprofiles, and (2) imputing missing mechanical parameters in a dense-site\ndataset. We apply TabPFN in a zero-training, few-shot, in-context learning\nsetting - without hyper-parameter tuning - and provide it with additional\ncontext from the big indirect database (BID). The study demonstrates that\nTabPFN, as a general-purpose foundation model, achieved superior accuracy and\nwell-calibrated predictive distributions compared to a conventional\nhierarchical Bayesian model (HBM) baseline, while also offering significant\ngains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),\nTabPFN outperformed the HBM in prediction accuracy and delivered an\norder-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical\nparameter imputation), TabPFN likewise achieved lower RMSE for all target\nparameters with well-quantified uncertainties, though its cumulative\ncomputation cost was higher than HBM's due to its one-variable-at-a-time\ninference. These results mark the first successful use of a tabular foundation\nmodel in geotechnical modeling, suggesting a potential paradigm shift in\nprobabilistic site characterization."
                },
                "authors": [
                    {
                        "name": "Taiga Saito"
                    },
                    {
                        "name": "Yu Otake"
                    },
                    {
                        "name": "Stephen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Wu"
                },
                "author": "Stephen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17695v2",
                "updated": "2025-09-03T09:49:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    49,
                    56,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-23T17:01:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    1,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks"
                },
                "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance."
                },
                "authors": [
                    {
                        "name": "Ilias Chatzistefanidis"
                    },
                    {
                        "name": "Navid Nikaein"
                    }
                ],
                "author_detail": {
                    "name": "Navid Nikaein"
                },
                "author": "Navid Nikaein",
                "arxiv_comment": "Submitted to Computer Networks AI for 6G",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19828v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19828v3",
                "updated": "2025-09-03T09:33:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    33,
                    30,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T12:26:55Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    26,
                    55,
                    2,
                    239,
                    0
                ],
                "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations,\nincluding adding, updating, deleting, or taking no operation on memory entries;\nand an Answer Agent that selects the most relevant entries and reasons over\nthem to produce an answer. Both agents are fine-tuned with outcome-driven RL\n(PPO and GRPO), enabling adaptive memory management and utilization with\nminimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the\nstrongest existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behavior in LLMs, pointing toward richer, more persistent\nreasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations,\nincluding adding, updating, deleting, or taking no operation on memory entries;\nand an Answer Agent that selects the most relevant entries and reasons over\nthem to produce an answer. Both agents are fine-tuned with outcome-driven RL\n(PPO and GRPO), enabling adaptive memory management and utilization with\nminimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the\nstrongest existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behavior in LLMs, pointing toward richer, more persistent\nreasoning systems."
                },
                "authors": [
                    {
                        "name": "Sikuan Yan"
                    },
                    {
                        "name": "Xiufeng Yang"
                    },
                    {
                        "name": "Zuchao Huang"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Zonggen Li"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19828v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19828v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01415v3",
                "updated": "2025-09-03T09:32:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    27,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-02T15:39:42Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    15,
                    39,
                    42,
                    5,
                    214,
                    0
                ],
                "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems"
                },
                "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots."
                },
                "authors": [
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Honghao Cai"
                    },
                    {
                        "name": "Binbin Que"
                    },
                    {
                        "name": "Zezhou Cui"
                    },
                    {
                        "name": "Liangchen Tan"
                    },
                    {
                        "name": "Junkun Hong"
                    },
                    {
                        "name": "Gehan Hu"
                    },
                    {
                        "name": "Shuangyu Zhu"
                    },
                    {
                        "name": "Yimou Wu"
                    },
                    {
                        "name": "Shaohan Jiang"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Yatong Han"
                    }
                ],
                "author_detail": {
                    "name": "Yatong Han"
                },
                "author": "Yatong Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03165v1",
                "updated": "2025-09-03T09:32:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    13,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:32:13Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    13,
                    2,
                    246,
                    0
                ],
                "title": "PatchNet: A hierarchical approach for neural field-level inference from\n  Quijote Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchNet: A hierarchical approach for neural field-level inference from\n  Quijote Simulations"
                },
                "summary": "\\textit{What is the cosmological information content of a cubic Gigaparsec of\ndark matter? } Extracting cosmological information from the non-linear matter\ndistribution has high potential to tighten parameter constraints in the era of\nnext-generation surveys such as Euclid, DESI, and the Vera Rubin Observatory.\nTraditional approaches relying on summary statistics like the power spectrum\nand bispectrum, though analytically tractable, fail to capture the full\nnon-Gaussian and non-linear structure of the density field. Simulation-Based\nInference (SBI) provides a powerful alternative by learning directly from\nforward-modeled simulations. In this work, we apply SBI to the \\textit{Quijote}\ndark matter simulations and introduce a hierarchical method that integrates\nsmall-scale information from field sub-volumes or \\textit{patches} with\nlarge-scale statistics such as power spectrum and bispectrum. This hybrid\nstrategy is efficient both computationally and in terms of the amount of\ntraining data required. It overcomes the memory limitations associated with\nfull-field training. We show that our approach enhances Fisher information\nrelative to analytical summaries and matches that of a very different approach\n(wavelet-based statistics), providing evidence that we are estimating the full\ninformation content of the dark matter density field at the resolution of $\\sim\n7.8~\\mathrm{Mpc}/h$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textit{What is the cosmological information content of a cubic Gigaparsec of\ndark matter? } Extracting cosmological information from the non-linear matter\ndistribution has high potential to tighten parameter constraints in the era of\nnext-generation surveys such as Euclid, DESI, and the Vera Rubin Observatory.\nTraditional approaches relying on summary statistics like the power spectrum\nand bispectrum, though analytically tractable, fail to capture the full\nnon-Gaussian and non-linear structure of the density field. Simulation-Based\nInference (SBI) provides a powerful alternative by learning directly from\nforward-modeled simulations. In this work, we apply SBI to the \\textit{Quijote}\ndark matter simulations and introduce a hierarchical method that integrates\nsmall-scale information from field sub-volumes or \\textit{patches} with\nlarge-scale statistics such as power spectrum and bispectrum. This hybrid\nstrategy is efficient both computationally and in terms of the amount of\ntraining data required. It overcomes the memory limitations associated with\nfull-field training. We show that our approach enhances Fisher information\nrelative to analytical summaries and matches that of a very different approach\n(wavelet-based statistics), providing evidence that we are estimating the full\ninformation content of the dark matter density field at the resolution of $\\sim\n7.8~\\mathrm{Mpc}/h$."
                },
                "authors": [
                    {
                        "name": "Anirban Bairagi"
                    },
                    {
                        "name": "Benjamin Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wandelt"
                },
                "author": "Benjamin Wandelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08040v2",
                "updated": "2025-09-03T09:32:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-11T14:42:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models"
                },
                "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Maozhen Zhang"
                    },
                    {
                        "name": "Mengnan Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03164v1",
                "updated": "2025-09-03T09:30:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:30:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models"
                },
                "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."
                },
                "authors": [
                    {
                        "name": "Sangbong Yoo"
                    },
                    {
                        "name": "Seongbum Seo"
                    },
                    {
                        "name": "Chanyoung Yoon"
                    },
                    {
                        "name": "Hyelim Lee"
                    },
                    {
                        "name": "Jeong-Nam Kim"
                    },
                    {
                        "name": "Chansoo Kim"
                    },
                    {
                        "name": "Yun Jang"
                    },
                    {
                        "name": "Takanori Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takanori Fujiwara"
                },
                "author": "Takanori Fujiwara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02776v2",
                "updated": "2025-09-03T09:25:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    25,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-04T18:00:00Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    18,
                    0,
                    0,
                    0,
                    216,
                    0
                ],
                "title": "Investigation of mass substructure in gravitational lens system SDP.81\n  with ALMA long-baseline observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigation of mass substructure in gravitational lens system SDP.81\n  with ALMA long-baseline observations"
                },
                "summary": "The prevalence and properties of low-mass dark matter haloes serve as a\ncrucial test for understanding the nature of dark matter, and may be\nconstrained through the gravitational deflection of strongly lensed arcs.\nPrevious studies found evidence for the presence of low-mass dark matter haloes\nin observations of the gravitationally lensed, dusty star-forming galaxy\nSDP.81, using the Atacama Large Millimetre/sub-millimetre Array (ALMA). In this\nwork, we analyse these observations to assess the robustness of these reported\nresults. While our analysis indicates that the data support additional angular\nstructure in the lensing mass distribution beyond an elliptical power-law\ndensity profile, we do not find evidence for two previously reported sub-halo\ndetections. However, we verify with realistic mock data that we could have\nfound evidence in favour of a previously reported $\\approx 10^{9}\\,{\\rm\nM_{\\odot}}$ sub-halo with a log Bayes factor of 29, should it exist in the real\ndata. After testing various systematics, we find that this previous sub-halo\ninference was most likely spurious and resulted from an inadequate smooth\nmodel, specifically, poorly fitting multipoles. While we do not find evidence\nin favour of any individual sub-halo, we find evidence for similarity in the\nlensing signatures of multipoles ($m=3,4$) and single massive sub-haloes,\nconsistent with other recent work. We suggest that future searches for low-mass\nhaloes in lensed arcs include lens angular structure in the form of multipoles\nup to 4th order and require a good-fitting smooth model as a prerequisite.\nOverall, our findings demonstrate the suitability of ALMA data of this quality\nto simultaneously constrain the abundance of low-mass haloes and lens angular\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalence and properties of low-mass dark matter haloes serve as a\ncrucial test for understanding the nature of dark matter, and may be\nconstrained through the gravitational deflection of strongly lensed arcs.\nPrevious studies found evidence for the presence of low-mass dark matter haloes\nin observations of the gravitationally lensed, dusty star-forming galaxy\nSDP.81, using the Atacama Large Millimetre/sub-millimetre Array (ALMA). In this\nwork, we analyse these observations to assess the robustness of these reported\nresults. While our analysis indicates that the data support additional angular\nstructure in the lensing mass distribution beyond an elliptical power-law\ndensity profile, we do not find evidence for two previously reported sub-halo\ndetections. However, we verify with realistic mock data that we could have\nfound evidence in favour of a previously reported $\\approx 10^{9}\\,{\\rm\nM_{\\odot}}$ sub-halo with a log Bayes factor of 29, should it exist in the real\ndata. After testing various systematics, we find that this previous sub-halo\ninference was most likely spurious and resulted from an inadequate smooth\nmodel, specifically, poorly fitting multipoles. While we do not find evidence\nin favour of any individual sub-halo, we find evidence for similarity in the\nlensing signatures of multipoles ($m=3,4$) and single massive sub-haloes,\nconsistent with other recent work. We suggest that future searches for low-mass\nhaloes in lensed arcs include lens angular structure in the form of multipoles\nup to 4th order and require a good-fitting smooth model as a prerequisite.\nOverall, our findings demonstrate the suitability of ALMA data of this quality\nto simultaneously constrain the abundance of low-mass haloes and lens angular\nstructure."
                },
                "authors": [
                    {
                        "name": "H. R. Stacey"
                    },
                    {
                        "name": "D. M. Powell"
                    },
                    {
                        "name": "S. Vegetti"
                    },
                    {
                        "name": "J. P. McKean"
                    },
                    {
                        "name": "D. Wen"
                    }
                ],
                "author_detail": {
                    "name": "D. Wen"
                },
                "author": "D. Wen",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12615v2",
                "updated": "2025-09-03T09:24:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    24,
                    0,
                    2,
                    246,
                    0
                ],
                "published": "2025-03-16T19:11:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    19,
                    11,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization"
                },
                "summary": "Text-to-image latent diffusion models (LDMs) have recently emerged as\npowerful generative models with great potential for solving inverse problems in\nimaging. However, leveraging such models in a Plug & Play (PnP), zero-shot\nmanner remains challenging because it requires identifying a suitable text\nprompt for the unknown image of interest. Also, existing text-to-image PnP\napproaches are highly computationally expensive. We herein address these\nchallenges by proposing a novel PnP inference paradigm specifically designed\nfor embedding generative models within stochastic inverse solvers, with special\nattention to Latent Consistency Models (LCMs), which distill LDMs into fast\ngenerators. We leverage our framework to propose LAtent consisTency INverse\nsOlver (LATINO), the first zero-shot PnP framework to solve inverse problems\nwith priors encoded by LCMs. Our conditioning mechanism avoids automatic\ndifferentiation and reaches SOTA quality in as little as 8 neural function\nevaluations. As a result, LATINO delivers remarkably accurate solutions and is\nsignificantly more memory and computationally efficient than previous\napproaches. We then embed LATINO within an empirical Bayesian framework that\nautomatically calibrates the text prompt from the observed measurements by\nmarginal maximum likelihood estimation. Extensive experiments show that prompt\nself-calibration greatly improves estimation, allowing LATINO with PRompt\nOptimization to define new SOTAs in image reconstruction quality and\ncomputational efficiency. The code is available at https://latino-pro.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image latent diffusion models (LDMs) have recently emerged as\npowerful generative models with great potential for solving inverse problems in\nimaging. However, leveraging such models in a Plug & Play (PnP), zero-shot\nmanner remains challenging because it requires identifying a suitable text\nprompt for the unknown image of interest. Also, existing text-to-image PnP\napproaches are highly computationally expensive. We herein address these\nchallenges by proposing a novel PnP inference paradigm specifically designed\nfor embedding generative models within stochastic inverse solvers, with special\nattention to Latent Consistency Models (LCMs), which distill LDMs into fast\ngenerators. We leverage our framework to propose LAtent consisTency INverse\nsOlver (LATINO), the first zero-shot PnP framework to solve inverse problems\nwith priors encoded by LCMs. Our conditioning mechanism avoids automatic\ndifferentiation and reaches SOTA quality in as little as 8 neural function\nevaluations. As a result, LATINO delivers remarkably accurate solutions and is\nsignificantly more memory and computationally efficient than previous\napproaches. We then embed LATINO within an empirical Bayesian framework that\nautomatically calibrates the text prompt from the observed measurements by\nmarginal maximum likelihood estimation. Extensive experiments show that prompt\nself-calibration greatly improves estimation, allowing LATINO with PRompt\nOptimization to define new SOTAs in image reconstruction quality and\ncomputational efficiency. The code is available at https://latino-pro.github.io"
                },
                "authors": [
                    {
                        "name": "Alessio Spagnoletti"
                    },
                    {
                        "name": "Jean Prost"
                    },
                    {
                        "name": "Andrés Almansa"
                    },
                    {
                        "name": "Nicolas Papadakis"
                    },
                    {
                        "name": "Marcelo Pereyra"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Pereyra"
                },
                "author": "Marcelo Pereyra",
                "arxiv_comment": "27 pages, 24 figures, International Conference on Computer Vision,\n  ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03162v1",
                "updated": "2025-09-03T09:22:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    22,
                    39,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:22:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    22,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language\n  Understanding in Sinhala",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language\n  Understanding in Sinhala"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive general knowledge and\nreasoning abilities, yet their evaluation has predominantly focused on global\nor anglocentric subjects, often neglecting low-resource languages and\nculturally specific content. While recent multilingual benchmarks attempt to\nbridge this gap, many rely on automatic translation, which can introduce errors\nand misrepresent the original cultural context. To address this, we introduce\nSinhalaMMLU, the first multiple-choice question answering benchmark designed\nspecifically for Sinhala, a low-resource language. The dataset includes over\n7,000 questions spanning secondary to collegiate education levels, aligned with\nthe Sri Lankan national curriculum, and covers six domains and 30 subjects,\nencompassing both general academic topics and culturally grounded knowledge. We\nevaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and\nGPT-4o achieve the highest average accuracies at 67% and 62% respectively,\noverall model performance remains limited. In particular, models struggle in\nculturally rich domains such as the Humanities, revealing substantial room for\nimprovement in adapting LLMs to low-resource and culturally specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive general knowledge and\nreasoning abilities, yet their evaluation has predominantly focused on global\nor anglocentric subjects, often neglecting low-resource languages and\nculturally specific content. While recent multilingual benchmarks attempt to\nbridge this gap, many rely on automatic translation, which can introduce errors\nand misrepresent the original cultural context. To address this, we introduce\nSinhalaMMLU, the first multiple-choice question answering benchmark designed\nspecifically for Sinhala, a low-resource language. The dataset includes over\n7,000 questions spanning secondary to collegiate education levels, aligned with\nthe Sri Lankan national curriculum, and covers six domains and 30 subjects,\nencompassing both general academic topics and culturally grounded knowledge. We\nevaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and\nGPT-4o achieve the highest average accuracies at 67% and 62% respectively,\noverall model performance remains limited. In particular, models struggle in\nculturally rich domains such as the Humanities, revealing substantial room for\nimprovement in adapting LLMs to low-resource and culturally specific contexts."
                },
                "authors": [
                    {
                        "name": "Ashmari Pramodya"
                    },
                    {
                        "name": "Nirasha Nelki"
                    },
                    {
                        "name": "Heshan Shalinda"
                    },
                    {
                        "name": "Chamila Liyanage"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Randil Pushpananda"
                    },
                    {
                        "name": "Ruvan Weerasinghe"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03161v1",
                "updated": "2025-09-03T09:21:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    21,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:21:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    21,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "Domain Adaptation of LLMs for Process Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of LLMs for Process Data"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization."
                },
                "authors": [
                    {
                        "name": "Rafael Seidi Oyamada"
                    },
                    {
                        "name": "Jari Peeperkorn"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    },
                    {
                        "name": "Johannes De Smedt"
                    }
                ],
                "author_detail": {
                    "name": "Johannes De Smedt"
                },
                "author": "Johannes De Smedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15048v2",
                "updated": "2025-09-03T09:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    7,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-19T09:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    10,
                    49,
                    5,
                    293,
                    0
                ],
                "title": "MorphAgent: Empowering Agents through Self-Evolving Profiles and\n  Decentralized Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphAgent: Empowering Agents through Self-Evolving Profiles and\n  Decentralized Collaboration"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems (MAS) have shown promise\nin tackling complex tasks, but often rely on predefined roles and centralized\ncoordination, limiting their adaptability to evolving challenges. This paper\nintroduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive\nMulti-Agent System for decentralized agent collaboration that enables agents to\ndynamically evolve their roles and capabilities. Our approach employs\nself-evolving agent profiles, optimized through three key metrics, guiding\nagents in refining their individual expertise while maintaining complementary\nteam dynamics. MorphAgent implements a two-phase process: a Profile Update\nphase for profile optimization, followed by a Task Execution phase where agents\ncontinuously adapt their roles based on task feedback. Our experimental results\nshow that MorphAgent outperforms existing frameworks in terms of task\nperformance and adaptability to changing requirements, paving the way for more\nrobust and versatile multi-agent collaborative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems (MAS) have shown promise\nin tackling complex tasks, but often rely on predefined roles and centralized\ncoordination, limiting their adaptability to evolving challenges. This paper\nintroduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive\nMulti-Agent System for decentralized agent collaboration that enables agents to\ndynamically evolve their roles and capabilities. Our approach employs\nself-evolving agent profiles, optimized through three key metrics, guiding\nagents in refining their individual expertise while maintaining complementary\nteam dynamics. MorphAgent implements a two-phase process: a Profile Update\nphase for profile optimization, followed by a Task Execution phase where agents\ncontinuously adapt their roles based on task feedback. Our experimental results\nshow that MorphAgent outperforms existing frameworks in terms of task\nperformance and adaptability to changing requirements, paving the way for more\nrobust and versatile multi-agent collaborative systems."
                },
                "authors": [
                    {
                        "name": "Siyuan Lu"
                    },
                    {
                        "name": "Jiaqi Shao"
                    },
                    {
                        "name": "Bing Luo"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03148v1",
                "updated": "2025-09-03T08:57:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:57:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader"
                },
                "summary": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging."
                },
                "authors": [
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Ignacio Pérez Prat"
                    },
                    {
                        "name": "Not Battesta Soliva"
                    },
                    {
                        "name": "Sandra Baltermia-Guetg"
                    },
                    {
                        "name": "Andrina Beeli"
                    },
                    {
                        "name": "Simona Beeli"
                    },
                    {
                        "name": "Madlaina Capeder"
                    },
                    {
                        "name": "Laura Decurtins"
                    },
                    {
                        "name": "Gian Peder Gregori"
                    },
                    {
                        "name": "Flavia Hobi"
                    },
                    {
                        "name": "Gabriela Holderegger"
                    },
                    {
                        "name": "Arina Lazzarini"
                    },
                    {
                        "name": "Viviana Lazzarini"
                    },
                    {
                        "name": "Walter Rosselli"
                    },
                    {
                        "name": "Bettina Vital"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "Submitted to WMT25 (Open Language Data Initiative Shared Task)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06932v3",
                "updated": "2025-09-03T08:45:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    45,
                    12,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-10T19:00:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    19,
                    0,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Assessing subhalo finders in cosmological hydrodynamical simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing subhalo finders in cosmological hydrodynamical simulations"
                },
                "summary": "Cosmological simulations are essential for inferring cosmological and galaxy\npopulation properties based on forward-modelling, but this typically requires\nfinding the population of (sub)haloes and galaxies that they contain. The\nproperties of said populations vary depending on the algorithm used to find\nthem, which is concerning as it may bias key statistics. We compare how the\npredicted (sub)halo mass functions, satellite radial distributions and\ncorrelation functions vary across algorithms in the dark-matter-only and\nhydrodynamical versions of the FLAMINGO simulations. We test three\nrepresentative approaches to finding subhaloes: grouping particles in\nconfiguration- (Subfind), phase- (ROCKSTAR and VELOCIraptor) and history-space\n(HBT-HERONS). We also present HBT-HERONS, a new version of the HBT+ subhalo\nfinder that improves the tracking of subhaloes. We find 10%-level differences\nin the $M_{\\mathrm{200c}}$ mass function, reflecting different field halo\ndefinitions and occasional miscentering. The bound mass functions can differ by\n75% at the high mass end, even when using the maximum circular velocity as a\nmass proxy. The number of well-resolved subhaloes differs by up to 20% near\n$R_{\\mathrm{200c}}$, reflecting differences in the assignment of mass to\nsubhaloes and their identification. The predictions of different subhalo\nfinders increasingly diverge towards the centres of the host haloes. The\nperformance of most subhalo finders does not improve with the resolution of the\nsimulation and is worse for hydrodynamical than for dark-matter-only\nsimulations. We conclude that HBT-HERONS is the preferred choice of subhalo\nfinder due to its low computational cost, self-consistently made and robust\nmerger trees, and robust subhalo identification capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological simulations are essential for inferring cosmological and galaxy\npopulation properties based on forward-modelling, but this typically requires\nfinding the population of (sub)haloes and galaxies that they contain. The\nproperties of said populations vary depending on the algorithm used to find\nthem, which is concerning as it may bias key statistics. We compare how the\npredicted (sub)halo mass functions, satellite radial distributions and\ncorrelation functions vary across algorithms in the dark-matter-only and\nhydrodynamical versions of the FLAMINGO simulations. We test three\nrepresentative approaches to finding subhaloes: grouping particles in\nconfiguration- (Subfind), phase- (ROCKSTAR and VELOCIraptor) and history-space\n(HBT-HERONS). We also present HBT-HERONS, a new version of the HBT+ subhalo\nfinder that improves the tracking of subhaloes. We find 10%-level differences\nin the $M_{\\mathrm{200c}}$ mass function, reflecting different field halo\ndefinitions and occasional miscentering. The bound mass functions can differ by\n75% at the high mass end, even when using the maximum circular velocity as a\nmass proxy. The number of well-resolved subhaloes differs by up to 20% near\n$R_{\\mathrm{200c}}$, reflecting differences in the assignment of mass to\nsubhaloes and their identification. The predictions of different subhalo\nfinders increasingly diverge towards the centres of the host haloes. The\nperformance of most subhalo finders does not improve with the resolution of the\nsimulation and is worse for hydrodynamical than for dark-matter-only\nsimulations. We conclude that HBT-HERONS is the preferred choice of subhalo\nfinder due to its low computational cost, self-consistently made and robust\nmerger trees, and robust subhalo identification capabilities."
                },
                "authors": [
                    {
                        "name": "Victor J. Forouhar Moreno"
                    },
                    {
                        "name": "John Helly"
                    },
                    {
                        "name": "Robert McGibbon"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Jiaxin Han"
                    },
                    {
                        "name": "Roi Kugel"
                    },
                    {
                        "name": "Yannick M. Bahe"
                    }
                ],
                "author_detail": {
                    "name": "Yannick M. Bahe"
                },
                "author": "Yannick M. Bahe",
                "arxiv_comment": "Accepted for publication in MNRAS. 34 pages total: 24 pages of main\n  text and 10 of appendices. v3: fixed hyperlinks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00061v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00061v4",
                "updated": "2025-09-03T08:36:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    36,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2024-08-22T14:27:47Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    27,
                    47,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Natural Language Inference Performance with Knowledge Graph\n  for COVID-19 Automated Fact-Checking in Indonesian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Natural Language Inference Performance with Knowledge Graph\n  for COVID-19 Automated Fact-Checking in Indonesian Language"
                },
                "summary": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking."
                },
                "authors": [
                    {
                        "name": "Arief Purnama Muharram"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    }
                ],
                "author_detail": {
                    "name": "Ayu Purwarianti"
                },
                "author": "Ayu Purwarianti",
                "arxiv_comment": "Accepted for publication in the Journal of ICT Research and\n  Applications (JICTRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00061v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00061v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09701v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09701v5",
                "updated": "2025-09-03T08:35:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    35,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-14T04:01:25Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    4,
                    1,
                    25,
                    4,
                    166,
                    0
                ],
                "title": "Towards Explainable Vulnerability Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Vulnerability Detection with Large Language Models"
                },
                "summary": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security."
                },
                "authors": [
                    {
                        "name": "Qiheng Mao"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09701v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09701v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03131v1",
                "updated": "2025-09-03T08:33:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    33,
                    43,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:33:43Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    33,
                    43,
                    2,
                    246,
                    0
                ],
                "title": "RecBase: Generative Foundation Model Pretraining for Zero-Shot\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecBase: Generative Foundation Model Pretraining for Zero-Shot\n  Recommendation"
                },
                "summary": "Recent advances in LLM-based recommendation have shown promise, yet their\ncross-domain generalization is hindered by a fundamental mismatch between\nlanguage-centric pretraining and the recommendation task. Existing methods,\nrelying on language-level knowledge, fail to capture dynamic, item-level user\ninterests across domains. To bridge this gap, we propose RecBase, a\ndomain-agnostic foundational model pretrained with a recommendation-oriented\nobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus\nwith unified textual representations and feature mappings to enhance\ncross-domain generalization. To further align item semantics across domains, we\nintroduce a unified item tokenizer that encodes items into hierarchical concept\nidentifiers, enabling structured representation and efficient vocabulary\nsharing. The model is trained using an autoregressive objective to capture\ncomplex item-level sequential patterns. On eight real-world datasets, our\n1.5B-parameter model matches or surpasses the performance of LLM baselines up\nto 7B parameters in zero-shot and cross-domain recommendation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM-based recommendation have shown promise, yet their\ncross-domain generalization is hindered by a fundamental mismatch between\nlanguage-centric pretraining and the recommendation task. Existing methods,\nrelying on language-level knowledge, fail to capture dynamic, item-level user\ninterests across domains. To bridge this gap, we propose RecBase, a\ndomain-agnostic foundational model pretrained with a recommendation-oriented\nobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus\nwith unified textual representations and feature mappings to enhance\ncross-domain generalization. To further align item semantics across domains, we\nintroduce a unified item tokenizer that encodes items into hierarchical concept\nidentifiers, enabling structured representation and efficient vocabulary\nsharing. The model is trained using an autoregressive objective to capture\ncomplex item-level sequential patterns. On eight real-world datasets, our\n1.5B-parameter model matches or surpasses the performance of LLM baselines up\nto 7B parameters in zero-shot and cross-domain recommendation tasks."
                },
                "authors": [
                    {
                        "name": "Sashuai Zhou"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Ke Lei"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_journal_ref": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03128v1",
                "updated": "2025-09-03T08:28:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    28,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:28:20Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    28,
                    20,
                    2,
                    246,
                    0
                ],
                "title": "Successive Cancellation Decoding For General Monotone Chain Polar Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successive Cancellation Decoding For General Monotone Chain Polar Codes"
                },
                "summary": "Monotone chain polar codes generalize classical polar codes to multivariate\nsettings, offering a flexible approach for achieving the entire admissible rate\nregion in the distributed lossless coding problem. However, this flexibility\nalso introduces significant challenges for existing successive cancellation\n(SC) based decoding schemes. Motivated by the need for a general SC decoding\nsolution, we present a comprehensive decoding strategy for monotone chain polar\ncodes that can handle arbitrary numbers of terminals, non-binary alphabets, and\ndecoding along arbitrary monotone chains. Specifically, we formulate the SC\ndecoding task as a series of inference subtasks over the polar transform and\npropose a computational graph framework based on probability propagation\nprinciples. This approach highlights the impact of variable switching during\ndecoding and shows that time complexity varies between $O(N\\log{N})$ and\n$O(N^2)$, depending on the specific chain structure. Moreover, we demonstrate\nthat the widely used $O(N)$ space optimization is not universally applicable to\nmonotone chain polar codes, which prompts us to introduce a constant-time\ndecoder forking strategy based on the proposed logical computation graphs. This\nstrategy enables time-efficient list decoding without relying on $O(N)$-space\ntechniques. Numerical results verify the superior performance of the proposed\nscheme compared with the classical lazy-copy scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotone chain polar codes generalize classical polar codes to multivariate\nsettings, offering a flexible approach for achieving the entire admissible rate\nregion in the distributed lossless coding problem. However, this flexibility\nalso introduces significant challenges for existing successive cancellation\n(SC) based decoding schemes. Motivated by the need for a general SC decoding\nsolution, we present a comprehensive decoding strategy for monotone chain polar\ncodes that can handle arbitrary numbers of terminals, non-binary alphabets, and\ndecoding along arbitrary monotone chains. Specifically, we formulate the SC\ndecoding task as a series of inference subtasks over the polar transform and\npropose a computational graph framework based on probability propagation\nprinciples. This approach highlights the impact of variable switching during\ndecoding and shows that time complexity varies between $O(N\\log{N})$ and\n$O(N^2)$, depending on the specific chain structure. Moreover, we demonstrate\nthat the widely used $O(N)$ space optimization is not universally applicable to\nmonotone chain polar codes, which prompts us to introduce a constant-time\ndecoder forking strategy based on the proposed logical computation graphs. This\nstrategy enables time-efficient list decoding without relying on $O(N)$-space\ntechniques. Numerical results verify the superior performance of the proposed\nscheme compared with the classical lazy-copy scheme."
                },
                "authors": [
                    {
                        "name": "Zichang Ren"
                    },
                    {
                        "name": "Chunhang Zheng"
                    },
                    {
                        "name": "Dou Li"
                    },
                    {
                        "name": "Yuping Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuping Zhao"
                },
                "author": "Yuping Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03123v1",
                "updated": "2025-09-03T08:22:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:22:20Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    20,
                    2,
                    246,
                    0
                ],
                "title": "Kangaroo: A Private and Amortized Inference Framework over WAN for\n  Large-Scale Decision Tree Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kangaroo: A Private and Amortized Inference Framework over WAN for\n  Large-Scale Decision Tree Evaluation"
                },
                "summary": "With the rapid adoption of Models-as-a-Service, concerns about data and model\nprivacy have become increasingly critical. To solve these problems, various\nprivacy-preserving inference schemes have been proposed. In particular, due to\nthe efficiency and interpretability of decision trees, private decision tree\nevaluation (PDTE) has garnered significant attention. However, existing PDTE\nschemes suffer from significant limitations: their communication and\ncomputation costs scale with the number of trees, the number of nodes, or the\ntree depth, which makes them inefficient for large-scale models, especially\nover WAN networks. To address these issues, we propose Kangaroo, a private and\namortized decision tree inference framework build upon packed homomorphic\nencryption. Specifically, we design a novel model hiding and encoding scheme,\ntogether with secure feature selection, oblivious comparison, and secure path\nevaluation protocols, enabling full amortization of the overhead as the number\nof nodes or trees scales. Furthermore, we enhance the performance and\nfunctionality of the framework through optimizations, including\nsame-sharing-for-same-model, latency-aware, and adaptive encoding adjustment\nstrategies. Kangaroo achieves a $14\\times$ to $59\\times$ performance\nimprovement over state-of-the-art (SOTA) one-round interactive schemes in WAN\nenvironments. For large-scale decision tree inference tasks, it delivers a\n$3\\times$ to $44\\times$ speedup compared to existing schemes. Notably, Kangaroo\nenables the evaluation of a random forest with $969$ trees and $411825$ nodes\nin approximately $60$ ms per tree (amortized) under WAN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Models-as-a-Service, concerns about data and model\nprivacy have become increasingly critical. To solve these problems, various\nprivacy-preserving inference schemes have been proposed. In particular, due to\nthe efficiency and interpretability of decision trees, private decision tree\nevaluation (PDTE) has garnered significant attention. However, existing PDTE\nschemes suffer from significant limitations: their communication and\ncomputation costs scale with the number of trees, the number of nodes, or the\ntree depth, which makes them inefficient for large-scale models, especially\nover WAN networks. To address these issues, we propose Kangaroo, a private and\namortized decision tree inference framework build upon packed homomorphic\nencryption. Specifically, we design a novel model hiding and encoding scheme,\ntogether with secure feature selection, oblivious comparison, and secure path\nevaluation protocols, enabling full amortization of the overhead as the number\nof nodes or trees scales. Furthermore, we enhance the performance and\nfunctionality of the framework through optimizations, including\nsame-sharing-for-same-model, latency-aware, and adaptive encoding adjustment\nstrategies. Kangaroo achieves a $14\\times$ to $59\\times$ performance\nimprovement over state-of-the-art (SOTA) one-round interactive schemes in WAN\nenvironments. For large-scale decision tree inference tasks, it delivers a\n$3\\times$ to $44\\times$ speedup compared to existing schemes. Notably, Kangaroo\nenables the evaluation of a random forest with $969$ trees and $411825$ nodes\nin approximately $60$ ms per tree (amortized) under WAN environments."
                },
                "authors": [
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Hui Zhu"
                    },
                    {
                        "name": "Yandong Zheng"
                    },
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Ning Sun"
                    },
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Dengguo Feng"
                    },
                    {
                        "name": "Hui Li"
                    }
                ],
                "author_detail": {
                    "name": "Hui Li"
                },
                "author": "Hui Li",
                "arxiv_doi": "10.14722/ndss.2026.230892",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2026.230892",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.03123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.03518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03518v1",
                "updated": "2025-09-03T17:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    59,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    59,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Can LLMs Lie? Investigation beyond Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Lie? Investigation beyond Hallucination"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\na variety of tasks, but their increasing autonomy in real-world applications\nraises concerns about their trustworthiness. While hallucinations-unintentional\nfalsehoods-have been widely studied, the phenomenon of lying, where an LLM\nknowingly generates falsehoods to achieve an ulterior objective, remains\nunderexplored. In this work, we systematically investigate the lying behavior\nof LLMs, differentiating it from hallucinations and testing it in practical\nscenarios. Through mechanistic interpretability techniques, we uncover the\nneural mechanisms underlying deception, employing logit lens analysis, causal\ninterventions, and contrastive activation steering to identify and control\ndeceptive behavior. We study real-world lying scenarios and introduce\nbehavioral steering vectors that enable fine-grained manipulation of lying\ntendencies. Further, we explore the trade-offs between lying and end-task\nperformance, establishing a Pareto frontier where dishonesty can enhance goal\noptimization. Our findings contribute to the broader discourse on AI ethics,\nshedding light on the risks and potential safeguards for deploying LLMs in\nhigh-stakes environments. Code and more illustrations are available at\nhttps://llm-liar.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities across\na variety of tasks, but their increasing autonomy in real-world applications\nraises concerns about their trustworthiness. While hallucinations-unintentional\nfalsehoods-have been widely studied, the phenomenon of lying, where an LLM\nknowingly generates falsehoods to achieve an ulterior objective, remains\nunderexplored. In this work, we systematically investigate the lying behavior\nof LLMs, differentiating it from hallucinations and testing it in practical\nscenarios. Through mechanistic interpretability techniques, we uncover the\nneural mechanisms underlying deception, employing logit lens analysis, causal\ninterventions, and contrastive activation steering to identify and control\ndeceptive behavior. We study real-world lying scenarios and introduce\nbehavioral steering vectors that enable fine-grained manipulation of lying\ntendencies. Further, we explore the trade-offs between lying and end-task\nperformance, establishing a Pareto frontier where dishonesty can enhance goal\noptimization. Our findings contribute to the broader discourse on AI ethics,\nshedding light on the risks and potential safeguards for deploying LLMs in\nhigh-stakes environments. Code and more illustrations are available at\nhttps://llm-liar.github.io/"
                },
                "authors": [
                    {
                        "name": "Haoran Huan"
                    },
                    {
                        "name": "Mihir Prabhudesai"
                    },
                    {
                        "name": "Mengning Wu"
                    },
                    {
                        "name": "Shantanu Jaiswal"
                    },
                    {
                        "name": "Deepak Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Pathak"
                },
                "author": "Deepak Pathak",
                "arxiv_comment": "Website at https://llm-liar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00117v2",
                "updated": "2025-09-03T17:55:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    55,
                    11,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-28T17:59:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    59,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "Embodied AI: Emerging Risks and Opportunities for Policy Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI: Emerging Risks and Opportunities for Policy Action"
                },
                "summary": "The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI\nsystems can exist in, learn from, reason about, and act in the physical world.\nWith recent advances in AI models and hardware, EAI systems are becoming\nincreasingly capable across wider operational domains. While EAI systems can\noffer many benefits, they also pose significant risks, including physical harm\nfrom malicious use, mass surveillance, as well as economic and societal\ndisruption. These risks require urgent attention from policymakers, as existing\npolicies governing industrial robots and autonomous vehicles are insufficient\nto address the full range of concerns EAI systems present. To help address this\nissue, this paper makes three contributions. First, we provide a taxonomy of\nthe physical, informational, economic, and social risks EAI systems pose.\nSecond, we analyze policies in the US, EU, and UK to assess how existing\nframeworks address these risks and to identify critical gaps. We conclude by\noffering policy recommendations for the safe and beneficial deployment of EAI\nsystems, such as mandatory testing and certification schemes, clarified\nliability frameworks, and strategies to manage EAI's potentially transformative\neconomic and societal impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI\nsystems can exist in, learn from, reason about, and act in the physical world.\nWith recent advances in AI models and hardware, EAI systems are becoming\nincreasingly capable across wider operational domains. While EAI systems can\noffer many benefits, they also pose significant risks, including physical harm\nfrom malicious use, mass surveillance, as well as economic and societal\ndisruption. These risks require urgent attention from policymakers, as existing\npolicies governing industrial robots and autonomous vehicles are insufficient\nto address the full range of concerns EAI systems present. To help address this\nissue, this paper makes three contributions. First, we provide a taxonomy of\nthe physical, informational, economic, and social risks EAI systems pose.\nSecond, we analyze policies in the US, EU, and UK to assess how existing\nframeworks address these risks and to identify critical gaps. We conclude by\noffering policy recommendations for the safe and beneficial deployment of EAI\nsystems, such as mandatory testing and certification schemes, clarified\nliability frameworks, and strategies to manage EAI's potentially transformative\neconomic and societal impacts."
                },
                "authors": [
                    {
                        "name": "Jared Perlo"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Luciano Floridi"
                    },
                    {
                        "name": "Jakob Mökander"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Mökander"
                },
                "author": "Jakob Mökander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01086v2",
                "updated": "2025-09-03T17:46:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    46,
                    48,
                    2,
                    246,
                    0
                ],
                "published": "2025-04-01T18:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    7,
                    7,
                    1,
                    91,
                    0
                ],
                "title": "MPCritic: A plug-and-play MPC architecture for reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCritic: A plug-and-play MPC architecture for reinforcement learning"
                },
                "summary": "The reinforcement learning (RL) and model predictive control (MPC)\ncommunities have developed vast ecosystems of theoretical approaches and\ncomputational tools for solving optimal control problems. Given their\nconceptual similarities but differing strengths, there has been increasing\ninterest in synergizing RL and MPC. However, existing approaches tend to be\nlimited for various reasons, including computational cost of MPC in an RL\nalgorithm and software hurdles towards seamless integration of MPC and RL\ntools. These challenges often result in the use of \"simple\" MPC schemes or RL\nalgorithms, neglecting the state-of-the-art in both areas. This paper presents\nMPCritic, a machine learning-friendly architecture that interfaces seamlessly\nwith MPC tools. MPCritic utilizes the loss landscape defined by a parameterized\nMPC problem, focusing on \"soft\" optimization over batched training steps;\nthereby updating the MPC parameters while avoiding costly minimization and\nparametric sensitivities. Since the MPC structure is preserved during training,\nan MPC agent can be readily used for online deployment, where robust constraint\nsatisfaction is paramount. We demonstrate the versatility of MPCritic, in terms\nof MPC architectures and RL algorithms that it can accommodate, on classic\ncontrol benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reinforcement learning (RL) and model predictive control (MPC)\ncommunities have developed vast ecosystems of theoretical approaches and\ncomputational tools for solving optimal control problems. Given their\nconceptual similarities but differing strengths, there has been increasing\ninterest in synergizing RL and MPC. However, existing approaches tend to be\nlimited for various reasons, including computational cost of MPC in an RL\nalgorithm and software hurdles towards seamless integration of MPC and RL\ntools. These challenges often result in the use of \"simple\" MPC schemes or RL\nalgorithms, neglecting the state-of-the-art in both areas. This paper presents\nMPCritic, a machine learning-friendly architecture that interfaces seamlessly\nwith MPC tools. MPCritic utilizes the loss landscape defined by a parameterized\nMPC problem, focusing on \"soft\" optimization over batched training steps;\nthereby updating the MPC parameters while avoiding costly minimization and\nparametric sensitivities. Since the MPC structure is preserved during training,\nan MPC agent can be readily used for online deployment, where robust constraint\nsatisfaction is paramount. We demonstrate the versatility of MPCritic, in terms\nof MPC architectures and RL algorithms that it can accommodate, on classic\ncontrol benchmarks."
                },
                "authors": [
                    {
                        "name": "Nathan P. Lawrence"
                    },
                    {
                        "name": "Thomas Banker"
                    },
                    {
                        "name": "Ali Mesbah"
                    }
                ],
                "author_detail": {
                    "name": "Ali Mesbah"
                },
                "author": "Ali Mesbah",
                "arxiv_comment": "CDC 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03501v1",
                "updated": "2025-09-03T17:33:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    33,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:33:20Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    33,
                    20,
                    2,
                    246,
                    0
                ],
                "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data"
                },
                "summary": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs."
                },
                "authors": [
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Xiangyu Peng"
                    },
                    {
                        "name": "Shrikant Kendre"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles",
                "arxiv_comment": "This technical report serves as the archival version of our paper\n  accepted at the ICCV 2025 Workshop. For more information, please visit our\n  project website: https://strefer.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v1",
                "updated": "2025-09-03T17:29:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03493v1",
                "updated": "2025-09-03T17:23:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    19,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T17:23:19Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    23,
                    19,
                    2,
                    246,
                    0
                ],
                "title": "On Entropy Control in LLM-RL Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Entropy Control in LLM-RL Algorithms"
                },
                "summary": "For RL algorithms, appropriate entropy control is crucial to their\neffectiveness. To control the policy entropy, a commonly used method is entropy\nregularization, which is adopted in various popular RL algorithms including\nPPO, SAC and A3C. Although entropy regularization proves effective in robotic\nand games RL conventionally, studies found that it gives weak to no gains in\nLLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL\nsetting. Specifically, we first argue that the conventional entropy\nregularization suffers from the LLM's extremely large response space and the\nsparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy\ncontrol method that utilizes a new clamped entropy bonus with an automatically\nadjusted coefficient. The clamped entropy is evaluated with the re-normalized\npolicy defined on certain smaller token space, which encourages exploration\nwithin a more compact response set. In addition, the algorithm automatically\nadjusts entropy coefficient according to the clamped entropy value, effectively\ncontrolling the entropy-induced bias while leveraging the entropy's benefits.\nAEnt is tested in math-reasoning tasks under different base models and\ndatasets, and it is observed that AEnt outperforms the baselines consistently\nacross multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For RL algorithms, appropriate entropy control is crucial to their\neffectiveness. To control the policy entropy, a commonly used method is entropy\nregularization, which is adopted in various popular RL algorithms including\nPPO, SAC and A3C. Although entropy regularization proves effective in robotic\nand games RL conventionally, studies found that it gives weak to no gains in\nLLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL\nsetting. Specifically, we first argue that the conventional entropy\nregularization suffers from the LLM's extremely large response space and the\nsparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy\ncontrol method that utilizes a new clamped entropy bonus with an automatically\nadjusted coefficient. The clamped entropy is evaluated with the re-normalized\npolicy defined on certain smaller token space, which encourages exploration\nwithin a more compact response set. In addition, the algorithm automatically\nadjusts entropy coefficient according to the clamped entropy value, effectively\ncontrolling the entropy-induced bias while leveraging the entropy's benefits.\nAEnt is tested in math-reasoning tasks under different base models and\ndatasets, and it is observed that AEnt outperforms the baselines consistently\nacross multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Han Shen"
                    }
                ],
                "author_detail": {
                    "name": "Han Shen"
                },
                "author": "Han Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12100v2",
                "updated": "2025-09-03T17:18:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    18,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-12T21:20:10Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    20,
                    10,
                    3,
                    163,
                    0
                ],
                "title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions\n  to Generative Model's Response for Vulnerability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions\n  to Generative Model's Response for Vulnerability Analysis"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for cybersecurity threat\nanalysis, but their deployment in security-sensitive environments raises trust\nand safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual\nanalysis is infeasible, making scalable and verifiable AI support critical.\nWhen querying LLMs, dealing with emerging vulnerabilities is challenging as\nthey have a training cut-off date. While Retrieval-Augmented Generation (RAG)\ncan inject up-to-date context to alleviate the cut-off date limitation, it\nremains unclear how much LLMs rely on retrieved evidence versus the model's\ninternal knowledge, and whether the retrieved information is meaningful or even\ncorrect. This uncertainty could mislead security analysts, mis-prioritize\npatches, and increase security risks. Therefore, this work proposes LLM\nEmbedding-based Attribution (LEA) to analyze the generated responses for\nvulnerability exploitation analysis. More specifically, LEA quantifies the\nrelative contribution of internal knowledge vs. retrieved content in the\ngenerated responses. We evaluate LEA on 500 critical vulnerabilities disclosed\nbetween 2016 and 2025, across three RAG settings -- valid, generic, and\nincorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's\nability to detect clear distinctions between non-retrieval, generic-retrieval,\nand valid-retrieval scenarios with over 95% accuracy on larger models. Finally,\nwe demonstrate the limitations posed by incorrect retrieval of vulnerability\ninformation and raise a cautionary note to the cybersecurity community\nregarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA\noffers security analysts with a metric to audit RAG-enhanced workflows,\nimproving the transparent and trustworthy deployment of AI in cybersecurity\nthreat analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for cybersecurity threat\nanalysis, but their deployment in security-sensitive environments raises trust\nand safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual\nanalysis is infeasible, making scalable and verifiable AI support critical.\nWhen querying LLMs, dealing with emerging vulnerabilities is challenging as\nthey have a training cut-off date. While Retrieval-Augmented Generation (RAG)\ncan inject up-to-date context to alleviate the cut-off date limitation, it\nremains unclear how much LLMs rely on retrieved evidence versus the model's\ninternal knowledge, and whether the retrieved information is meaningful or even\ncorrect. This uncertainty could mislead security analysts, mis-prioritize\npatches, and increase security risks. Therefore, this work proposes LLM\nEmbedding-based Attribution (LEA) to analyze the generated responses for\nvulnerability exploitation analysis. More specifically, LEA quantifies the\nrelative contribution of internal knowledge vs. retrieved content in the\ngenerated responses. We evaluate LEA on 500 critical vulnerabilities disclosed\nbetween 2016 and 2025, across three RAG settings -- valid, generic, and\nincorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's\nability to detect clear distinctions between non-retrieval, generic-retrieval,\nand valid-retrieval scenarios with over 95% accuracy on larger models. Finally,\nwe demonstrate the limitations posed by incorrect retrieval of vulnerability\ninformation and raise a cautionary note to the cybersecurity community\nregarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA\noffers security analysts with a metric to audit RAG-enhanced workflows,\nimproving the transparent and trustworthy deployment of AI in cybersecurity\nthreat analysis."
                },
                "authors": [
                    {
                        "name": "Reza Fayyazi"
                    },
                    {
                        "name": "Michael Zuzak"
                    },
                    {
                        "name": "Shanchieh Jay Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shanchieh Jay Yang"
                },
                "author": "Shanchieh Jay Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20800v3",
                "updated": "2025-09-03T17:09:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    9,
                    49,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-28T13:08:33Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    8,
                    33,
                    0,
                    209,
                    0
                ],
                "title": "LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted\n  Lanternfly Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted\n  Lanternfly Populations"
                },
                "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes."
                },
                "authors": [
                    {
                        "name": "Vinil Polepalli"
                    }
                ],
                "author_detail": {
                    "name": "Vinil Polepalli"
                },
                "author": "Vinil Polepalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02479v2",
                "updated": "2025-09-03T17:06:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    6,
                    42,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T16:30:19Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    19,
                    1,
                    245,
                    0
                ],
                "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning"
                },
                "summary": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation."
                },
                "authors": [
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Longtao Zheng"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Yingru Li"
                    },
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00634v2",
                "updated": "2025-09-03T17:06:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    6,
                    38,
                    2,
                    246,
                    0
                ],
                "published": "2025-03-01T22:00:03Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    22,
                    0,
                    3,
                    5,
                    60,
                    0
                ],
                "title": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts"
                },
                "summary": "Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts."
                },
                "authors": [
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    }
                ],
                "author_detail": {
                    "name": "Hany Hassan Awadalla"
                },
                "author": "Hany Hassan Awadalla",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11133v2",
                "updated": "2025-09-03T17:03:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    3,
                    15,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-15T00:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    0,
                    58,
                    10,
                    4,
                    227,
                    0
                ],
                "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens\n  of Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens\n  of Documents"
                },
                "summary": "Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco"
                },
                "authors": [
                    {
                        "name": "Tomer Wolfson"
                    },
                    {
                        "name": "Harsh Trivedi"
                    },
                    {
                        "name": "Mor Geva"
                    },
                    {
                        "name": "Yoav Goldberg"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Tushar Khot"
                    },
                    {
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    }
                ],
                "author_detail": {
                    "name": "Reut Tsarfaty"
                },
                "author": "Reut Tsarfaty",
                "arxiv_comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03465v1",
                "updated": "2025-09-03T16:41:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    41,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:41:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    41,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "Joint Training of Image Generator and Detector for Road Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Training of Image Generator and Detector for Road Defect Detection"
                },
                "summary": "Road defect detection is important for road authorities to reduce the vehicle\ndamage caused by road defects. Considering the practical scenarios where the\ndefect detectors are typically deployed on edge devices with limited memory and\ncomputational resource, we aim at performing road defect detection without\nusing ensemble-based methods or test-time augmentation (TTA). To this end, we\npropose to Jointly Train the image Generator and Detector for road defect\ndetection (dubbed as JTGD). We design the dual discriminators for the\ngenerative model to enforce both the synthesized defect patches and overall\nimages to look plausible. The synthesized image quality is improved by our\nproposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in\nJTGD is trained jointly with the detector to encourage the generative model to\nsynthesize harder examples for the detector. Since harder synthesized images of\nbetter quality caused by the aforesaid design are used in the data\naugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road\ndefect detection benchmark across various countries under the condition of no\nensemble and TTA. JTGD only uses less than 20% of the number of parameters\ncompared with the competing baseline, which makes it more suitable for\ndeployment on edge devices in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road defect detection is important for road authorities to reduce the vehicle\ndamage caused by road defects. Considering the practical scenarios where the\ndefect detectors are typically deployed on edge devices with limited memory and\ncomputational resource, we aim at performing road defect detection without\nusing ensemble-based methods or test-time augmentation (TTA). To this end, we\npropose to Jointly Train the image Generator and Detector for road defect\ndetection (dubbed as JTGD). We design the dual discriminators for the\ngenerative model to enforce both the synthesized defect patches and overall\nimages to look plausible. The synthesized image quality is improved by our\nproposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in\nJTGD is trained jointly with the detector to encourage the generative model to\nsynthesize harder examples for the detector. Since harder synthesized images of\nbetter quality caused by the aforesaid design are used in the data\naugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road\ndefect detection benchmark across various countries under the condition of no\nensemble and TTA. JTGD only uses less than 20% of the number of parameters\ncompared with the competing baseline, which makes it more suitable for\ndeployment on edge devices in practice."
                },
                "authors": [
                    {
                        "name": "Kuan-Chuan Peng"
                    }
                ],
                "author_detail": {
                    "name": "Kuan-Chuan Peng"
                },
                "author": "Kuan-Chuan Peng",
                "arxiv_comment": "This paper is accepted to ICCV 2025 Workshop on Representation\n  Learning with Very Limited Resources: When Data, Modalities, Labels, and\n  Computing Resources are Scarce as an oral paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03463v1",
                "updated": "2025-09-03T16:39:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    39,
                    25,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:39:25Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    39,
                    25,
                    2,
                    246,
                    0
                ],
                "title": "The Impact of Critique on LLM-Based Model Generation from Natural\n  Language: The Case of Activity Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Critique on LLM-Based Model Generation from Natural\n  Language: The Case of Activity Diagrams"
                },
                "summary": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average."
                },
                "authors": [
                    {
                        "name": "Parham Khamsepour"
                    },
                    {
                        "name": "Mark Cole"
                    },
                    {
                        "name": "Ish Ashraf"
                    },
                    {
                        "name": "Sandeep Puri"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Shiva Nejati"
                    }
                ],
                "author_detail": {
                    "name": "Shiva Nejati"
                },
                "author": "Shiva Nejati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03462v1",
                "updated": "2025-09-03T16:37:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    37,
                    49,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T16:37:49Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    37,
                    49,
                    2,
                    246,
                    0
                ],
                "title": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning"
                },
                "summary": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Zhuo Cao"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v3",
                "updated": "2025-09-03T16:36:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    36,
                    17,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13231v3",
                "updated": "2025-09-03T16:22:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    22,
                    6,
                    2,
                    246,
                    0
                ],
                "published": "2025-04-17T14:43:56Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    43,
                    56,
                    3,
                    107,
                    0
                ],
                "title": "WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada"
                },
                "summary": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. In this work, we focus on\nmultimodal wildfire social media data, which, although existing in current\ndatasets, is currently underrepresented in Canadian contexts. We present\nWildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian\nwildfires, annotated across twelve key themes. We evaluate zero-shot\nvision-language models on this dataset and compare their results with those of\ncustom-trained and baseline classifiers. We show that while baseline methods\nand zero-shot prompting offer quick deployment, custom-trained models\noutperform them when labelled data is available. Our best-performing custom\nmodel reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We\nalso demonstrate how this model can be used to uncover trends during wildfires,\nthrough the collection and analysis of a large unlabeled dataset. Our dataset\nfacilitates future research in wildfire response, and our findings highlight\nthe importance of tailored datasets and task-specific training. Importantly,\nsuch datasets should be localized, as disaster response requirements vary\nacross regions and contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. In this work, we focus on\nmultimodal wildfire social media data, which, although existing in current\ndatasets, is currently underrepresented in Canadian contexts. We present\nWildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian\nwildfires, annotated across twelve key themes. We evaluate zero-shot\nvision-language models on this dataset and compare their results with those of\ncustom-trained and baseline classifiers. We show that while baseline methods\nand zero-shot prompting offer quick deployment, custom-trained models\noutperform them when labelled data is available. Our best-performing custom\nmodel reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We\nalso demonstrate how this model can be used to uncover trends during wildfires,\nthrough the collection and analysis of a large unlabeled dataset. Our dataset\nfacilitates future research in wildfire response, and our findings highlight\nthe importance of tailored datasets and task-specific training. Importantly,\nsuch datasets should be localized, as disaster response requirements vary\nacross regions and contexts."
                },
                "authors": [
                    {
                        "name": "Braeden Sherritt"
                    },
                    {
                        "name": "Isar Nejadgholi"
                    },
                    {
                        "name": "Efstratios Aivaliotis"
                    },
                    {
                        "name": "Khaled Mslmani"
                    },
                    {
                        "name": "Marzieh Amini"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Amini"
                },
                "author": "Marzieh Amini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v3",
                "updated": "2025-09-03T16:15:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    15,
                    16,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v3",
                "updated": "2025-09-03T16:14:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    14,
                    12,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10978v2",
                "updated": "2025-09-03T16:13:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    16,
                    13,
                    41,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-16T08:26:59Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    26,
                    59,
                    4,
                    136,
                    0
                ],
                "title": "Group-in-Group Policy Optimization for LLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-in-Group Policy Optimization for LLM Agent Training"
                },
                "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost."
                },
                "authors": [
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Tingcong Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18179v2",
                "updated": "2025-09-03T15:53:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    53,
                    18,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-25T13:11:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs"
                },
                "summary": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "arxiv_comment": "accepted at EMNLP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03419v1",
                "updated": "2025-09-03T15:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:48:33Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "title": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges"
                },
                "summary": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels."
                },
                "authors": [
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Qingqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "8 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01527v2",
                "updated": "2025-09-03T15:43:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    43,
                    1,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-01T15:02:00Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    2,
                    0,
                    0,
                    244,
                    0
                ],
                "title": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model"
                },
                "summary": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling."
                },
                "authors": [
                    {
                        "name": "Amirreza Nayyeri"
                    },
                    {
                        "name": "Abbas Rasoolzadegan"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rasoolzadegan"
                },
                "author": "Abbas Rasoolzadegan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18096v2",
                "updated": "2025-09-03T15:32:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    32,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-22T16:52:48Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    16,
                    52,
                    48,
                    6,
                    173,
                    0
                ],
                "title": "Deep Research Agents: A Systematic Examination And Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research Agents: A Systematic Examination And Roadmap"
                },
                "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Haozheng Zhang"
                    },
                    {
                        "name": "Kang Li"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Songcen Xu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03700v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03700v4",
                "updated": "2025-09-03T15:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    20,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-19T12:33:43Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    12,
                    33,
                    43,
                    5,
                    200,
                    0
                ],
                "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning"
                },
                "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."
                },
                "authors": [
                    {
                        "name": "Liujian Tang"
                    },
                    {
                        "name": "Shaokang Dong"
                    },
                    {
                        "name": "Yijia Huang"
                    },
                    {
                        "name": "Minqi Xiang"
                    },
                    {
                        "name": "Hongtao Ruan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Zhihui Cao"
                    },
                    {
                        "name": "Hailiang Pang"
                    },
                    {
                        "name": "Heng Kong"
                    },
                    {
                        "name": "He Yang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhilin Gao"
                    },
                    {
                        "name": "Xingyu Liu"
                    },
                    {
                        "name": "Yingnan Fu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Yuran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuran Wang"
                },
                "author": "Yuran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03700v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03700v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03391v1",
                "updated": "2025-09-03T15:15:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    31,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:31Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    31,
                    2,
                    246,
                    0
                ],
                "title": "More Parameters Than Populations: A Systematic Literature Review of\n  Large Language Models within Survey Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Parameters Than Populations: A Systematic Literature Review of\n  Large Language Models within Survey Research"
                },
                "summary": "Survey research has a long-standing history of being a human-powered field,\nbut one that embraces various technologies for the collection, processing, and\nanalysis of various behavioral, political, and social outcomes of interest,\namong others. At the same time, Large Language Models (LLMs) bring new\ntechnological challenges and prerequisites in order to fully harness their\npotential. In this paper, we report work-in-progress on a systematic literature\nreview based on keyword searches from multiple large-scale databases as well as\ncitation networks that assesses how LLMs are currently being applied within the\nsurvey research process. We synthesize and organize our findings according to\nthe survey research process to include examples of LLM usage across three broad\nphases: pre-data collection, data collection, and post-data collection. We\ndiscuss selected examples of potential use cases for LLMs as well as its\npitfalls based on examples from existing literature. Considering survey\nresearch has rich experience and history regarding data quality, we discuss\nsome opportunities and describe future outlooks for survey research to\ncontribute to the continued development and refinement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey research has a long-standing history of being a human-powered field,\nbut one that embraces various technologies for the collection, processing, and\nanalysis of various behavioral, political, and social outcomes of interest,\namong others. At the same time, Large Language Models (LLMs) bring new\ntechnological challenges and prerequisites in order to fully harness their\npotential. In this paper, we report work-in-progress on a systematic literature\nreview based on keyword searches from multiple large-scale databases as well as\ncitation networks that assesses how LLMs are currently being applied within the\nsurvey research process. We synthesize and organize our findings according to\nthe survey research process to include examples of LLM usage across three broad\nphases: pre-data collection, data collection, and post-data collection. We\ndiscuss selected examples of potential use cases for LLMs as well as its\npitfalls based on examples from existing literature. Considering survey\nresearch has rich experience and history regarding data quality, we discuss\nsome opportunities and describe future outlooks for survey research to\ncontribute to the continued development and refinement of LLMs."
                },
                "authors": [
                    {
                        "name": "Trent D. Buskirk"
                    },
                    {
                        "name": "Florian Keusch"
                    },
                    {
                        "name": "Leah von der Heyde"
                    },
                    {
                        "name": "Adam Eck"
                    }
                ],
                "author_detail": {
                    "name": "Adam Eck"
                },
                "author": "Adam Eck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02654v2",
                "updated": "2025-09-03T14:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    58,
                    7,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-03T14:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    18,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference\n  Infrastructure"
                },
                "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Sanchari Sen"
                    },
                    {
                        "name": "Swagath Venkataramani"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00096v2",
                "updated": "2025-09-03T14:58:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    58,
                    3,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T15:48:18Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    15,
                    48,
                    18,
                    2,
                    239,
                    0
                ],
                "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning\n  LLMs"
                },
                "summary": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Accepted to EMNLP2025 findings (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03381v1",
                "updated": "2025-09-03T14:57:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:57:21Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    21,
                    2,
                    246,
                    0
                ],
                "title": "Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle\n  Tutorial to Static Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle\n  Tutorial to Static Verification"
                },
                "summary": "Robot Operating System 2 (ROS 2) relies on the Data Distribution Service\n(DDS), which offers more than 20 Quality of Service (QoS) policies governing\navailability, reliability, and resource usage. Yet ROS 2 users lack clear\nguidance on safe policy combinations and validation processes prior to\ndeployment, which often leads to trial-and-error tuning and unexpected runtime\nfailures. To address these challenges, we analyze DDS Publisher-Subscriber\ncommunication over a life cycle divided into Discovery, Data Exchange, and\nDisassociation, and provide a user oriented tutorial explaining how 16 QoS\npolicies operate in each phase. Building on this analysis, we derive a QoS\ndependency chain that formalizes inter-policy relationships and classifies 41\ndependency violation rules, capturing constraints that commonly cause\ncommunication failures in practice. Finally, we introduce QoS Guard, a ROS 2\npackage that statically validates DDS XML profiles offline, flags conflicts,\nand enables safe, predeployment tuning without establishing a live ROS 2\nsession. Together, these contributions give ROS 2 users both conceptual insight\nand a concrete tool that enables early detection of misconfigurations,\nimproving the reliability and resource efficiency of ROS 2 based robotic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Operating System 2 (ROS 2) relies on the Data Distribution Service\n(DDS), which offers more than 20 Quality of Service (QoS) policies governing\navailability, reliability, and resource usage. Yet ROS 2 users lack clear\nguidance on safe policy combinations and validation processes prior to\ndeployment, which often leads to trial-and-error tuning and unexpected runtime\nfailures. To address these challenges, we analyze DDS Publisher-Subscriber\ncommunication over a life cycle divided into Discovery, Data Exchange, and\nDisassociation, and provide a user oriented tutorial explaining how 16 QoS\npolicies operate in each phase. Building on this analysis, we derive a QoS\ndependency chain that formalizes inter-policy relationships and classifies 41\ndependency violation rules, capturing constraints that commonly cause\ncommunication failures in practice. Finally, we introduce QoS Guard, a ROS 2\npackage that statically validates DDS XML profiles offline, flags conflicts,\nand enables safe, predeployment tuning without establishing a live ROS 2\nsession. Together, these contributions give ROS 2 users both conceptual insight\nand a concrete tool that enables early detection of misconfigurations,\nimproving the reliability and resource efficiency of ROS 2 based robotic\nsystems."
                },
                "authors": [
                    {
                        "name": "Sanghoon Lee"
                    },
                    {
                        "name": "Junha Kang"
                    },
                    {
                        "name": "Kyung-Joon Park"
                    }
                ],
                "author_detail": {
                    "name": "Kyung-Joon Park"
                },
                "author": "Kyung-Joon Park",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03380v1",
                "updated": "2025-09-03T14:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    57,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems"
                },
                "summary": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "7th International Workshop on Agent-Based Modelling of Human\n  Behaviour (ABMHuB'25), ALife 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93A16",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01550v2",
                "updated": "2025-09-03T14:56:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    47,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-03T02:34:16Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    2,
                    34,
                    16,
                    6,
                    215,
                    0
                ],
                "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End\n  Data Curation Pipeline Synergizing SFT and RL at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End\n  Data Curation Pipeline Synergizing SFT and RL at Scale"
                },
                "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks."
                },
                "authors": [
                    {
                        "name": "Zhilong Chen"
                    },
                    {
                        "name": "Chengzong Zhao"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Gustavo A. Oliva"
                    },
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Aaditya Bhatia"
                    },
                    {
                        "name": "Chong Chun Yong"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v1",
                "updated": "2025-09-03T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03363v1",
                "updated": "2025-09-03T14:46:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    46,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:46:57Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    46,
                    57,
                    2,
                    246,
                    0
                ],
                "title": "Variation-matching sensitivity-based virtual fields for hyperelastic\n  material model calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation-matching sensitivity-based virtual fields for hyperelastic\n  material model calibration"
                },
                "summary": "Accurate identification of nonlinear material parameters from\nthree-dimensional full-field deformation data remains a challenge in\nexperimental mechanics. The virtual fields method (VFM) provides a powerful,\ncomputationally efficient approach for material model calibration, however, its\nsuccess depends critically on the choice of virtual fields and the\ninformativeness of available kinematic data. In this work, we advance the\nstate-of-the-art discrete formulation of the sensitivity-based virtual fields\n(SBVF) method by systematically developing and comparing alternative\nvariational and analytical SBVFs within a strain-invariant-based modeling\nframework.\n  A central contribution of this work is the implementation and assessment of\nvariation-based SBVFs (vSBVFs), formulated using directional G\\^ateaux\nderivatives, as well as virtual fields derived from analytical differentiation\n(aSBVFs) which provide explicit, model-tailored virtual displacement fields for\nparameter identification. Using simulated noisy volumetric datasets, we\ndemonstrate that vSBVFs and aSBVFs enable procedural, automated construction of\noptimal virtual fields for each material parameter, substantially enhancing the\nrobustness and efficiency of calibration without the need for manual field\nselection or high temporal resolution in the data acquisition. We quantify data\nrichness -- the effective diversity of sampled kinematic states -- showing that\nincreased data richness via sample geometry and loading protocols leads to\nimproved parameter identifiability. These findings establish a pathway for\nautomated, noise-robust material model calibration suitable for future\ndeployment with experimental full-field imaging of soft, complex materials, and\nprovide a foundation for optimizing shape topology and extending to\nviscoelastic and anisotropic behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of nonlinear material parameters from\nthree-dimensional full-field deformation data remains a challenge in\nexperimental mechanics. The virtual fields method (VFM) provides a powerful,\ncomputationally efficient approach for material model calibration, however, its\nsuccess depends critically on the choice of virtual fields and the\ninformativeness of available kinematic data. In this work, we advance the\nstate-of-the-art discrete formulation of the sensitivity-based virtual fields\n(SBVF) method by systematically developing and comparing alternative\nvariational and analytical SBVFs within a strain-invariant-based modeling\nframework.\n  A central contribution of this work is the implementation and assessment of\nvariation-based SBVFs (vSBVFs), formulated using directional G\\^ateaux\nderivatives, as well as virtual fields derived from analytical differentiation\n(aSBVFs) which provide explicit, model-tailored virtual displacement fields for\nparameter identification. Using simulated noisy volumetric datasets, we\ndemonstrate that vSBVFs and aSBVFs enable procedural, automated construction of\noptimal virtual fields for each material parameter, substantially enhancing the\nrobustness and efficiency of calibration without the need for manual field\nselection or high temporal resolution in the data acquisition. We quantify data\nrichness -- the effective diversity of sampled kinematic states -- showing that\nincreased data richness via sample geometry and loading protocols leads to\nimproved parameter identifiability. These findings establish a pathway for\nautomated, noise-robust material model calibration suitable for future\ndeployment with experimental full-field imaging of soft, complex materials, and\nprovide a foundation for optimizing shape topology and extending to\nviscoelastic and anisotropic behaviors."
                },
                "authors": [
                    {
                        "name": "Denislav P. Nikolov"
                    },
                    {
                        "name": "Zhiren Zhu"
                    },
                    {
                        "name": "Jonathan B. Estrada"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan B. Estrada"
                },
                "author": "Jonathan B. Estrada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23751v2",
                "updated": "2025-09-03T14:36:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    36,
                    0,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-31T17:38:50Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    38,
                    50,
                    3,
                    212,
                    0
                ],
                "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks"
                },
                "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18721v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18721v5",
                "updated": "2025-09-03T14:34:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    34,
                    30,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-26T06:38:38Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    6,
                    38,
                    38,
                    1,
                    238,
                    0
                ],
                "title": "LLM as an Execution Estimator: Recovering Missing Dependency for\n  Practical Time-travelling Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as an Execution Estimator: Recovering Missing Dependency for\n  Practical Time-travelling Debugging"
                },
                "summary": "Determining the dynamic data dependency of a step that reads a variable $v$\nis challenging. It typically requires either exhaustive instrumentation, which\nbecomes prohibitively expensive when $v$ is defined within library calls, or\nrepeated executions, which are impractical for non-deterministic programs. In\nthis work, we propose RecovSlicing for computing dynamic data dependency in a\nsingle run, with only partial instrumentation. We explore the intuition that\nLLM can potentially infer program dynamics based on a partially recorded trace\nand relevant code as its context. Given (1) a partially recorded trace of a\nprogram $P$ and (2) the slicing criteria consisting of a query step $s$ and a\nquery variable $v$ read by $s$, RecovSlicing computes the runtime definition of\n$v$ on the trace by estimating the miss-recorded execution of $P$. In this\nwork, we allow the user to specify implicit query variable. Technically, built\nupon non-deterministic LLM, we address the challenges of (1) precise recovery\nof runtime variable value and structure from the recorded execution and (2)\naligning the memory address of recovered variables and the recorded variables\nfor definition analysis. We evaluate RecovSlicing on 8300 data dependencies\nacross three slicing benchmarks, comparing it with Slicer4J, ND-Slicer, LLM\nSlicer, and re-execution Slicer. RecovSlicing achieves significantly higher\naccuracy (80.3%, 91.1%, 98.3%) and recall (up to 98.3%) than the best baseline\n(accuracy: 39.0%, 82.0%, 59.9%; recall: 53.4%, 79.1%, 87.1%). Integrated into a\ndual-slicing regression bug localizer, it identifies 16% more regressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the dynamic data dependency of a step that reads a variable $v$\nis challenging. It typically requires either exhaustive instrumentation, which\nbecomes prohibitively expensive when $v$ is defined within library calls, or\nrepeated executions, which are impractical for non-deterministic programs. In\nthis work, we propose RecovSlicing for computing dynamic data dependency in a\nsingle run, with only partial instrumentation. We explore the intuition that\nLLM can potentially infer program dynamics based on a partially recorded trace\nand relevant code as its context. Given (1) a partially recorded trace of a\nprogram $P$ and (2) the slicing criteria consisting of a query step $s$ and a\nquery variable $v$ read by $s$, RecovSlicing computes the runtime definition of\n$v$ on the trace by estimating the miss-recorded execution of $P$. In this\nwork, we allow the user to specify implicit query variable. Technically, built\nupon non-deterministic LLM, we address the challenges of (1) precise recovery\nof runtime variable value and structure from the recorded execution and (2)\naligning the memory address of recovered variables and the recorded variables\nfor definition analysis. We evaluate RecovSlicing on 8300 data dependencies\nacross three slicing benchmarks, comparing it with Slicer4J, ND-Slicer, LLM\nSlicer, and re-execution Slicer. RecovSlicing achieves significantly higher\naccuracy (80.3%, 91.1%, 98.3%) and recall (up to 98.3%) than the best baseline\n(accuracy: 39.0%, 82.0%, 59.9%; recall: 53.4%, 79.1%, 87.1%). Integrated into a\ndual-slicing regression bug localizer, it identifies 16% more regressions."
                },
                "authors": [
                    {
                        "name": "Yunrui Pei"
                    },
                    {
                        "name": "Hongshu Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Weiyu Kong"
                    },
                    {
                        "name": "Jin song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin song Dong"
                },
                "author": "Jin song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18721v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18721v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03345v1",
                "updated": "2025-09-03T14:22:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    22,
                    42,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:22:42Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    22,
                    42,
                    2,
                    246,
                    0
                ],
                "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning"
                },
                "summary": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR."
                },
                "authors": [
                    {
                        "name": "Yunxin Sun"
                    },
                    {
                        "name": "Abulhair Saparov"
                    }
                ],
                "author_detail": {
                    "name": "Abulhair Saparov"
                },
                "author": "Abulhair Saparov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03336v1",
                "updated": "2025-09-03T14:12:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    12,
                    32,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:12:32Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    12,
                    32,
                    2,
                    246,
                    0
                ],
                "title": "AI-Driven Drug Repurposing through miRNA-mRNA Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Drug Repurposing through miRNA-mRNA Relation"
                },
                "summary": "miRNA mRNA relations are closely linked to several biological processes and\ndisease mechanisms In a recent study we tested the performance of large\nlanguage models LLMs on extracting miRNA mRNA relations from PubMed PubMedBERT\nachieved the best performance of 0.783 F1 score for miRNA mRNA Interaction\nCorpus MMIC Here we first applied the finetuned PubMedBERT model to extract\nmiRNA mRNA relations from PubMed for chronic obstructive pulmonary disease COPD\nAlzheimers disease AD stroke type 2 diabetes mellitus T2DM chronic liver\ndisease and cancer Next we retrieved miRNA drug relations using KinderMiner a\nliterature mining tool for relation extraction Then we constructed three\ninteraction networks 1 disease centric network 2 drug centric network and 3\nmiRNA centric network comprising 3497 nodes and 16417 edges organized as a\ndirected graph to capture complex biological relationships Finally we validated\nthe drugs using MIMIC IV Our integrative approach revealed both established and\nnovel candidate drugs for diseases under study through 595 miRNA drug relations\nextracted from PubMed To the best of our knowledge this is the first study to\nsystematically extract and visualize relationships among four distinct\nbiomedical entities miRNA mRNA drug and disease",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "miRNA mRNA relations are closely linked to several biological processes and\ndisease mechanisms In a recent study we tested the performance of large\nlanguage models LLMs on extracting miRNA mRNA relations from PubMed PubMedBERT\nachieved the best performance of 0.783 F1 score for miRNA mRNA Interaction\nCorpus MMIC Here we first applied the finetuned PubMedBERT model to extract\nmiRNA mRNA relations from PubMed for chronic obstructive pulmonary disease COPD\nAlzheimers disease AD stroke type 2 diabetes mellitus T2DM chronic liver\ndisease and cancer Next we retrieved miRNA drug relations using KinderMiner a\nliterature mining tool for relation extraction Then we constructed three\ninteraction networks 1 disease centric network 2 drug centric network and 3\nmiRNA centric network comprising 3497 nodes and 16417 edges organized as a\ndirected graph to capture complex biological relationships Finally we validated\nthe drugs using MIMIC IV Our integrative approach revealed both established and\nnovel candidate drugs for diseases under study through 595 miRNA drug relations\nextracted from PubMed To the best of our knowledge this is the first study to\nsystematically extract and visualize relationships among four distinct\nbiomedical entities miRNA mRNA drug and disease"
                },
                "authors": [
                    {
                        "name": "Sharanya Manoharan"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Oviya Ramalakshmi Iyyappan"
                    },
                    {
                        "name": "Mohamed Saleem Abdul Shukkoor"
                    },
                    {
                        "name": "Malathi Sellapan"
                    },
                    {
                        "name": "Kalpana Raja"
                    }
                ],
                "author_detail": {
                    "name": "Kalpana Raja"
                },
                "author": "Kalpana Raja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03335v2",
                "updated": "2025-09-04T09:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    25,
                    5,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T14:10:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    10,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms"
                },
                "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03331v1",
                "updated": "2025-09-03T14:06:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    6,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:06:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    6,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing\n  Large Language Model Vulnerability Repair Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing\n  Large Language Model Vulnerability Repair Capabilities"
                },
                "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios."
                },
                "authors": [
                    {
                        "name": "Weizhe Wang"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Jianfei Sun"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Guangquan Xu"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03329v1",
                "updated": "2025-09-03T14:04:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    4,
                    51,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:04:51Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    4,
                    51,
                    2,
                    246,
                    0
                ],
                "title": "SESGO: Spanish Evaluation of Stereotypical Generative Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SESGO: Spanish Evaluation of Stereotypical Generative Outputs"
                },
                "summary": "This paper addresses the critical gap in evaluating bias in multilingual\nLarge Language Models (LLMs), with a specific focus on Spanish language within\nculturally-aware Latin American contexts. Despite widespread global deployment,\ncurrent evaluations remain predominantly US-English-centric, leaving potential\nharms in other linguistic and cultural contexts largely underexamined. We\nintroduce a novel, culturally-grounded framework for detecting social biases in\ninstruction-tuned LLMs. Our approach adapts the underspecified question\nmethodology from the BBQ dataset by incorporating culturally-specific\nexpressions and sayings that encode regional stereotypes across four social\ncategories: gender, race, socioeconomic class, and national origin. Using more\nthan 4,000 prompts, we propose a new metric that combines accuracy with the\ndirection of error to effectively balance model performance and bias alignment\nin both ambiguous and disambiguated contexts. To our knowledge, our work\npresents the first systematic evaluation examining how leading commercial LLMs\nrespond to culturally specific bias in the Spanish language, revealing varying\npatterns of bias manifestation across state-of-the-art models. We also\ncontribute evidence that bias mitigation techniques optimized for English do\nnot effectively transfer to Spanish tasks, and that bias patterns remain\nlargely consistent across different sampling temperatures. Our modular\nframework offers a natural extension to new stereotypes, bias categories, or\nlanguages and cultural contexts, representing a significant step toward more\nequitable and culturally-aware evaluation of AI systems in the diverse\nlinguistic environments where they operate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical gap in evaluating bias in multilingual\nLarge Language Models (LLMs), with a specific focus on Spanish language within\nculturally-aware Latin American contexts. Despite widespread global deployment,\ncurrent evaluations remain predominantly US-English-centric, leaving potential\nharms in other linguistic and cultural contexts largely underexamined. We\nintroduce a novel, culturally-grounded framework for detecting social biases in\ninstruction-tuned LLMs. Our approach adapts the underspecified question\nmethodology from the BBQ dataset by incorporating culturally-specific\nexpressions and sayings that encode regional stereotypes across four social\ncategories: gender, race, socioeconomic class, and national origin. Using more\nthan 4,000 prompts, we propose a new metric that combines accuracy with the\ndirection of error to effectively balance model performance and bias alignment\nin both ambiguous and disambiguated contexts. To our knowledge, our work\npresents the first systematic evaluation examining how leading commercial LLMs\nrespond to culturally specific bias in the Spanish language, revealing varying\npatterns of bias manifestation across state-of-the-art models. We also\ncontribute evidence that bias mitigation techniques optimized for English do\nnot effectively transfer to Spanish tasks, and that bias patterns remain\nlargely consistent across different sampling temperatures. Our modular\nframework offers a natural extension to new stereotypes, bias categories, or\nlanguages and cultural contexts, representing a significant step toward more\nequitable and culturally-aware evaluation of AI systems in the diverse\nlinguistic environments where they operate."
                },
                "authors": [
                    {
                        "name": "Melissa Robles"
                    },
                    {
                        "name": "Catalina Bernal"
                    },
                    {
                        "name": "Denniss Raigoso"
                    },
                    {
                        "name": "Mateo Dulce Rubio"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Dulce Rubio"
                },
                "author": "Mateo Dulce Rubio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03321v1",
                "updated": "2025-09-03T13:53:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    53,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:53:29Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    53,
                    29,
                    2,
                    246,
                    0
                ],
                "title": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT"
                },
                "summary": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Linyu Ou"
                    }
                ],
                "author_detail": {
                    "name": "Linyu Ou"
                },
                "author": "Linyu Ou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04387v2",
                "updated": "2025-09-03T13:46:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    46,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-02-05T21:36:21Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    36,
                    21,
                    2,
                    36,
                    0
                ],
                "title": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual\n  LLMs"
                },
                "summary": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft."
                },
                "authors": [
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Minyoung Kim"
                    },
                    {
                        "name": "Fady Rezk"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03312v1",
                "updated": "2025-09-03T13:42:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    42,
                    14,
                    2,
                    246,
                    0
                ],
                "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"
                },
                "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03310v1",
                "updated": "2025-09-03T13:41:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    41,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:41:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    41,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding"
                },
                "summary": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems."
                },
                "authors": [
                    {
                        "name": "Evgenii Kniazev"
                    },
                    {
                        "name": "Arseny Kravchenko"
                    },
                    {
                        "name": "Igor Rekun"
                    },
                    {
                        "name": "James Broadhead"
                    },
                    {
                        "name": "Nikita Shamgunov"
                    },
                    {
                        "name": "Pranav Sah"
                    },
                    {
                        "name": "Pratik Nichite"
                    },
                    {
                        "name": "Ivan Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Yamshchikov"
                },
                "author": "Ivan Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03300v1",
                "updated": "2025-09-03T13:25:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    25,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:25:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    25,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "LatPhon: Lightweight Multilingual G2P for Romance Languages and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatPhon: Lightweight Multilingual G2P for Romance Languages and English"
                },
                "summary": "Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech\n(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)\nand alignment systems, especially across multiple Latin-script languages.We\npresent LatPhon, a 7.5 M - parameter Transformer jointly trained on six such\nlanguages--English, Spanish, French, Italian, Portuguese, and Romanian. On the\npublic ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,\noutperforming the byte-level ByT5 baseline (5.4%) and approaching\nlanguage-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes\non-device deployment feasible when needed. These results indicate that compact\nmultilingual G2P can serve as a universal front-end for Latin-language speech\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech\n(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)\nand alignment systems, especially across multiple Latin-script languages.We\npresent LatPhon, a 7.5 M - parameter Transformer jointly trained on six such\nlanguages--English, Spanish, French, Italian, Portuguese, and Romanian. On the\npublic ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,\noutperforming the byte-level ByT5 baseline (5.4%) and approaching\nlanguage-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes\non-device deployment feasible when needed. These results indicate that compact\nmultilingual G2P can serve as a universal front-end for Latin-language speech\npipelines."
                },
                "authors": [
                    {
                        "name": "Luis Felipe Chary"
                    },
                    {
                        "name": "Miguel Arjona Ramirez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Arjona Ramirez"
                },
                "author": "Miguel Arjona Ramirez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03290v1",
                "updated": "2025-09-03T13:18:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    18,
                    41,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T13:18:41Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    13,
                    18,
                    41,
                    2,
                    246,
                    0
                ],
                "title": "Machine Learning-Driven Anomaly Detection for 5G O-RAN Performance\n  Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning-Driven Anomaly Detection for 5G O-RAN Performance\n  Metrics"
                },
                "summary": "The ever-increasing reliance of critical services on network infrastructure\ncoupled with the increased operational complexity of beyond-5G/6G networks\nnecessitate the need for proactive and automated network fault management. The\nprovision for open interfaces among different radio access network\\,(RAN)\nelements and the integration of AI/ML into network architecture enabled by the\nOpen RAN\\,(O-RAN) specifications bring new possibilities for active network\nhealth monitoring and anomaly detection. In this paper we leverage these\nadvantages and develop an anomaly detection framework that proactively detect\nthe possible throughput drops for a UE and minimize the post-handover failures.\nWe propose two actionable anomaly detection algorithms tailored for real-world\ndeployment. The first algorithm identifies user equipment (UE) at risk of\nsevere throughput degradation by analyzing key performance indicators (KPIs)\nsuch as resource block utilization and signal quality metrics, enabling\nproactive handover initiation. The second algorithm evaluates neighbor cell\nradio coverage quality, filtering out cells with anomalous signal strength or\ninterference levels. This reduces candidate targets for handover by 41.27\\% on\naverage. Together, these methods mitigate post-handover failures and throughput\ndrops while operating much faster than the near-real-time latency constraints.\nThis paves the way for self-healing 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing reliance of critical services on network infrastructure\ncoupled with the increased operational complexity of beyond-5G/6G networks\nnecessitate the need for proactive and automated network fault management. The\nprovision for open interfaces among different radio access network\\,(RAN)\nelements and the integration of AI/ML into network architecture enabled by the\nOpen RAN\\,(O-RAN) specifications bring new possibilities for active network\nhealth monitoring and anomaly detection. In this paper we leverage these\nadvantages and develop an anomaly detection framework that proactively detect\nthe possible throughput drops for a UE and minimize the post-handover failures.\nWe propose two actionable anomaly detection algorithms tailored for real-world\ndeployment. The first algorithm identifies user equipment (UE) at risk of\nsevere throughput degradation by analyzing key performance indicators (KPIs)\nsuch as resource block utilization and signal quality metrics, enabling\nproactive handover initiation. The second algorithm evaluates neighbor cell\nradio coverage quality, filtering out cells with anomalous signal strength or\ninterference levels. This reduces candidate targets for handover by 41.27\\% on\naverage. Together, these methods mitigate post-handover failures and throughput\ndrops while operating much faster than the near-real-time latency constraints.\nThis paves the way for self-healing 6G networks."
                },
                "authors": [
                    {
                        "name": "Babak Azkaei"
                    },
                    {
                        "name": "Kishor Chandra Joshi"
                    },
                    {
                        "name": "George Exarchakos"
                    }
                ],
                "author_detail": {
                    "name": "George Exarchakos"
                },
                "author": "George Exarchakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03269v1",
                "updated": "2025-09-03T12:38:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    12,
                    38,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T12:38:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    12,
                    38,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "Bridging Gaps Between Student and Expert Evaluations of AI-Generated\n  Programming Hints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Gaps Between Student and Expert Evaluations of AI-Generated\n  Programming Hints"
                },
                "summary": "Generative AI has the potential to enhance education by providing\npersonalized feedback to students at scale. Recent work has proposed techniques\nto improve AI-generated programming hints and has evaluated their performance\nbased on expert-designed rubrics or student ratings. However, it remains\nunclear how the rubrics used to design these techniques align with students'\nperceived helpfulness of hints. In this paper, we systematically study the\nmismatches in perceived hint quality from students' and experts' perspectives\nbased on the deployment of AI-generated hints in a Python programming course.\nWe analyze scenarios with discrepancies between student and expert evaluations,\nin particular, where experts rated a hint as high-quality while the student\nfound it unhelpful. We identify key reasons for these discrepancies and\nclassify them into categories, such as hints not accounting for the student's\nmain concern or not considering previous help requests. Finally, we propose and\ndiscuss preliminary results on potential methods to bridge these gaps, first by\nextending the expert-designed quality rubric and then by adapting the hint\ngeneration process, e.g., incorporating the student's comments or history.\nThese efforts contribute toward scalable, personalized, and pedagogically sound\nAI-assisted feedback systems, which are particularly important for\nhigh-enrollment educational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has the potential to enhance education by providing\npersonalized feedback to students at scale. Recent work has proposed techniques\nto improve AI-generated programming hints and has evaluated their performance\nbased on expert-designed rubrics or student ratings. However, it remains\nunclear how the rubrics used to design these techniques align with students'\nperceived helpfulness of hints. In this paper, we systematically study the\nmismatches in perceived hint quality from students' and experts' perspectives\nbased on the deployment of AI-generated hints in a Python programming course.\nWe analyze scenarios with discrepancies between student and expert evaluations,\nin particular, where experts rated a hint as high-quality while the student\nfound it unhelpful. We identify key reasons for these discrepancies and\nclassify them into categories, such as hints not accounting for the student's\nmain concern or not considering previous help requests. Finally, we propose and\ndiscuss preliminary results on potential methods to bridge these gaps, first by\nextending the expert-designed quality rubric and then by adapting the hint\ngeneration process, e.g., incorporating the student's comments or history.\nThese efforts contribute toward scalable, personalized, and pedagogically sound\nAI-assisted feedback systems, which are particularly important for\nhigh-enrollment educational settings."
                },
                "authors": [
                    {
                        "name": "Tung Phung"
                    },
                    {
                        "name": "Mengyan Wu"
                    },
                    {
                        "name": "Heeryung Choi"
                    },
                    {
                        "name": "Gustavo Soares"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Christopher Brooks"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Brooks"
                },
                "author": "Christopher Brooks",
                "arxiv_comment": "L@S'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01326v2",
                "updated": "2025-09-03T12:09:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    12,
                    9,
                    34,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-02T05:11:21Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    11,
                    21,
                    0,
                    153,
                    0
                ],
                "title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for\n  Operations Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for\n  Operations Research"
                },
                "summary": "Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Bokui Chen"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Qingxing Cao"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Accepted by Annual Meetings of the Association for Computational\n  Linguistics 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03234v1",
                "updated": "2025-09-03T11:46:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    46,
                    24,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:46:24Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    46,
                    24,
                    2,
                    246,
                    0
                ],
                "title": "TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of\n  Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have significantly reduced the number of trainable parameters needed in\nfine-tuning large language models (LLMs). Subsequent developments of LoRA-style\nadapters have diverged into two main directions: (1) enhancing model\nexpressivity with high-rank adapters, and (2) pushing for further parameter\nreduction, as exemplified by vector-based methods. However, these approaches\npresent a trade-off, as achieving the expressivity of high-rank weight updates\ntypically comes at the cost of sacrificing the extreme parameter efficiency\noffered by vector-based techniques. To address this issue, we propose a\nvector-based random \\underline{\\textbf{Te}}nsor network for\nhigh-\\underline{\\textbf{R}}ank \\underline{\\textbf{A}}daptation (TeRA), a novel\nPEFT method that achieves high-rank weight updates while retaining the\nparameter efficiency of vector-based PEFT adapters. This is achieved by\nparameterizing the tensorized weight update matrix as a Tucker-like tensor\nnetwork (TN), in which large randomly initialized factors are frozen and shared\nacross layers, while only small layer-specific scaling vectors, formed by\nentries in diagonal factor matrices, are trained. This design effectively\ndecouples the rank of the weight update matrix from the number of trainable\nparameters. Comprehensive experiments demonstrate that TeRA matches or even\noutperforms high-rank adapters, while requiring a trainable parameter count\nsimilar to vector-based methods. Theoretical analysis and ablation studies\nfurther validate the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have significantly reduced the number of trainable parameters needed in\nfine-tuning large language models (LLMs). Subsequent developments of LoRA-style\nadapters have diverged into two main directions: (1) enhancing model\nexpressivity with high-rank adapters, and (2) pushing for further parameter\nreduction, as exemplified by vector-based methods. However, these approaches\npresent a trade-off, as achieving the expressivity of high-rank weight updates\ntypically comes at the cost of sacrificing the extreme parameter efficiency\noffered by vector-based techniques. To address this issue, we propose a\nvector-based random \\underline{\\textbf{Te}}nsor network for\nhigh-\\underline{\\textbf{R}}ank \\underline{\\textbf{A}}daptation (TeRA), a novel\nPEFT method that achieves high-rank weight updates while retaining the\nparameter efficiency of vector-based PEFT adapters. This is achieved by\nparameterizing the tensorized weight update matrix as a Tucker-like tensor\nnetwork (TN), in which large randomly initialized factors are frozen and shared\nacross layers, while only small layer-specific scaling vectors, formed by\nentries in diagonal factor matrices, are trained. This design effectively\ndecouples the rank of the weight update matrix from the number of trainable\nparameters. Comprehensive experiments demonstrate that TeRA matches or even\noutperforms high-rank adapters, while requiring a trainable parameter count\nsimilar to vector-based methods. Theoretical analysis and ablation studies\nfurther validate the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05225v2",
                "updated": "2025-09-03T11:11:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    11,
                    28,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-08T13:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation"
                },
                "summary": "The rapid advancement of Chinese LLMs underscores the need for\nvertical-domain evaluations to ensure reliable applications. However, existing\nbenchmarks often lack domain coverage and provide limited insights into the\nChinese working context. Leveraging qualification exams as a unified framework\nfor expertise evaluation, we introduce QualBench, the first multi-domain\nChinese QA benchmark dedicated to localized assessment of Chinese LLMs. The\ndataset includes over 17,000 questions across six vertical domains, drawn from\n24 Chinese qualifications to align with national policies and professional\nstandards. Results reveal an interesting pattern of Chinese LLMs consistently\nsurpassing non-Chinese models, with the Qwen2.5 model outperforming the more\nadvanced GPT-4o, emphasizing the value of localized domain knowledge in meeting\nqualification requirements. The average accuracy of 53.98% reveals the current\ngaps in domain coverage within model capabilities. Furthermore, we identify\nperformance degradation caused by LLM crowdsourcing, assess data contamination,\nand illustrate the effectiveness of prompt engineering and model fine-tuning,\nsuggesting opportunities for future improvements through multi-domain RAG and\nFederated Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Chinese LLMs underscores the need for\nvertical-domain evaluations to ensure reliable applications. However, existing\nbenchmarks often lack domain coverage and provide limited insights into the\nChinese working context. Leveraging qualification exams as a unified framework\nfor expertise evaluation, we introduce QualBench, the first multi-domain\nChinese QA benchmark dedicated to localized assessment of Chinese LLMs. The\ndataset includes over 17,000 questions across six vertical domains, drawn from\n24 Chinese qualifications to align with national policies and professional\nstandards. Results reveal an interesting pattern of Chinese LLMs consistently\nsurpassing non-Chinese models, with the Qwen2.5 model outperforming the more\nadvanced GPT-4o, emphasizing the value of localized domain knowledge in meeting\nqualification requirements. The average accuracy of 53.98% reveals the current\ngaps in domain coverage within model capabilities. Furthermore, we identify\nperformance degradation caused by LLM crowdsourcing, assess data contamination,\nand illustrate the effectiveness of prompt engineering and model fine-tuning,\nsuggesting opportunities for future improvements through multi-domain RAG and\nFederated Learning."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "arxiv_comment": "Accepted by EMNLP 2025 Main Conference. Homepage:\n  https://github.com/mengze-hong/QualBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09049v4",
                "updated": "2025-09-03T11:09:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    9,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2024-12-12T08:19:01Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    19,
                    1,
                    3,
                    347,
                    0
                ],
                "title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues"
                },
                "summary": "Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "arxiv_comment": "Accepted by EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03212v1",
                "updated": "2025-09-03T11:00:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    0,
                    46,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:00:46Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    0,
                    46,
                    2,
                    246,
                    0
                ],
                "title": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have significantly improved\nnatural language understanding and generation, enhancing Human-Computer\nInteraction (HCI). However, LLMs are limited to unimodal text processing and\nlack the ability to interpret emotional cues from non-verbal signals, hindering\nmore immersive and empathetic interactions. This work explores integrating\nmultimodal sentiment perception into LLMs to create emotion-aware agents. We\npropose \\ours, an AI-based virtual companion that captures multimodal sentiment\ncues, enabling emotionally aligned and animated HCI. \\ours introduces a\nMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusion\ntransformer and supervised contrastive learning to provide emotional cues.\nAdditionally, we develop an emotion-aware prompt engineering strategy for\ngenerating empathetic responses and integrate a Text-to-Speech (TTS) system and\nanimated avatar module for expressive interactions. \\ours provides a framework\nfor emotion-aware agents with applications in companion robotics, social care,\nmental health, and human-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have significantly improved\nnatural language understanding and generation, enhancing Human-Computer\nInteraction (HCI). However, LLMs are limited to unimodal text processing and\nlack the ability to interpret emotional cues from non-verbal signals, hindering\nmore immersive and empathetic interactions. This work explores integrating\nmultimodal sentiment perception into LLMs to create emotion-aware agents. We\npropose \\ours, an AI-based virtual companion that captures multimodal sentiment\ncues, enabling emotionally aligned and animated HCI. \\ours introduces a\nMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusion\ntransformer and supervised contrastive learning to provide emotional cues.\nAdditionally, we develop an emotion-aware prompt engineering strategy for\ngenerating empathetic responses and integrate a Text-to-Speech (TTS) system and\nanimated avatar module for expressive interactions. \\ours provides a framework\nfor emotion-aware agents with applications in companion robotics, social care,\nmental health, and human-centered AI."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Li"
                },
                "author": "Chenxi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02327v2",
                "updated": "2025-09-03T10:56:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    56,
                    13,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T13:53:09Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    53,
                    9,
                    1,
                    245,
                    0
                ],
                "title": "Variational Uncertainty Decomposition for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Uncertainty Decomposition for In-Context Learning"
                },
                "summary": "As large language models (LLMs) gain popularity in conducting prediction\ntasks in-context, understanding the sources of uncertainty in in-context\nlearning becomes essential to ensuring reliability. The recent hypothesis of\nin-context learning performing predictive Bayesian inference opens the avenue\nfor Bayesian uncertainty estimation, particularly for decomposing uncertainty\ninto epistemic uncertainty due to lack of in-context data and aleatoric\nuncertainty inherent in the in-context prediction task. However, the\ndecomposition idea remains under-explored due to the intractability of the\nlatent parameter posterior from the underlying Bayesian model. In this work, we\nintroduce a variational uncertainty decomposition framework for in-context\nlearning without explicitly sampling from the latent parameter posterior, by\noptimising auxiliary queries as probes to obtain an upper bound to the\naleatoric uncertainty of an LLM's in-context learning procedure, which also\ninduces a lower bound to the epistemic uncertainty. Through experiments on\nsynthetic and real-world tasks, we show quantitatively and qualitatively that\nthe decomposed uncertainties obtained from our method exhibit desirable\nproperties of epistemic and aleatoric uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain popularity in conducting prediction\ntasks in-context, understanding the sources of uncertainty in in-context\nlearning becomes essential to ensuring reliability. The recent hypothesis of\nin-context learning performing predictive Bayesian inference opens the avenue\nfor Bayesian uncertainty estimation, particularly for decomposing uncertainty\ninto epistemic uncertainty due to lack of in-context data and aleatoric\nuncertainty inherent in the in-context prediction task. However, the\ndecomposition idea remains under-explored due to the intractability of the\nlatent parameter posterior from the underlying Bayesian model. In this work, we\nintroduce a variational uncertainty decomposition framework for in-context\nlearning without explicitly sampling from the latent parameter posterior, by\noptimising auxiliary queries as probes to obtain an upper bound to the\naleatoric uncertainty of an LLM's in-context learning procedure, which also\ninduces a lower bound to the epistemic uncertainty. Through experiments on\nsynthetic and real-world tasks, we show quantitatively and qualitatively that\nthe decomposed uncertainties obtained from our method exhibit desirable\nproperties of epistemic and aleatoric uncertainty."
                },
                "authors": [
                    {
                        "name": "I. Shavindra Jayasekera"
                    },
                    {
                        "name": "Jacob Si"
                    },
                    {
                        "name": "Filippo Valdettaro"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "Fixing author order; typo p.20",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09997v3",
                "updated": "2025-09-03T10:48:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    48,
                    53,
                    2,
                    246,
                    0
                ],
                "published": "2025-01-17T07:30:01Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models"
                },
                "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02170v2",
                "updated": "2025-09-03T10:39:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    39,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-02T10:22:46Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    10,
                    22,
                    46,
                    1,
                    245,
                    0
                ],
                "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoidance Decoding for Diverse Multi-Branch Story Generation"
                },
                "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity."
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Nakyeong Yang"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16822v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16822v5",
                "updated": "2025-09-03T10:35:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    10,
                    35,
                    17,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-22T08:48:52Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    48,
                    52,
                    1,
                    296,
                    0
                ],
                "title": "Can Large Language Models Act as Ensembler for Multi-GNNs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Act as Ensembler for Multi-GNNs?"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://github.com/AquariusAQ/LensGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://github.com/AquariusAQ/LensGNN."
                },
                "authors": [
                    {
                        "name": "Hanqi Duan"
                    },
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16822v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16822v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17695v2",
                "updated": "2025-09-03T09:49:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    49,
                    56,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-23T17:01:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    1,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks"
                },
                "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance."
                },
                "authors": [
                    {
                        "name": "Ilias Chatzistefanidis"
                    },
                    {
                        "name": "Navid Nikaein"
                    }
                ],
                "author_detail": {
                    "name": "Navid Nikaein"
                },
                "author": "Navid Nikaein",
                "arxiv_comment": "Submitted to Computer Networks AI for 6G",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19828v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19828v3",
                "updated": "2025-09-03T09:33:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    33,
                    30,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T12:26:55Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    26,
                    55,
                    2,
                    239,
                    0
                ],
                "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations,\nincluding adding, updating, deleting, or taking no operation on memory entries;\nand an Answer Agent that selects the most relevant entries and reasons over\nthem to produce an answer. Both agents are fine-tuned with outcome-driven RL\n(PPO and GRPO), enabling adaptive memory management and utilization with\nminimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the\nstrongest existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behavior in LLMs, pointing toward richer, more persistent\nreasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations,\nincluding adding, updating, deleting, or taking no operation on memory entries;\nand an Answer Agent that selects the most relevant entries and reasons over\nthem to produce an answer. Both agents are fine-tuned with outcome-driven RL\n(PPO and GRPO), enabling adaptive memory management and utilization with\nminimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the\nstrongest existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behavior in LLMs, pointing toward richer, more persistent\nreasoning systems."
                },
                "authors": [
                    {
                        "name": "Sikuan Yan"
                    },
                    {
                        "name": "Xiufeng Yang"
                    },
                    {
                        "name": "Zuchao Huang"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Zonggen Li"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19828v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19828v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01415v3",
                "updated": "2025-09-03T09:32:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    27,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-02T15:39:42Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    15,
                    39,
                    42,
                    5,
                    214,
                    0
                ],
                "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems"
                },
                "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots."
                },
                "authors": [
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Honghao Cai"
                    },
                    {
                        "name": "Binbin Que"
                    },
                    {
                        "name": "Zezhou Cui"
                    },
                    {
                        "name": "Liangchen Tan"
                    },
                    {
                        "name": "Junkun Hong"
                    },
                    {
                        "name": "Gehan Hu"
                    },
                    {
                        "name": "Shuangyu Zhu"
                    },
                    {
                        "name": "Yimou Wu"
                    },
                    {
                        "name": "Shaohan Jiang"
                    },
                    {
                        "name": "Ge Wang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Yatong Han"
                    }
                ],
                "author_detail": {
                    "name": "Yatong Han"
                },
                "author": "Yatong Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08040v2",
                "updated": "2025-09-03T09:32:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    32,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-11T14:42:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models"
                },
                "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Maozhen Zhang"
                    },
                    {
                        "name": "Mengnan Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03164v1",
                "updated": "2025-09-03T09:30:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:30:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models"
                },
                "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."
                },
                "authors": [
                    {
                        "name": "Sangbong Yoo"
                    },
                    {
                        "name": "Seongbum Seo"
                    },
                    {
                        "name": "Chanyoung Yoon"
                    },
                    {
                        "name": "Hyelim Lee"
                    },
                    {
                        "name": "Jeong-Nam Kim"
                    },
                    {
                        "name": "Chansoo Kim"
                    },
                    {
                        "name": "Yun Jang"
                    },
                    {
                        "name": "Takanori Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takanori Fujiwara"
                },
                "author": "Takanori Fujiwara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03162v1",
                "updated": "2025-09-03T09:22:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    22,
                    39,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:22:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    22,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language\n  Understanding in Sinhala",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language\n  Understanding in Sinhala"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive general knowledge and\nreasoning abilities, yet their evaluation has predominantly focused on global\nor anglocentric subjects, often neglecting low-resource languages and\nculturally specific content. While recent multilingual benchmarks attempt to\nbridge this gap, many rely on automatic translation, which can introduce errors\nand misrepresent the original cultural context. To address this, we introduce\nSinhalaMMLU, the first multiple-choice question answering benchmark designed\nspecifically for Sinhala, a low-resource language. The dataset includes over\n7,000 questions spanning secondary to collegiate education levels, aligned with\nthe Sri Lankan national curriculum, and covers six domains and 30 subjects,\nencompassing both general academic topics and culturally grounded knowledge. We\nevaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and\nGPT-4o achieve the highest average accuracies at 67% and 62% respectively,\noverall model performance remains limited. In particular, models struggle in\nculturally rich domains such as the Humanities, revealing substantial room for\nimprovement in adapting LLMs to low-resource and culturally specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive general knowledge and\nreasoning abilities, yet their evaluation has predominantly focused on global\nor anglocentric subjects, often neglecting low-resource languages and\nculturally specific content. While recent multilingual benchmarks attempt to\nbridge this gap, many rely on automatic translation, which can introduce errors\nand misrepresent the original cultural context. To address this, we introduce\nSinhalaMMLU, the first multiple-choice question answering benchmark designed\nspecifically for Sinhala, a low-resource language. The dataset includes over\n7,000 questions spanning secondary to collegiate education levels, aligned with\nthe Sri Lankan national curriculum, and covers six domains and 30 subjects,\nencompassing both general academic topics and culturally grounded knowledge. We\nevaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and\nGPT-4o achieve the highest average accuracies at 67% and 62% respectively,\noverall model performance remains limited. In particular, models struggle in\nculturally rich domains such as the Humanities, revealing substantial room for\nimprovement in adapting LLMs to low-resource and culturally specific contexts."
                },
                "authors": [
                    {
                        "name": "Ashmari Pramodya"
                    },
                    {
                        "name": "Nirasha Nelki"
                    },
                    {
                        "name": "Heshan Shalinda"
                    },
                    {
                        "name": "Chamila Liyanage"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Randil Pushpananda"
                    },
                    {
                        "name": "Ruvan Weerasinghe"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03161v1",
                "updated": "2025-09-03T09:21:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    21,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T09:21:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    21,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "Domain Adaptation of LLMs for Process Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of LLMs for Process Data"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization."
                },
                "authors": [
                    {
                        "name": "Rafael Seidi Oyamada"
                    },
                    {
                        "name": "Jari Peeperkorn"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    },
                    {
                        "name": "Johannes De Smedt"
                    }
                ],
                "author_detail": {
                    "name": "Johannes De Smedt"
                },
                "author": "Johannes De Smedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15048v2",
                "updated": "2025-09-03T09:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    7,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-19T09:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    10,
                    49,
                    5,
                    293,
                    0
                ],
                "title": "MorphAgent: Empowering Agents through Self-Evolving Profiles and\n  Decentralized Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphAgent: Empowering Agents through Self-Evolving Profiles and\n  Decentralized Collaboration"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems (MAS) have shown promise\nin tackling complex tasks, but often rely on predefined roles and centralized\ncoordination, limiting their adaptability to evolving challenges. This paper\nintroduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive\nMulti-Agent System for decentralized agent collaboration that enables agents to\ndynamically evolve their roles and capabilities. Our approach employs\nself-evolving agent profiles, optimized through three key metrics, guiding\nagents in refining their individual expertise while maintaining complementary\nteam dynamics. MorphAgent implements a two-phase process: a Profile Update\nphase for profile optimization, followed by a Task Execution phase where agents\ncontinuously adapt their roles based on task feedback. Our experimental results\nshow that MorphAgent outperforms existing frameworks in terms of task\nperformance and adaptability to changing requirements, paving the way for more\nrobust and versatile multi-agent collaborative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems (MAS) have shown promise\nin tackling complex tasks, but often rely on predefined roles and centralized\ncoordination, limiting their adaptability to evolving challenges. This paper\nintroduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive\nMulti-Agent System for decentralized agent collaboration that enables agents to\ndynamically evolve their roles and capabilities. Our approach employs\nself-evolving agent profiles, optimized through three key metrics, guiding\nagents in refining their individual expertise while maintaining complementary\nteam dynamics. MorphAgent implements a two-phase process: a Profile Update\nphase for profile optimization, followed by a Task Execution phase where agents\ncontinuously adapt their roles based on task feedback. Our experimental results\nshow that MorphAgent outperforms existing frameworks in terms of task\nperformance and adaptability to changing requirements, paving the way for more\nrobust and versatile multi-agent collaborative systems."
                },
                "authors": [
                    {
                        "name": "Siyuan Lu"
                    },
                    {
                        "name": "Jiaqi Shao"
                    },
                    {
                        "name": "Bing Luo"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17960v3",
                "updated": "2025-09-03T09:04:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    4,
                    30,
                    2,
                    246,
                    0
                ],
                "published": "2024-11-27T00:36:27Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    36,
                    27,
                    2,
                    332,
                    0
                ],
                "title": "Calibrating DRAMPower Model for HPC: A Runtime Perspective from\n  Real-Time Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating DRAMPower Model for HPC: A Runtime Perspective from\n  Real-Time Measurements"
                },
                "summary": "Main memory's rising energy consumption has emerged as a critical challenge\nin modern computing architectures, particularly in large-scale systems, driven\nby frequent access patterns, growing data volumes, and insufficient power\nmanagement strategies. Accurate modeling of DRAM power consumption is essential\nto address this challenge and optimize energy efficiency. However, existing\nmodeling tools often rely on vendor-provided datasheet values that are obtained\nunder worst-case or idealized conditions. As a result, they fail to capture\nimportant system-level factors, such as temperature variations, chip aging, and\nworkload-induced variability, which leads to significant discrepancies between\nestimated and actual power consumption observed in real deployments. In this\nwork, we propose a runtime calibration methodology for the DRAMPower model\nusing energy measurements collected from real-system experiments. By applying\ncustom memory benchmarks on an HPC cluster and leveraging fine-grained power\nmonitoring infrastructure, we refine key current parameters (IDD values) in the\nmodel. Our calibration reduces the average energy estimation error to less than\n5%, substantially improving modeling accuracy and making DRAMPower a more\nreliable tool for power-aware system design and optimization on the target\nserver platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Main memory's rising energy consumption has emerged as a critical challenge\nin modern computing architectures, particularly in large-scale systems, driven\nby frequent access patterns, growing data volumes, and insufficient power\nmanagement strategies. Accurate modeling of DRAM power consumption is essential\nto address this challenge and optimize energy efficiency. However, existing\nmodeling tools often rely on vendor-provided datasheet values that are obtained\nunder worst-case or idealized conditions. As a result, they fail to capture\nimportant system-level factors, such as temperature variations, chip aging, and\nworkload-induced variability, which leads to significant discrepancies between\nestimated and actual power consumption observed in real deployments. In this\nwork, we propose a runtime calibration methodology for the DRAMPower model\nusing energy measurements collected from real-system experiments. By applying\ncustom memory benchmarks on an HPC cluster and leveraging fine-grained power\nmonitoring infrastructure, we refine key current parameters (IDD values) in the\nmodel. Our calibration reduces the average energy estimation error to less than\n5%, substantially improving modeling accuracy and making DRAMPower a more\nreliable tool for power-aware system design and optimization on the target\nserver platform."
                },
                "authors": [
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Dina Ali Abdelhamid"
                    },
                    {
                        "name": "Thomas Ilsche"
                    },
                    {
                        "name": "Saeideh Alinezhad Chamazcoti"
                    },
                    {
                        "name": "Timon Evenblij"
                    },
                    {
                        "name": "Mohit Gupta"
                    },
                    {
                        "name": "Francky Catthoor"
                    }
                ],
                "author_detail": {
                    "name": "Francky Catthoor"
                },
                "author": "Francky Catthoor",
                "arxiv_comment": "Supplementary Materials for Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03148v1",
                "updated": "2025-09-03T08:57:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:57:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader"
                },
                "summary": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging."
                },
                "authors": [
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Ignacio Pérez Prat"
                    },
                    {
                        "name": "Not Battesta Soliva"
                    },
                    {
                        "name": "Sandra Baltermia-Guetg"
                    },
                    {
                        "name": "Andrina Beeli"
                    },
                    {
                        "name": "Simona Beeli"
                    },
                    {
                        "name": "Madlaina Capeder"
                    },
                    {
                        "name": "Laura Decurtins"
                    },
                    {
                        "name": "Gian Peder Gregori"
                    },
                    {
                        "name": "Flavia Hobi"
                    },
                    {
                        "name": "Gabriela Holderegger"
                    },
                    {
                        "name": "Arina Lazzarini"
                    },
                    {
                        "name": "Viviana Lazzarini"
                    },
                    {
                        "name": "Walter Rosselli"
                    },
                    {
                        "name": "Bettina Vital"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "Submitted to WMT25 (Open Language Data Initiative Shared Task)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19352v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19352v3",
                "updated": "2025-09-03T08:38:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    8,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-26T18:25:54Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    18,
                    25,
                    54,
                    1,
                    238,
                    0
                ],
                "title": "Memorization in Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Graph Neural Networks"
                },
                "summary": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment."
                },
                "authors": [
                    {
                        "name": "Adarsh Jamadandi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "Franziska Boenisch"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Boenisch"
                },
                "author": "Franziska Boenisch",
                "arxiv_comment": "Version3, updated affiliation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19352v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19352v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09701v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09701v5",
                "updated": "2025-09-03T08:35:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    35,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-14T04:01:25Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    4,
                    1,
                    25,
                    4,
                    166,
                    0
                ],
                "title": "Towards Explainable Vulnerability Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Vulnerability Detection with Large Language Models"
                },
                "summary": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security."
                },
                "authors": [
                    {
                        "name": "Qiheng Mao"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09701v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09701v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03131v1",
                "updated": "2025-09-03T08:33:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    33,
                    43,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:33:43Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    33,
                    43,
                    2,
                    246,
                    0
                ],
                "title": "RecBase: Generative Foundation Model Pretraining for Zero-Shot\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecBase: Generative Foundation Model Pretraining for Zero-Shot\n  Recommendation"
                },
                "summary": "Recent advances in LLM-based recommendation have shown promise, yet their\ncross-domain generalization is hindered by a fundamental mismatch between\nlanguage-centric pretraining and the recommendation task. Existing methods,\nrelying on language-level knowledge, fail to capture dynamic, item-level user\ninterests across domains. To bridge this gap, we propose RecBase, a\ndomain-agnostic foundational model pretrained with a recommendation-oriented\nobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus\nwith unified textual representations and feature mappings to enhance\ncross-domain generalization. To further align item semantics across domains, we\nintroduce a unified item tokenizer that encodes items into hierarchical concept\nidentifiers, enabling structured representation and efficient vocabulary\nsharing. The model is trained using an autoregressive objective to capture\ncomplex item-level sequential patterns. On eight real-world datasets, our\n1.5B-parameter model matches or surpasses the performance of LLM baselines up\nto 7B parameters in zero-shot and cross-domain recommendation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM-based recommendation have shown promise, yet their\ncross-domain generalization is hindered by a fundamental mismatch between\nlanguage-centric pretraining and the recommendation task. Existing methods,\nrelying on language-level knowledge, fail to capture dynamic, item-level user\ninterests across domains. To bridge this gap, we propose RecBase, a\ndomain-agnostic foundational model pretrained with a recommendation-oriented\nobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus\nwith unified textual representations and feature mappings to enhance\ncross-domain generalization. To further align item semantics across domains, we\nintroduce a unified item tokenizer that encodes items into hierarchical concept\nidentifiers, enabling structured representation and efficient vocabulary\nsharing. The model is trained using an autoregressive objective to capture\ncomplex item-level sequential patterns. On eight real-world datasets, our\n1.5B-parameter model matches or surpasses the performance of LLM baselines up\nto 7B parameters in zero-shot and cross-domain recommendation tasks."
                },
                "authors": [
                    {
                        "name": "Sashuai Zhou"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Ke Lei"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_journal_ref": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03122v1",
                "updated": "2025-09-03T08:22:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    4,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "From Evaluation to Defense: Constructing Persistent Edit-Based\n  Fingerprints for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Evaluation to Defense: Constructing Persistent Edit-Based\n  Fingerprints for Large Language Models"
                },
                "summary": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Dongsheng Shi"
                    },
                    {
                        "name": "Yongyi Cui"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Xiaoling Wang"
                    },
                    {
                        "name": "Linlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Wang"
                },
                "author": "Linlin Wang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23294v2",
                "updated": "2025-09-03T08:22:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    1,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-29T15:31:11Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    31,
                    11,
                    6,
                    180,
                    0
                ],
                "title": "Threshold Signatures for Central Bank Digital Currencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold Signatures for Central Bank Digital Currencies"
                },
                "summary": "Digital signatures are crucial for securing Central Bank Digital Currencies\n(CBDCs) transactions. Like most forms of digital currencies, CBDC solutions\nrely on signatures for transaction authenticity and integrity, leading to major\nissues in the case of private key compromise. Our work explores threshold\nsignature schemes (TSSs) in the context of CBDCs. TSSs allow distributed key\nmanagement and signing, reducing the risk of a compromised key. We analyze\nCBDC-specific requirements, considering the applicability of TSSs, and use\nFilia CBDC solution as a base for a detailed evaluation. As most of the current\nsolutions rely on ECDSA for compatibility, we focus on ECDSA-based TSSs and\ntheir supporting libraries. Our performance evaluation measured the\ncomputational and communication complexity across key processes, as well as the\nthroughput and latency of end-to-end transactions. The results confirm that TSS\ncan enhance the security of CBDC implementations while maintaining acceptable\nperformance for real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital signatures are crucial for securing Central Bank Digital Currencies\n(CBDCs) transactions. Like most forms of digital currencies, CBDC solutions\nrely on signatures for transaction authenticity and integrity, leading to major\nissues in the case of private key compromise. Our work explores threshold\nsignature schemes (TSSs) in the context of CBDCs. TSSs allow distributed key\nmanagement and signing, reducing the risk of a compromised key. We analyze\nCBDC-specific requirements, considering the applicability of TSSs, and use\nFilia CBDC solution as a base for a detailed evaluation. As most of the current\nsolutions rely on ECDSA for compatibility, we focus on ECDSA-based TSSs and\ntheir supporting libraries. Our performance evaluation measured the\ncomputational and communication complexity across key processes, as well as the\nthroughput and latency of end-to-end transactions. The results confirm that TSS\ncan enhance the security of CBDC implementations while maintaining acceptable\nperformance for real-world deployments."
                },
                "authors": [
                    {
                        "name": "Mostafa Abdelrahman"
                    },
                    {
                        "name": "Filip Rezabek"
                    },
                    {
                        "name": "Lars Hupel"
                    },
                    {
                        "name": "Kilian Glas"
                    },
                    {
                        "name": "Georg Carle"
                    }
                ],
                "author_detail": {
                    "name": "Georg Carle"
                },
                "author": "Georg Carle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03117v1",
                "updated": "2025-09-03T08:19:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:19:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "PromptCOS: Towards System Prompt Copyright Auditing for LLMs via\n  Content-level Output Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptCOS: Towards System Prompt Copyright Auditing for LLMs via\n  Content-level Output Similarity"
                },
                "summary": "The rapid progress of large language models (LLMs) has greatly enhanced\nreasoning tasks and facilitated the development of LLM-based applications. A\ncritical factor in improving LLM-based applications is the design of effective\nsystem prompts, which significantly impact the behavior and output quality of\nLLMs. However, system prompts are susceptible to theft and misuse, which could\nundermine the interests of prompt owners. Existing methods protect prompt\ncopyrights through watermark injection and verification but face challenges due\nto their reliance on intermediate LLM outputs (e.g., logits), which limits\ntheir practical feasibility.\n  In this paper, we propose PromptCOS, a method for auditing prompt copyright\nbased on content-level output similarity. It embeds watermarks by optimizing\nthe prompt while simultaneously co-optimizing a special verification query and\ncontent-level signal marks. This is achieved by leveraging cyclic output\nsignals and injecting auxiliary tokens to ensure reliable auditing in\ncontent-only scenarios. Additionally, it incorporates cover tokens to protect\nthe watermark from malicious deletion. For copyright verification, PromptCOS\nidentifies unauthorized usage by comparing the similarity between the\nsuspicious output and the signal mark. Experimental results demonstrate that\nour method achieves high effectiveness (99.3% average watermark similarity),\nstrong distinctiveness (60.8% greater than the best baseline), high fidelity\n(accuracy degradation of no more than 0.58%), robustness (resilience against\nthree types of potential attacks), and computational efficiency (up to 98.1%\nreduction in computational cost). Our code is available at GitHub\nhttps://github.com/LianPing-cyber/PromptCOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has greatly enhanced\nreasoning tasks and facilitated the development of LLM-based applications. A\ncritical factor in improving LLM-based applications is the design of effective\nsystem prompts, which significantly impact the behavior and output quality of\nLLMs. However, system prompts are susceptible to theft and misuse, which could\nundermine the interests of prompt owners. Existing methods protect prompt\ncopyrights through watermark injection and verification but face challenges due\nto their reliance on intermediate LLM outputs (e.g., logits), which limits\ntheir practical feasibility.\n  In this paper, we propose PromptCOS, a method for auditing prompt copyright\nbased on content-level output similarity. It embeds watermarks by optimizing\nthe prompt while simultaneously co-optimizing a special verification query and\ncontent-level signal marks. This is achieved by leveraging cyclic output\nsignals and injecting auxiliary tokens to ensure reliable auditing in\ncontent-only scenarios. Additionally, it incorporates cover tokens to protect\nthe watermark from malicious deletion. For copyright verification, PromptCOS\nidentifies unauthorized usage by comparing the similarity between the\nsuspicious output and the signal mark. Experimental results demonstrate that\nour method achieves high effectiveness (99.3% average watermark similarity),\nstrong distinctiveness (60.8% greater than the best baseline), high fidelity\n(accuracy degradation of no more than 0.58%), robustness (resilience against\nthree types of potential attacks), and computational efficiency (up to 98.1%\nreduction in computational cost). Our code is available at GitHub\nhttps://github.com/LianPing-cyber/PromptCOS."
                },
                "authors": [
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Enhao Huang"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Bingrun Yang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03116v1",
                "updated": "2025-09-03T08:19:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    13,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:19:13Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    13,
                    2,
                    246,
                    0
                ],
                "title": "Measuring Scalar Constructs in Social Science with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Scalar Constructs in Social Science with LLMs"
                },
                "summary": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study yields\nactionable findings for applied researchers. First, LLMs prompted to generate\npointwise scores directly from texts produce discontinuous distributions with\nbunching at arbitrary numbers. The quality of the measurements improves with\npairwise comparisons made by LLMs, but it improves even more by taking\npointwise scores and weighting them by token probability. Finally, finetuning\nsmaller models with as few as 1,000 training pairs can match or exceed the\nperformance of prompted LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study yields\nactionable findings for applied researchers. First, LLMs prompted to generate\npointwise scores directly from texts produce discontinuous distributions with\nbunching at arbitrary numbers. The quality of the measurements improves with\npairwise comparisons made by LLMs, but it improves even more by taking\npointwise scores and weighting them by token probability. Finally, finetuning\nsmaller models with as few as 1,000 training pairs can match or exceed the\nperformance of prompted LLMs."
                },
                "authors": [
                    {
                        "name": "Hauke Licht"
                    },
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Patrick Y. Wu"
                    },
                    {
                        "name": "Pranav Goel"
                    },
                    {
                        "name": "Niklas Stoehr"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Alexander Miserlis Hoyle"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Miserlis Hoyle"
                },
                "author": "Alexander Miserlis Hoyle",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17642v2",
                "updated": "2025-09-03T08:09:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    9,
                    31,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-25T15:31:01Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    15,
                    31,
                    1,
                    1,
                    177,
                    0
                ],
                "title": "Banishing LLM Hallucinations Requires Rethinking Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Banishing LLM Hallucinations Requires Rethinking Generalization"
                },
                "summary": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically."
                },
                "authors": [
                    {
                        "name": "Johnny Li"
                    },
                    {
                        "name": "Saksham Consul"
                    },
                    {
                        "name": "Eda Zhou"
                    },
                    {
                        "name": "James Wong"
                    },
                    {
                        "name": "Naila Farooqui"
                    },
                    {
                        "name": "Yuxin Ye"
                    },
                    {
                        "name": "Nithyashree Manohar"
                    },
                    {
                        "name": "Zhuxiaona Wei"
                    },
                    {
                        "name": "Tian Wu"
                    },
                    {
                        "name": "Ben Echols"
                    },
                    {
                        "name": "Sharon Zhou"
                    },
                    {
                        "name": "Gregory Diamos"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Diamos"
                },
                "author": "Gregory Diamos",
                "arxiv_comment": "I want to revisit some of the experiments in this paper, specifically\n  figure 5",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03104v1",
                "updated": "2025-09-03T08:02:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    2,
                    3,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:02:03Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    2,
                    3,
                    2,
                    246,
                    0
                ],
                "title": "The High Cost of Keeping Warm: Characterizing Overhead in Serverless\n  Autoscaling Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The High Cost of Keeping Warm: Characterizing Overhead in Serverless\n  Autoscaling Policies"
                },
                "summary": "Serverless computing is transforming cloud application development, but the\nperformance-cost trade-offs of control plane designs remain poorly understood\ndue to a lack of open, cross-platform benchmarks and detailed system analyses.\nIn this work, we address these gaps by designing a serverless system that\napproximates the scaling behaviors of commercial providers, including AWS\nLambda and Google Cloud Run. We systematically compare the performance and\ncost-efficiency of both synchronous and asynchronous autoscaling policies by\nreplaying real-world workloads and varying key autoscaling parameters.\n  We demonstrate that our open-source systems can closely replicate the\noperational characteristics of commercial platforms, enabling reproducible and\ntransparent experimentation. By evaluating how autoscaling parameters affect\nlatency, memory usage, and CPU overhead, we reveal several key findings. First,\nwe find that serverless systems exhibit significant computational overhead due\nto instance churn equivalent to 10-40% of the CPU cycles spent on request\nhandling, primarily originating from worker nodes. Second, we observe high\nmemory allocation due to scaling policy: 2-10 times more than actively used.\nFinally, we demonstrate that reducing these overheads typically results in\nsignificant performance degradation in the current systems, underscoring the\nneed for new, cost-efficient autoscaling strategies. Additionally, we employ a\nhybrid methodology that combines real control plane deployments with\nlarge-scale simulation to extend our evaluation closer to a production scale,\nthereby bridging the gap between small research clusters and real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing is transforming cloud application development, but the\nperformance-cost trade-offs of control plane designs remain poorly understood\ndue to a lack of open, cross-platform benchmarks and detailed system analyses.\nIn this work, we address these gaps by designing a serverless system that\napproximates the scaling behaviors of commercial providers, including AWS\nLambda and Google Cloud Run. We systematically compare the performance and\ncost-efficiency of both synchronous and asynchronous autoscaling policies by\nreplaying real-world workloads and varying key autoscaling parameters.\n  We demonstrate that our open-source systems can closely replicate the\noperational characteristics of commercial platforms, enabling reproducible and\ntransparent experimentation. By evaluating how autoscaling parameters affect\nlatency, memory usage, and CPU overhead, we reveal several key findings. First,\nwe find that serverless systems exhibit significant computational overhead due\nto instance churn equivalent to 10-40% of the CPU cycles spent on request\nhandling, primarily originating from worker nodes. Second, we observe high\nmemory allocation due to scaling policy: 2-10 times more than actively used.\nFinally, we demonstrate that reducing these overheads typically results in\nsignificant performance degradation in the current systems, underscoring the\nneed for new, cost-efficient autoscaling strategies. Additionally, we employ a\nhybrid methodology that combines real control plane deployments with\nlarge-scale simulation to extend our evaluation closer to a production scale,\nthereby bridging the gap between small research clusters and real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Leonid Kondrashov"
                    },
                    {
                        "name": "Boxi Zhou"
                    },
                    {
                        "name": "Hancheng Wang"
                    },
                    {
                        "name": "Dmitrii Ustiugov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Ustiugov"
                },
                "author": "Dmitrii Ustiugov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03103v1",
                "updated": "2025-09-03T07:59:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    7,
                    59,
                    31,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T07:59:31Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    7,
                    59,
                    31,
                    2,
                    246,
                    0
                ],
                "title": "FastCaps: A Design Methodology for Accelerating Capsule Network on Field\n  Programmable Gate Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCaps: A Design Methodology for Accelerating Capsule Network on Field\n  Programmable Gate Arrays"
                },
                "summary": "Capsule Network (CapsNet) has shown significant improvement in understanding\nthe variation in images along with better generalization ability compared to\ntraditional Convolutional Neural Network (CNN). CapsNet preserves spatial\nrelationship among extracted features and apply dynamic routing to efficiently\nlearn the internal connections between capsules. However, due to the capsule\nstructure and the complexity of the routing mechanism, it is non-trivial to\naccelerate CapsNet performance in its original form on Field Programmable Gate\nArray (FPGA). Most of the existing works on CapsNet have achieved limited\nacceleration as they implement only the dynamic routing algorithm on FPGA,\nwhile considering all the processing steps synergistically is important for\nreal-world applications of Capsule Networks. Towards this, we propose a novel\ntwo-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune\nthe network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that\nuses the sum of look-ahead scores of the model parameters. Next, we simplify\nthe nonlinear operations, reorder loops, and parallelize operations of the\nrouting algorithm to reduce CapsNet hardware complexity. To the best of our\nknowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.\nExperimental results on the MNIST and F-MNIST datasets (typical in Capsule\nNetwork community) show that the proposed LAKP approach achieves an effective\ncompression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and\n48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware\ncomplexity of the routing algorithm increases the throughput to 1351 FPS and\n934 FPS respectively. As corroborated by our results, this work enables highly\nperformance-efficient deployment of CapsNets on low-cost FPGA that are popular\nin modern edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capsule Network (CapsNet) has shown significant improvement in understanding\nthe variation in images along with better generalization ability compared to\ntraditional Convolutional Neural Network (CNN). CapsNet preserves spatial\nrelationship among extracted features and apply dynamic routing to efficiently\nlearn the internal connections between capsules. However, due to the capsule\nstructure and the complexity of the routing mechanism, it is non-trivial to\naccelerate CapsNet performance in its original form on Field Programmable Gate\nArray (FPGA). Most of the existing works on CapsNet have achieved limited\nacceleration as they implement only the dynamic routing algorithm on FPGA,\nwhile considering all the processing steps synergistically is important for\nreal-world applications of Capsule Networks. Towards this, we propose a novel\ntwo-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune\nthe network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that\nuses the sum of look-ahead scores of the model parameters. Next, we simplify\nthe nonlinear operations, reorder loops, and parallelize operations of the\nrouting algorithm to reduce CapsNet hardware complexity. To the best of our\nknowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.\nExperimental results on the MNIST and F-MNIST datasets (typical in Capsule\nNetwork community) show that the proposed LAKP approach achieves an effective\ncompression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and\n48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware\ncomplexity of the routing algorithm increases the throughput to 1351 FPS and\n934 FPS respectively. As corroborated by our results, this work enables highly\nperformance-efficient deployment of CapsNets on low-cost FPGA that are popular\nin modern edge devices."
                },
                "authors": [
                    {
                        "name": "Abdul Rahoof"
                    },
                    {
                        "name": "Vivek Chaturvedi"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_doi": "10.1109/IJCNN54540.2023.10191653",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN54540.2023.10191653",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.03103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2023 International Joint Conference on Neural Networks (IJCNN)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03093v1",
                "updated": "2025-09-03T07:48:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    7,
                    48,
                    38,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T07:48:38Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    7,
                    48,
                    38,
                    2,
                    246,
                    0
                ],
                "title": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design\n  Principle Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design\n  Principle Violations"
                },
                "summary": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis."
                },
                "authors": [
                    {
                        "name": "Fatih Pehlivan"
                    },
                    {
                        "name": "Arçin Ülkü Ergüzen"
                    },
                    {
                        "name": "Sahand Moslemi Yengejeh"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ASE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20757v2",
                "updated": "2025-09-03T07:21:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    7,
                    21,
                    2,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-28T13:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    14,
                    20,
                    3,
                    240,
                    0
                ],
                "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation"
                },
                "summary": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD."
                },
                "authors": [
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Esteban Garces Arias"
                    },
                    {
                        "name": "Meimingwei Li"
                    },
                    {
                        "name": "Julian Rodemann"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Danlu Chen"
                    },
                    {
                        "name": "Gaojuan Fan"
                    },
                    {
                        "name": "Christian Heumann"
                    },
                    {
                        "name": "Chongsheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chongsheng Zhang"
                },
                "author": "Chongsheng Zhang",
                "arxiv_comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03066v1",
                "updated": "2025-09-03T06:52:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    52,
                    50,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T06:52:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    52,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled\n  Multi-branch Mamba for ECG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled\n  Multi-branch Mamba for ECG"
                },
                "summary": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis."
                },
                "authors": [
                    {
                        "name": "Huaicheng Zhang"
                    },
                    {
                        "name": "Ruoxin Wang"
                    },
                    {
                        "name": "Chenlian Zhou"
                    },
                    {
                        "name": "Jiguang Shi"
                    },
                    {
                        "name": "Yue Ge"
                    },
                    {
                        "name": "Zhoutong Li"
                    },
                    {
                        "name": "Sheng Chang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Jin He"
                    },
                    {
                        "name": "Qijun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qijun Huang"
                },
                "author": "Qijun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03059v1",
                "updated": "2025-09-03T06:42:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    42,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T06:42:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    42,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong."
                },
                "authors": [
                    {
                        "name": "Xingyue Huang"
                    },
                    {
                        "name": "Rishabh"
                    },
                    {
                        "name": "Gregor Franke"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Jiamu Bai"
                    },
                    {
                        "name": "Weijie Bai"
                    },
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Chengyu Fan"
                    },
                    {
                        "name": "Wendong Fan"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Zhuangzhuang He"
                    },
                    {
                        "name": "Xianglong Hu"
                    },
                    {
                        "name": "Neil Johnson"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Fangru Lin"
                    },
                    {
                        "name": "Siyu Lin"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Beibei Wang"
                    },
                    {
                        "name": "Fangyijie Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Zikai Xiao"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Junxiao Yang"
                    },
                    {
                        "name": "Qianshuo Ye"
                    },
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Yuwen Ebony Zhang"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Guohao Li"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Li"
                },
                "author": "Guohao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03058v1",
                "updated": "2025-09-03T06:40:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    40,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T06:40:57Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    40,
                    57,
                    2,
                    246,
                    0
                ],
                "title": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust\n  Probabilistic Fingerprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust\n  Probabilistic Fingerprint"
                },
                "summary": "The proliferation of large language models (LLMs) has intensified concerns\nover model theft and license violations, necessitating robust and stealthy\nownership verification. Existing fingerprinting methods either require\nimpractical white-box access or introduce detectable statistical anomalies. We\npropose EverTracer, a novel gray-box fingerprinting framework that ensures\nstealthy and robust model provenance tracing. EverTracer is the first to\nrepurpose Membership Inference Attacks (MIAs) for defensive use, embedding\nownership signals via memorization instead of artificial trigger-output\noverfitting. It consists of Fingerprint Injection, which fine-tunes the model\non any natural language data without detectable artifacts, and Verification,\nwhich leverages calibrated probability variation signal to distinguish\nfingerprinted models. This approach remains robust against adaptive\nadversaries, including input level modification, and model-level modifications.\nExtensive experiments across architectures demonstrate EverTracer's\nstate-of-the-art effectiveness, stealthness, and resilience, establishing it as\na practical solution for securing LLM intellectual property. Our code and data\nare publicly available at https://github.com/Xuzhenhua55/EverTracer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has intensified concerns\nover model theft and license violations, necessitating robust and stealthy\nownership verification. Existing fingerprinting methods either require\nimpractical white-box access or introduce detectable statistical anomalies. We\npropose EverTracer, a novel gray-box fingerprinting framework that ensures\nstealthy and robust model provenance tracing. EverTracer is the first to\nrepurpose Membership Inference Attacks (MIAs) for defensive use, embedding\nownership signals via memorization instead of artificial trigger-output\noverfitting. It consists of Fingerprint Injection, which fine-tunes the model\non any natural language data without detectable artifacts, and Verification,\nwhich leverages calibrated probability variation signal to distinguish\nfingerprinted models. This approach remains robust against adaptive\nadversaries, including input level modification, and model-level modifications.\nExtensive experiments across architectures demonstrate EverTracer's\nstate-of-the-art effectiveness, stealthness, and resilience, establishing it as\na practical solution for securing LLM intellectual property. Our code and data\nare publicly available at https://github.com/Xuzhenhua55/EverTracer."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Wenpeng Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Xing"
                },
                "author": "Wenpeng Xing",
                "arxiv_comment": "Accepted by EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03054v1",
                "updated": "2025-09-03T06:36:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    36,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T06:36:21Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    36,
                    21,
                    2,
                    246,
                    0
                ],
                "title": "Binary Quantization For LLMs Through Dynamic Grouping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Quantization For LLMs Through Dynamic Grouping"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of Natural Language Processing (NLP) tasks, but require\nsubstantial memory and computational resources. Binary quantization, which\ncompresses model weights from 16-bit Brain Float to 1-bit representations in\n{-1, 1}, offers significant reductions in storage and inference costs. However,\nsuch aggressive quantization often leads to notable performance degradation\ncompared to more conservative 4-bit quantization methods. In this research, we\npropose a novel optimization objective tailored for binary quantization, along\nwith three algorithms designed to realize it effectively. Our method enhances\nblocked quantization by dynamically identifying optimal unstructured\nsub-matrices through adaptive grouping strategies. Experimental results\ndemonstrate that our approach achieves an average bit length of just 1.007\nbits, while maintaining high model quality. Specifically, our quantized LLaMA\n3.2 3B model attains a perplexity of 8.23, remarkably close to the original\n7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.\nFurthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ\nin both performance and efficiency. The compression process is highly\nefficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights\non a single CPU core, with the entire process completing in under 100 minutes\nand exhibiting embarrassingly parallel properties.\n  Code - https://github.com/johnnyzheng0636/WGM_bi_quan",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of Natural Language Processing (NLP) tasks, but require\nsubstantial memory and computational resources. Binary quantization, which\ncompresses model weights from 16-bit Brain Float to 1-bit representations in\n{-1, 1}, offers significant reductions in storage and inference costs. However,\nsuch aggressive quantization often leads to notable performance degradation\ncompared to more conservative 4-bit quantization methods. In this research, we\npropose a novel optimization objective tailored for binary quantization, along\nwith three algorithms designed to realize it effectively. Our method enhances\nblocked quantization by dynamically identifying optimal unstructured\nsub-matrices through adaptive grouping strategies. Experimental results\ndemonstrate that our approach achieves an average bit length of just 1.007\nbits, while maintaining high model quality. Specifically, our quantized LLaMA\n3.2 3B model attains a perplexity of 8.23, remarkably close to the original\n7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.\nFurthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ\nin both performance and efficiency. The compression process is highly\nefficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights\non a single CPU core, with the entire process completing in under 100 minutes\nand exhibiting embarrassingly parallel properties.\n  Code - https://github.com/johnnyzheng0636/WGM_bi_quan"
                },
                "authors": [
                    {
                        "name": "Xinzhe Zheng"
                    },
                    {
                        "name": "Zhen-Qun Yang"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "S. Joe Qin"
                    },
                    {
                        "name": "Arlene Chen"
                    },
                    {
                        "name": "Fangzhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhen Lin"
                },
                "author": "Fangzhen Lin",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03034v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03034v4",
                "updated": "2025-09-03T06:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    34,
                    12,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-03T02:45:51Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    2,
                    45,
                    51,
                    3,
                    184,
                    0
                ],
                "title": "Rethinking Data Protection in the (Generative) Artificial Intelligence\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Protection in the (Generative) Artificial Intelligence\n  Era"
                },
                "summary": "The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike."
                },
                "authors": [
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Junfeng Guo"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "arxiv_comment": "Perspective paper for a broader scientific audience. The first two\n  authors contributed equally to this paper. 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03034v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03034v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03047v1",
                "updated": "2025-09-03T06:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    19,
                    59,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T06:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    19,
                    59,
                    2,
                    246,
                    0
                ],
                "title": "FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale\n  Training of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale\n  Training of LLMs"
                },
                "summary": "Large language models (LLMs) have made a profound impact across various\nfields due to their advanced capabilities. However, training these models at\nunprecedented scales requires extensive AI accelerator clusters and\nsophisticated parallelism strategies, which pose significant challenges in\nmaintaining system reliability over prolonged training periods. A major concern\nis the substantial loss of training time caused by inevitable hardware and\nsoftware failures. To address these challenges, we present FlashRecovery, a\nfast and low-cost failure recovery system comprising three core modules: (1)\nActive and real-time failure detection. This module performs continuous\ntraining state monitoring, enabling immediate identification of hardware and\nsoftware failures within seconds, thus ensuring rapid incident response; (2)\nScale-independent task restart. By employing different recovery strategies for\nnormal and faulty nodes, combined with an optimized communication group\nreconstruction protocol, our approach ensures that the recovery time remains\nnearly constant, regardless of cluster scale; (3) Checkpoint-free recovery\nwithin one step. Our novel recovery mechanism enables single-step restoration,\ncompletely eliminating dependence on traditional checkpointing methods and\ntheir associated overhead. Collectively, these innovations enable FlashRecovery\nto achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective\n(RPO), substantially improving the reliability and efficiency of long-duration\nLLM training. Experimental results demonstrate that FlashRecovery system can\nachieve training restoration on training cluster with 4, 800 devices in 150\nseconds. We also verify that the time required for failure recovery is nearly\nconsistent for different scales of training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made a profound impact across various\nfields due to their advanced capabilities. However, training these models at\nunprecedented scales requires extensive AI accelerator clusters and\nsophisticated parallelism strategies, which pose significant challenges in\nmaintaining system reliability over prolonged training periods. A major concern\nis the substantial loss of training time caused by inevitable hardware and\nsoftware failures. To address these challenges, we present FlashRecovery, a\nfast and low-cost failure recovery system comprising three core modules: (1)\nActive and real-time failure detection. This module performs continuous\ntraining state monitoring, enabling immediate identification of hardware and\nsoftware failures within seconds, thus ensuring rapid incident response; (2)\nScale-independent task restart. By employing different recovery strategies for\nnormal and faulty nodes, combined with an optimized communication group\nreconstruction protocol, our approach ensures that the recovery time remains\nnearly constant, regardless of cluster scale; (3) Checkpoint-free recovery\nwithin one step. Our novel recovery mechanism enables single-step restoration,\ncompletely eliminating dependence on traditional checkpointing methods and\ntheir associated overhead. Collectively, these innovations enable FlashRecovery\nto achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective\n(RPO), substantially improving the reliability and efficiency of long-duration\nLLM training. Experimental results demonstrate that FlashRecovery system can\nachieve training restoration on training cluster with 4, 800 devices in 150\nseconds. We also verify that the time required for failure recovery is nearly\nconsistent for different scales of training tasks."
                },
                "authors": [
                    {
                        "name": "Haijun Zhang"
                    },
                    {
                        "name": "Jinxiang Wang"
                    },
                    {
                        "name": "Zhenhua Yu"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Xuejie Ji"
                    },
                    {
                        "name": "Kaining Mao"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yaqing Zhang"
                    },
                    {
                        "name": "Ting Wu"
                    },
                    {
                        "name": "Fei Jie"
                    },
                    {
                        "name": "Xiemin Huang"
                    },
                    {
                        "name": "Zhifang Cai"
                    },
                    {
                        "name": "Junhua Cheng"
                    },
                    {
                        "name": "Shuwei Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xiaoming Bao"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Shixiong Zhao"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Hongwei Sun"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Chunsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunsheng Li"
                },
                "author": "Chunsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01245v2",
                "updated": "2025-09-03T06:09:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    9,
                    5,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-01T08:38:49Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    38,
                    49,
                    0,
                    244,
                    0
                ],
                "title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"
                },
                "summary": "Operating system schedulers suffer from a fundamental semantic gap, where\nkernel policies fail to understand application-specific needs, leading to\nsuboptimal performance. We introduce SchedCP, the first framework that enables\nfully autonomous Large Language Model (LLM) agents to safely and efficiently\noptimize Linux schedulers without human involvement. Our core insight is that\nthe challenge is not merely to apply a better LLM, but to architect a decoupled\ncontrol plane that separates the AI's role of semantic reasoning (\"what to\noptimize\") from the system's role of execution (\"how to observe and act\").\nImplemented as Model Context Protocol(MCP) server, SchedCP provides a stable\ninterface with three key services: a Workload Analysis Engine, an evolving\nScheduler Policy Repository, and an Execution Verifier that validates all\nAI-generated code and configure before deployment with static and dynamic\nanalysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent\nsystem that autonomously analyzes workloads, synthesizes custom eBPF scheduling\npolicies, and deploys them via the sched\\_ext infrastructure. Our evaluation\nshows that SchedCP achieves up to an 1.79x performance improvement, and a 13x\ncost reduction compared to naive agentic approaches, all while maintaining high\nsuccess rate. By bridging the semantic gap, SchedCP democratizes expert-level\nsystem optimization and represents a step towards creating truly\nself-optimizing, application-aware operating systems. The code is open-sourced\nin https://github.com/eunomia-bpf/schedcp",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating system schedulers suffer from a fundamental semantic gap, where\nkernel policies fail to understand application-specific needs, leading to\nsuboptimal performance. We introduce SchedCP, the first framework that enables\nfully autonomous Large Language Model (LLM) agents to safely and efficiently\noptimize Linux schedulers without human involvement. Our core insight is that\nthe challenge is not merely to apply a better LLM, but to architect a decoupled\ncontrol plane that separates the AI's role of semantic reasoning (\"what to\noptimize\") from the system's role of execution (\"how to observe and act\").\nImplemented as Model Context Protocol(MCP) server, SchedCP provides a stable\ninterface with three key services: a Workload Analysis Engine, an evolving\nScheduler Policy Repository, and an Execution Verifier that validates all\nAI-generated code and configure before deployment with static and dynamic\nanalysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent\nsystem that autonomously analyzes workloads, synthesizes custom eBPF scheduling\npolicies, and deploys them via the sched\\_ext infrastructure. Our evaluation\nshows that SchedCP achieves up to an 1.79x performance improvement, and a 13x\ncost reduction compared to naive agentic approaches, all while maintaining high\nsuccess rate. By bridging the semantic gap, SchedCP democratizes expert-level\nsystem optimization and represents a step towards creating truly\nself-optimizing, application-aware operating systems. The code is open-sourced\nin https://github.com/eunomia-bpf/schedcp"
                },
                "authors": [
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Yanpeng Hu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00591v2",
                "updated": "2025-09-03T06:00:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    0,
                    33,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-30T19:03:14Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    19,
                    3,
                    14,
                    5,
                    242,
                    0
                ],
                "title": "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and\n  Quantifying Evaluation Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and\n  Quantifying Evaluation Awareness"
                },
                "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."
                },
                "authors": [
                    {
                        "name": "Lang Xiong"
                    },
                    {
                        "name": "Nishant Bhargava"
                    },
                    {
                        "name": "Wesley Chang"
                    },
                    {
                        "name": "Jianhang Hong"
                    },
                    {
                        "name": "Haihao Liu"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03038v1",
                "updated": "2025-09-03T05:55:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    55,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T05:55:22Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    55,
                    22,
                    2,
                    246,
                    0
                ],
                "title": "Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS\n  Blockage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS\n  Blockage"
                },
                "summary": "This paper considers a power-splitting (PS)-based simultaneous wireless\ninformation and power transfer (SWIPT) system employing a reconfigurable\npinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We\nformulate a joint optimization of the PA position and the PS ratio to maximize\nthe average signal-to-noise ratio (SNR) at a user, subject to its average\nenergy harvesting (EH) and PA placement limits. We derive a closed-form optimal\nsolution. Results demonstrate that the EH requirement has a deterministic\nimpact on the optimal PA position as well as its feasible region, requiring\ndeployment of the PA as close to the user as possible to maximize average\nchannel gain. This spatial adaptation, combined with dynamic PS, enables robust\nSWIPT performance in the presence of probabilistic LoS blockage, revealing that\nmechanical reconfigurability primarily enhances sustainability by ensuring\nenergy feasibility in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers a power-splitting (PS)-based simultaneous wireless\ninformation and power transfer (SWIPT) system employing a reconfigurable\npinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We\nformulate a joint optimization of the PA position and the PS ratio to maximize\nthe average signal-to-noise ratio (SNR) at a user, subject to its average\nenergy harvesting (EH) and PA placement limits. We derive a closed-form optimal\nsolution. Results demonstrate that the EH requirement has a deterministic\nimpact on the optimal PA position as well as its feasible region, requiring\ndeployment of the PA as close to the user as possible to maximize average\nchannel gain. This spatial adaptation, combined with dynamic PS, enables robust\nSWIPT performance in the presence of probabilistic LoS blockage, revealing that\nmechanical reconfigurability primarily enhances sustainability by ensuring\nenergy feasibility in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Ruihong Jiang"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Yanqing Xu"
                    },
                    {
                        "name": "Huimin Hu"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03037v1",
                "updated": "2025-09-03T05:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    53,
                    56,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T05:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    53,
                    56,
                    2,
                    246,
                    0
                ],
                "title": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in\n  Ethereum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in\n  Ethereum"
                },
                "summary": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Shuzheng Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Yuming Huang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03036v1",
                "updated": "2025-09-03T05:53:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    53,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T05:53:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    53,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Knowledge Integration for Physics-informed Symbolic Regression Using\n  Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Integration for Physics-informed Symbolic Regression Using\n  Pre-trained Large Language Models"
                },
                "summary": "Symbolic regression (SR) has emerged as a powerful tool for automated\nscientific discovery, enabling the derivation of governing equations from\nexperimental data. A growing body of work illustrates the promise of\nintegrating domain knowledge into the SR to improve the discovered equation's\ngenerality and usefulness. Physics-informed SR (PiSR) addresses this by\nincorporating domain knowledge, but current methods often require specialized\nformulations and manual feature engineering, limiting their adaptability only\nto domain experts. In this study, we leverage pre-trained Large Language Models\n(LLMs) to facilitate knowledge integration in PiSR. By harnessing the\ncontextual understanding of LLMs trained on vast scientific literature, we aim\nto automate the incorporation of domain knowledge, reducing the need for manual\nintervention and making the process more accessible to a broader range of\nscientific problems. Namely, the LLM is integrated into the SR's loss function,\nadding a term of the LLM's evaluation of the SR's produced equation. We\nextensively evaluate our method using three SR algorithms (DEAP, gplearn, and\nPySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three\nphysical dynamics (dropping ball, simple harmonic motion, and electromagnetic\nwave). The results demonstrate that LLM integration consistently improves the\nreconstruction of physical dynamics from data, enhancing the robustness of SR\nmodels to noise and complexity. We further explore the impact of prompt\nengineering, finding that more informative prompts significantly improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression (SR) has emerged as a powerful tool for automated\nscientific discovery, enabling the derivation of governing equations from\nexperimental data. A growing body of work illustrates the promise of\nintegrating domain knowledge into the SR to improve the discovered equation's\ngenerality and usefulness. Physics-informed SR (PiSR) addresses this by\nincorporating domain knowledge, but current methods often require specialized\nformulations and manual feature engineering, limiting their adaptability only\nto domain experts. In this study, we leverage pre-trained Large Language Models\n(LLMs) to facilitate knowledge integration in PiSR. By harnessing the\ncontextual understanding of LLMs trained on vast scientific literature, we aim\nto automate the incorporation of domain knowledge, reducing the need for manual\nintervention and making the process more accessible to a broader range of\nscientific problems. Namely, the LLM is integrated into the SR's loss function,\nadding a term of the LLM's evaluation of the SR's produced equation. We\nextensively evaluate our method using three SR algorithms (DEAP, gplearn, and\nPySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three\nphysical dynamics (dropping ball, simple harmonic motion, and electromagnetic\nwave). The results demonstrate that LLM integration consistently improves the\nreconstruction of physical dynamics from data, enhancing the robustness of SR\nmodels to noise and complexity. We further explore the impact of prompt\nengineering, finding that more informative prompts significantly improve\nperformance."
                },
                "authors": [
                    {
                        "name": "Bilge Taskin"
                    },
                    {
                        "name": "Wenxiong Xie"
                    },
                    {
                        "name": "Teddy Lazebnik"
                    }
                ],
                "author_detail": {
                    "name": "Teddy Lazebnik"
                },
                "author": "Teddy Lazebnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18942v2",
                "updated": "2025-09-03T05:49:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    49,
                    24,
                    2,
                    246,
                    0
                ],
                "published": "2025-04-26T15:01:55Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    15,
                    1,
                    55,
                    5,
                    116,
                    0
                ],
                "title": "LawFlow: Collecting and Simulating Lawyers' Thought Processes on\n  Business Formation Case Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LawFlow: Collecting and Simulating Lawyers' Thought Processes on\n  Business Formation Case Studies"
                },
                "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Our results\nhighlight both the current limitations of LLMs in supporting complex legal\nworkflows and opportunities for developing more collaborative, reasoning-aware\nlegal AI systems.\n  All data and code are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Our results\nhighlight both the current limitations of LLMs in supporting complex legal\nworkflows and opportunities for developing more collaborative, reasoning-aware\nlegal AI systems.\n  All data and code are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."
                },
                "authors": [
                    {
                        "name": "Debarati Das"
                    },
                    {
                        "name": "Khanh Chi Le"
                    },
                    {
                        "name": "Ritik Sachin Parkar"
                    },
                    {
                        "name": "Karin De Langis"
                    },
                    {
                        "name": "Brendan Madson"
                    },
                    {
                        "name": "Chad M. Berryman"
                    },
                    {
                        "name": "Robin M. Willis"
                    },
                    {
                        "name": "Daniel H. Moses"
                    },
                    {
                        "name": "Brett McDonnell"
                    },
                    {
                        "name": "Daniel Schwarcz"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00971v2",
                "updated": "2025-09-03T05:36:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    36,
                    42,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-31T19:31:53Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    19,
                    31,
                    53,
                    6,
                    243,
                    0
                ],
                "title": "CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks\n  with LLMs"
                },
                "summary": "We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel\nreasoning method called General Symbolics. This approach diverges from\nreasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),\nand Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General\nSymbolic Reasoner (GSR) is specifically structured around three key use cases:\ntool-calling, code generation, and planning, demonstrating exemplary\nperformance across a total of seven benchmarks in their respective areas.\nNotably, we are achieving SOTA scores of 66.66% on Livecodebench v6, 89% on\nInstruction-Following Evals, and 24.4% on ARC-AGI-2. We also present an agentic\ncoding IDE, developed using the principles of General Symbolics, which achieves\na state-of-the-art accuracy of 62.3% on SWE-Bench Lite. We are able to achieve\nthese improvements without any fine-tuning or training costs. Our Reasoning\nLayer is designed to provide a pure performance uplift, ensuring that a model's\naccuracy on reasoning tasks is never negatively impacted. We argue that\nincumbent methods will eventually lead to diminishing returns in LLM\nperformance, necessitating the development of new reasoning techniques. This\ntechnical report details our approach at a high level and the availability of\nthe CoreThink models for reasoning-intensive use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel\nreasoning method called General Symbolics. This approach diverges from\nreasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),\nand Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General\nSymbolic Reasoner (GSR) is specifically structured around three key use cases:\ntool-calling, code generation, and planning, demonstrating exemplary\nperformance across a total of seven benchmarks in their respective areas.\nNotably, we are achieving SOTA scores of 66.66% on Livecodebench v6, 89% on\nInstruction-Following Evals, and 24.4% on ARC-AGI-2. We also present an agentic\ncoding IDE, developed using the principles of General Symbolics, which achieves\na state-of-the-art accuracy of 62.3% on SWE-Bench Lite. We are able to achieve\nthese improvements without any fine-tuning or training costs. Our Reasoning\nLayer is designed to provide a pure performance uplift, ensuring that a model's\naccuracy on reasoning tasks is never negatively impacted. We argue that\nincumbent methods will eventually lead to diminishing returns in LLM\nperformance, necessitating the development of new reasoning techniques. This\ntechnical report details our approach at a high level and the availability of\nthe CoreThink models for reasoning-intensive use cases."
                },
                "authors": [
                    {
                        "name": "Jay Vaghasiya"
                    },
                    {
                        "name": "Omkar Ghugarkar"
                    },
                    {
                        "name": "Vishvesh Bhat"
                    },
                    {
                        "name": "Vipul Dholaria"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19992v2",
                "updated": "2025-09-03T05:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    10,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-06-24T20:22:00Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    20,
                    22,
                    0,
                    1,
                    175,
                    0
                ],
                "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs\n  for Efficient Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs\n  for Efficient Summarization"
                },
                "summary": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets."
                },
                "authors": [
                    {
                        "name": "Gabor Petnehazi"
                    },
                    {
                        "name": "Bernadett Aradi"
                    }
                ],
                "author_detail": {
                    "name": "Bernadett Aradi"
                },
                "author": "Bernadett Aradi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19099v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19099v6",
                "updated": "2025-09-03T05:05:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    5,
                    5,
                    25,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-25T11:28:34Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    28,
                    34,
                    6,
                    145,
                    0
                ],
                "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning"
                },
                "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."
                },
                "authors": [
                    {
                        "name": "Kun Xiang"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zirong Liu"
                    },
                    {
                        "name": "Peixin Qu"
                    },
                    {
                        "name": "Jixi He"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19099v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19099v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03021v1",
                "updated": "2025-09-03T04:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T04:56:21Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "title": "A Study on Zero-Shot Non-Intrusive Speech Intelligibility for Hearing\n  Aids Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Zero-Shot Non-Intrusive Speech Intelligibility for Hearing\n  Aids Using Large Language Models"
                },
                "summary": "This work focuses on zero-shot non-intrusive speech assessment for hearing\naids (HA) using large language models (LLMs). Specifically, we introduce\nGPT-Whisper-HA, an extension of GPT-Whisper, a zero-shot non-intrusive speech\nassessment model based on LLMs. GPT-Whisper-HA is designed for speech\nassessment for HA, incorporating MSBG hearing loss and NAL-R simulations to\nprocess audio input based on each individual's audiogram, two automatic speech\nrecognition (ASR) modules for audio-to-text representation, and GPT-4o to\npredict two corresponding scores, followed by score averaging for the final\nestimated score. Experimental results indicate that GPT-Whisper-HA achieves a\n2.59% relative root mean square error (RMSE) improvement over GPT-Whisper,\nconfirming the potential of LLMs for zero-shot speech assessment in predicting\nsubjective intelligibility for HA users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on zero-shot non-intrusive speech assessment for hearing\naids (HA) using large language models (LLMs). Specifically, we introduce\nGPT-Whisper-HA, an extension of GPT-Whisper, a zero-shot non-intrusive speech\nassessment model based on LLMs. GPT-Whisper-HA is designed for speech\nassessment for HA, incorporating MSBG hearing loss and NAL-R simulations to\nprocess audio input based on each individual's audiogram, two automatic speech\nrecognition (ASR) modules for audio-to-text representation, and GPT-4o to\npredict two corresponding scores, followed by score averaging for the final\nestimated score. Experimental results indicate that GPT-Whisper-HA achieves a\n2.59% relative root mean square error (RMSE) improvement over GPT-Whisper,\nconfirming the potential of LLMs for zero-shot speech assessment in predicting\nsubjective intelligibility for HA users."
                },
                "authors": [
                    {
                        "name": "Ryandhimas E. Zezario"
                    },
                    {
                        "name": "Dyah A. M. G. Wisnu"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "arxiv_comment": "Accepted to IEEE ICCE-TW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00891v2",
                "updated": "2025-09-03T04:55:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    55,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-31T15:08:41Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    8,
                    41,
                    6,
                    243,
                    0
                ],
                "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop\n  Insulin Adoption in Type 1 Diabetes Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop\n  Insulin Adoption in Type 1 Diabetes Care"
                },
                "summary": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond."
                },
                "authors": [
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Feiyun Ouyang"
                    },
                    {
                        "name": "Junhui Qian"
                    },
                    {
                        "name": "Lingxi Li"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Equal contribution for the first two authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03020v2",
                "updated": "2025-09-04T08:02:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    2,
                    20,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-03T04:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    55,
                    26,
                    2,
                    246,
                    0
                ],
                "title": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs to be Better Text Embedders through Bidirectional\n  Reconstruction"
                },
                "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales."
                },
                "authors": [
                    {
                        "name": "Chang Su"
                    },
                    {
                        "name": "Dengliang Shi"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Jintao Du"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "accepted by EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03018v1",
                "updated": "2025-09-03T04:49:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    49,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T04:49:20Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    49,
                    20,
                    2,
                    246,
                    0
                ],
                "title": "Mycroft: Tracing Dependencies in Collective Communication Towards\n  Reliable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mycroft: Tracing Dependencies in Collective Communication Towards\n  Reliable LLM Training"
                },
                "summary": "Reliability is essential for ensuring efficiency in LLM training. However,\nmany real-world reliability issues remain difficult to resolve, resulting in\nwasted resources and degraded model performance. Unfortunately, today's\ncollective communication libraries operate as black boxes, hiding critical\ninformation needed for effective root cause analysis. We propose Mycroft, a\nlightweight distributed tracing and root cause analysis system designed to\naddress previously hidden reliability issues in collective communication.\nMycroft's key idea is to trace collective communication states and leverage\ninternal control and data dependencies to resolve reliability problems in LLM\ntraining. Mycroft has been deployed at ByteDance for over six months to debug\ncollective communication related issues at runtime. It detected anomalies\nwithin 15 seconds in 90% of cases and identified the root cause within 20\nseconds in 60% of cases. We also conducted extensive fault injection\nexperiments to demonstrate Mycroft's capability and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability is essential for ensuring efficiency in LLM training. However,\nmany real-world reliability issues remain difficult to resolve, resulting in\nwasted resources and degraded model performance. Unfortunately, today's\ncollective communication libraries operate as black boxes, hiding critical\ninformation needed for effective root cause analysis. We propose Mycroft, a\nlightweight distributed tracing and root cause analysis system designed to\naddress previously hidden reliability issues in collective communication.\nMycroft's key idea is to trace collective communication states and leverage\ninternal control and data dependencies to resolve reliability problems in LLM\ntraining. Mycroft has been deployed at ByteDance for over six months to debug\ncollective communication related issues at runtime. It detected anomalies\nwithin 15 seconds in 90% of cases and identified the root cause within 20\nseconds in 60% of cases. We also conducted extensive fault injection\nexperiments to demonstrate Mycroft's capability and efficiency."
                },
                "authors": [
                    {
                        "name": "Yangtao Deng"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Qinlong Wang"
                    },
                    {
                        "name": "Xiaoyun Zhi"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Haohan Xu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13748v3",
                "updated": "2025-09-03T04:23:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    23,
                    51,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-19T18:01:08Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    18,
                    1,
                    8,
                    2,
                    171,
                    0
                ],
                "title": "Learn and Unlearn: Addressing Misinformation in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn and Unlearn: Addressing Misinformation in Multilingual LLMs"
                },
                "summary": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Philipp Koehn"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Koehn"
                },
                "author": "Philipp Koehn",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03000v1",
                "updated": "2025-09-03T04:17:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    17,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T04:17:57Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    4,
                    17,
                    57,
                    2,
                    246,
                    0
                ],
                "title": "Closing the Visibility Gap: A Monitoring Framework for Verifiable Open\n  RAN Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Visibility Gap: A Monitoring Framework for Verifiable Open\n  RAN Operations"
                },
                "summary": "Open Radio Access Network (Open RAN) is reshaping mobile network architecture\nby promoting openness, disaggregation, and cross-vendor interoperability.\nHowever, this architectural flexibility introduces new security challenges,\nespecially in deployments where multiple mobile network operators (MNOs)\njointly operate shared components. Existing Zero Trust Architectures (ZTA) in\nO-RAN, as defined by governmental and industry standards, implicitly assume\nthat authenticated components will comply with operational policies. However,\nthis assumption creates a critical blind spot: misconfigured or compromised\ncomponents can silently violate policies, misuse resources, or corrupt\ndownstream processes (e.g., ML-based RIC xApps).\n  To address this critical gap, we propose a monitoring framework for low-trust\nO-RAN environments that proactively verifies configuration state and control\nbehavior against tenant-defined policies. Our system provides scalable,\nverifiable oversight to enhance transparency and trust in O-RAN operations. We\nimplement and evaluate the framework using standardized O-RAN configurations,\nwith total processing latency of approximately 200 ms, demonstrating its\nefficiency and practicality for timely policy enforcement and compliance\nauditing in multi-MNO deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Radio Access Network (Open RAN) is reshaping mobile network architecture\nby promoting openness, disaggregation, and cross-vendor interoperability.\nHowever, this architectural flexibility introduces new security challenges,\nespecially in deployments where multiple mobile network operators (MNOs)\njointly operate shared components. Existing Zero Trust Architectures (ZTA) in\nO-RAN, as defined by governmental and industry standards, implicitly assume\nthat authenticated components will comply with operational policies. However,\nthis assumption creates a critical blind spot: misconfigured or compromised\ncomponents can silently violate policies, misuse resources, or corrupt\ndownstream processes (e.g., ML-based RIC xApps).\n  To address this critical gap, we propose a monitoring framework for low-trust\nO-RAN environments that proactively verifies configuration state and control\nbehavior against tenant-defined policies. Our system provides scalable,\nverifiable oversight to enhance transparency and trust in O-RAN operations. We\nimplement and evaluate the framework using standardized O-RAN configurations,\nwith total processing latency of approximately 200 ms, demonstrating its\nefficiency and practicality for timely policy enforcement and compliance\nauditing in multi-MNO deployments."
                },
                "authors": [
                    {
                        "name": "Hexuan Yu"
                    },
                    {
                        "name": "Md Mohaimin Al Barat"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Y. Thomas Hou"
                    },
                    {
                        "name": "Wenjing Lou"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Lou"
                },
                "author": "Wenjing Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]