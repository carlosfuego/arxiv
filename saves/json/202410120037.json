[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v1",
                "updated": "2024-10-05T03:47:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Luk Ondrek"
                    },
                    {
                        "name": "Ondej Mika"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Mika"
                },
                "author": "Ondej Mika",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorovi"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Leki"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Leki"
                },
                "author": "Aleksandra Leki",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clment Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "Franois Treussart"
                    }
                ],
                "author_detail": {
                    "name": "Franois Treussart"
                },
                "author": "Franois Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Hllein"
                    },
                    {
                        "name": "Alja Boi"
                    },
                    {
                        "name": "Michael Zollhfer"
                    },
                    {
                        "name": "Matthias Niener"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Niener"
                },
                "author": "Matthias Niener",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.08202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08202v1",
                "updated": "2024-10-10T17:59:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:59:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%."
                },
                "authors": [
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xizhou Zhu"
                },
                "author": "Xizhou Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08197v1",
                "updated": "2024-10-10T17:58:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:58:44Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    44,
                    3,
                    284,
                    0
                ],
                "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions"
                },
                "summary": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08193v1",
                "updated": "2024-10-10T17:58:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:58:24Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining."
                },
                "authors": [
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08189v1",
                "updated": "2024-10-10T17:57:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    57,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:57:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    57,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object\n  Navigation"
                },
                "summary": "In this paper, we propose a new framework for zero-shot object navigation.\nExisting zero-shot object navigation methods prompt LLM with the text of\nspatially closed objects, which lacks enough scene context for in-depth\nreasoning. To better preserve the information of environment and fully exploit\nthe reasoning ability of LLM, we propose to represent the observed scene with\n3D scene graph. The scene graph encodes the relationships between objects,\ngroups and rooms with a LLM-friendly structure, for which we design a\nhierarchical chain-of-thought prompt to help LLM reason the goal location\naccording to scene context by traversing the nodes and edges. Moreover, benefit\nfrom the scene graph representation, we further design a re-perception\nmechanism to empower the object navigation framework with the ability to\ncorrect perception error. We conduct extensive experiments on MP3D, HM3D and\nRoboTHOR environments, where SG-Nav surpasses previous state-of-the-art\nzero-shot methods by more than 10% SR on all benchmarks, while the decision\nprocess is explainable. To the best of our knowledge, SG-Nav is the first\nzero-shot method that achieves even higher performance than supervised object\nnavigation methods on the challenging MP3D benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new framework for zero-shot object navigation.\nExisting zero-shot object navigation methods prompt LLM with the text of\nspatially closed objects, which lacks enough scene context for in-depth\nreasoning. To better preserve the information of environment and fully exploit\nthe reasoning ability of LLM, we propose to represent the observed scene with\n3D scene graph. The scene graph encodes the relationships between objects,\ngroups and rooms with a LLM-friendly structure, for which we design a\nhierarchical chain-of-thought prompt to help LLM reason the goal location\naccording to scene context by traversing the nodes and edges. Moreover, benefit\nfrom the scene graph representation, we further design a re-perception\nmechanism to empower the object navigation framework with the ability to\ncorrect perception error. We conduct extensive experiments on MP3D, HM3D and\nRoboTHOR environments, where SG-Nav surpasses previous state-of-the-art\nzero-shot methods by more than 10% SR on all benchmarks, while the decision\nprocess is explainable. To the best of our knowledge, SG-Nav is the first\nzero-shot method that achieves even higher performance than supervised object\nnavigation methods on the challenging MP3D benchmark."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to NeurIPS 2024. Project page:\n  https://bagh2178.github.io/SG-Nav/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08178v1",
                "updated": "2024-10-10T17:52:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    52,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:52:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    52,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "The mystery of water in the atmosphere of $$ Botis b continues:\n  insights from revisiting archival CRIRES observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mystery of water in the atmosphere of $$ Botis b continues:\n  insights from revisiting archival CRIRES observations"
                },
                "summary": "The chemical abundances of gas-giant exoplanet atmospheres hold clues to the\nformation and evolution pathways that sculpt the exoplanet population. Recent\nground-based high-resolution spectroscopic observations of the non-transiting\nhot Jupiter $\\tau$ Bo\\\"otis b from different instruments have resulted in a\ntension on the presence of water vapour in the planet's atmosphere, which\nimpact the planet's inferred C/O and metallicity. To investigate this, we\nrevisit the archival CRIRES observations of the planet's dayside in the\nwavelength range 2.28 to 2.33 $\\mu$m. We reanalyse them using the latest\nmethods for correcting stellar and telluric systematics, and free-chemistry\nBayesian atmospheric retrieval. We find that a spurious detection of CH$_{4}$\ncan arise from inadequate telluric correction. We confirm the detection of CO\nand constrain its abundance to be near solar $\\log_{10}(\\mathrm{CO})$ =\n-3.44$^{+1.63}_{-0.85}$ VMR. We find a marginal evidence for H$_{2}$O with\n$\\log_{10}(\\mathrm{H_{2}O})$ = -5.13$^{+1.22}_{-6.37}$ VMR. This translates to\nsuper solar C/O (0.95$^{+0.06}_{-0.31}$), marginally sub-solar metallicity\n(-0.21 $^{+1.66}_{-0.87}$). Due to the relatively large uncertainty on H$_{2}$O\nabundance, we cannot confidently resolve the tension on the presence of\nH$_{2}$O and the super-solar atmospheric metallicity of $\\tau$ Bo\\\"otis b. We\nrecommend further observations of $\\tau$ Bo\\\"otis b in the wavelength ranges\nsimultaneously covering CO and $\\mathrm{H_{2}O}$ to confirm the peculiar case\nof the planet's super-solar C/O and metallicity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The chemical abundances of gas-giant exoplanet atmospheres hold clues to the\nformation and evolution pathways that sculpt the exoplanet population. Recent\nground-based high-resolution spectroscopic observations of the non-transiting\nhot Jupiter $\\tau$ Bo\\\"otis b from different instruments have resulted in a\ntension on the presence of water vapour in the planet's atmosphere, which\nimpact the planet's inferred C/O and metallicity. To investigate this, we\nrevisit the archival CRIRES observations of the planet's dayside in the\nwavelength range 2.28 to 2.33 $\\mu$m. We reanalyse them using the latest\nmethods for correcting stellar and telluric systematics, and free-chemistry\nBayesian atmospheric retrieval. We find that a spurious detection of CH$_{4}$\ncan arise from inadequate telluric correction. We confirm the detection of CO\nand constrain its abundance to be near solar $\\log_{10}(\\mathrm{CO})$ =\n-3.44$^{+1.63}_{-0.85}$ VMR. We find a marginal evidence for H$_{2}$O with\n$\\log_{10}(\\mathrm{H_{2}O})$ = -5.13$^{+1.22}_{-6.37}$ VMR. This translates to\nsuper solar C/O (0.95$^{+0.06}_{-0.31}$), marginally sub-solar metallicity\n(-0.21 $^{+1.66}_{-0.87}$). Due to the relatively large uncertainty on H$_{2}$O\nabundance, we cannot confidently resolve the tension on the presence of\nH$_{2}$O and the super-solar atmospheric metallicity of $\\tau$ Bo\\\"otis b. We\nrecommend further observations of $\\tau$ Bo\\\"otis b in the wavelength ranges\nsimultaneously covering CO and $\\mathrm{H_{2}O}$ to confirm the peculiar case\nof the planet's super-solar C/O and metallicity."
                },
                "authors": [
                    {
                        "name": "Vatsal Panwar"
                    },
                    {
                        "name": "Matteo Brogi"
                    },
                    {
                        "name": "Siddharth Gandhi"
                    },
                    {
                        "name": "Heather Cegla"
                    },
                    {
                        "name": "Marina Lafarga"
                    }
                ],
                "author_detail": {
                    "name": "Marina Lafarga"
                },
                "author": "Marina Lafarga",
                "arxiv_comment": "16 pages, 13 figures; Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08159v1",
                "updated": "2024-10-10T17:41:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    41,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:41:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    41,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image\n  Generation"
                },
                "summary": "Diffusion models have become the dominant approach for visual generation.\nThey are trained by denoising a Markovian process that gradually adds noise to\nthe input. We argue that the Markovian property limits the models ability to\nfully utilize the generation trajectory, leading to inefficiencies during\ntraining and inference. In this paper, we propose DART, a transformer-based\nmodel that unifies autoregressive (AR) and diffusion within a non-Markovian\nframework. DART iteratively denoises image patches spatially and spectrally\nusing an AR model with the same architecture as standard language models. DART\ndoes not rely on image quantization, enabling more effective image modeling\nwhile maintaining flexibility. Furthermore, DART seamlessly trains with both\ntext and image data in a unified model. Our approach demonstrates competitive\nperformance on class-conditioned and text-to-image generation tasks, offering a\nscalable, efficient alternative to traditional diffusion models. Through this\nunified framework, DART sets a new benchmark for scalable, high-quality image\nsynthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have become the dominant approach for visual generation.\nThey are trained by denoising a Markovian process that gradually adds noise to\nthe input. We argue that the Markovian property limits the models ability to\nfully utilize the generation trajectory, leading to inefficiencies during\ntraining and inference. In this paper, we propose DART, a transformer-based\nmodel that unifies autoregressive (AR) and diffusion within a non-Markovian\nframework. DART iteratively denoises image patches spatially and spectrally\nusing an AR model with the same architecture as standard language models. DART\ndoes not rely on image quantization, enabling more effective image modeling\nwhile maintaining flexibility. Furthermore, DART seamlessly trains with both\ntext and image data in a unified model. Our approach demonstrates competitive\nperformance on class-conditioned and text-to-image generation tasks, offering a\nscalable, efficient alternative to traditional diffusion models. Through this\nunified framework, DART sets a new benchmark for scalable, high-quality image\nsynthesis."
                },
                "authors": [
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Qihang Zhang"
                    },
                    {
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Josh Susskind"
                    },
                    {
                        "name": "Shuangfei Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Shuangfei Zhai"
                },
                "author": "Shuangfei Zhai",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08146v1",
                "updated": "2024-10-10T17:31:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:31:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning"
                },
                "summary": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Xinyang Geng"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Alekh Agarwal"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08145v1",
                "updated": "2024-10-10T17:31:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    17,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:31:17Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    17,
                    3,
                    284,
                    0
                ],
                "title": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs"
                },
                "summary": "This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08143v1",
                "updated": "2024-10-10T17:30:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:30:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory"
                },
                "summary": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\ntranslation accuracy, and the summary component of the agent also shows promise\nas a tool for query-based summarization tasks. We release our code and data at\nhttps://github.com/YutongWang1216/DocMTAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\ntranslation accuracy, and the summary component of the agent also shows promise\nas a tool for query-based summarization tasks. We release our code and data at\nhttps://github.com/YutongWang1216/DocMTAgent."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19580v2",
                "updated": "2024-10-10T17:25:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    25,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-28T20:39:16Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    20,
                    39,
                    16,
                    6,
                    210,
                    0
                ],
                "title": "Mini-batch Coresets for Memory-efficient Training of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-batch Coresets for Memory-efficient Training of Large Language\n  Models"
                },
                "summary": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced nature of the sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V -projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr with\nLoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the\nmemory requirement of fine-tuning by 2x and even outperforms training with 4x\nlarger mini-batches. Notably, CoLM easily stack with existing memory-efficient\ntraining methods, such as LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced nature of the sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V -projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr with\nLoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the\nmemory requirement of fine-tuning by 2x and even outperforms training with 4x\nlarger mini-batches. Notably, CoLM easily stack with existing memory-efficient\ntraining methods, such as LoRA."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Rathul Anand"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Baharan Mirzasoleiman"
                    }
                ],
                "author_detail": {
                    "name": "Baharan Mirzasoleiman"
                },
                "author": "Baharan Mirzasoleiman",
                "arxiv_comment": "18 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v2",
                "updated": "2024-10-10T17:24:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    24,
                    1,
                    3,
                    284,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08134v1",
                "updated": "2024-10-10T17:18:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    18,
                    30,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:18:30Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    18,
                    30,
                    3,
                    284,
                    0
                ],
                "title": "Steering Masked Discrete Diffusion Models via Discrete Denoising\n  Posterior Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Masked Discrete Diffusion Models via Discrete Denoising\n  Posterior Prediction"
                },
                "summary": "Generative modeling of discrete data underlies important applications\nspanning text-based agents like ChatGPT to the design of the very building\nblocks of life in protein sequences. However, application domains need to exert\ncontrol over the generated data by steering the generative process - typically\nvia RLHF - to satisfy a specified property, reward, or affinity metric. In this\npaper, we study the problem of steering Masked Diffusion Models (MDMs), a\nrecent class of discrete diffusion models that offer a compelling alternative\nto traditional autoregressive models. We introduce Discrete Denoising Posterior\nPrediction (DDPP), a novel framework that casts the task of steering\npre-trained MDMs as a problem of probabilistic inference by learning to sample\nfrom a target Bayesian posterior. Our DDPP framework leads to a family of three\nnovel objectives that are all simulation-free, and thus scalable while applying\nto general non-differentiable reward functions. Empirically, we instantiate\nDDPP by steering MDMs to perform class-conditional pixel-level image modeling,\nRLHF-based alignment of MDMs using text-based rewards, and finetuning protein\nlanguage models to generate more diverse secondary structures and shorter\nproteins. We substantiate our designs via wet-lab validation, where we observe\ntransient expression of reward-optimized protein sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modeling of discrete data underlies important applications\nspanning text-based agents like ChatGPT to the design of the very building\nblocks of life in protein sequences. However, application domains need to exert\ncontrol over the generated data by steering the generative process - typically\nvia RLHF - to satisfy a specified property, reward, or affinity metric. In this\npaper, we study the problem of steering Masked Diffusion Models (MDMs), a\nrecent class of discrete diffusion models that offer a compelling alternative\nto traditional autoregressive models. We introduce Discrete Denoising Posterior\nPrediction (DDPP), a novel framework that casts the task of steering\npre-trained MDMs as a problem of probabilistic inference by learning to sample\nfrom a target Bayesian posterior. Our DDPP framework leads to a family of three\nnovel objectives that are all simulation-free, and thus scalable while applying\nto general non-differentiable reward functions. Empirically, we instantiate\nDDPP by steering MDMs to perform class-conditional pixel-level image modeling,\nRLHF-based alignment of MDMs using text-based rewards, and finetuning protein\nlanguage models to generate more diverse secondary structures and shorter\nproteins. We substantiate our designs via wet-lab validation, where we observe\ntransient expression of reward-optimized protein sequences."
                },
                "authors": [
                    {
                        "name": "Jarrid Rector-Brooks"
                    },
                    {
                        "name": "Mohsin Hasan"
                    },
                    {
                        "name": "Zhangzhi Peng"
                    },
                    {
                        "name": "Zachary Quinn"
                    },
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Pranam Chatterjee"
                    },
                    {
                        "name": "Alexander Tong"
                    },
                    {
                        "name": "Avishek Joey Bose"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Joey Bose"
                },
                "author": "Avishek Joey Bose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08133v1",
                "updated": "2024-10-10T17:17:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    17,
                    38,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:17:38Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    17,
                    38,
                    3,
                    284,
                    0
                ],
                "title": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks"
                },
                "summary": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models."
                },
                "authors": [
                    {
                        "name": "Mathis Pink"
                    },
                    {
                        "name": "Vy A. Vo"
                    },
                    {
                        "name": "Qinyuan Wu"
                    },
                    {
                        "name": "Jianing Mu"
                    },
                    {
                        "name": "Javier S. Turek"
                    },
                    {
                        "name": "Uri Hasson"
                    },
                    {
                        "name": "Kenneth A. Norman"
                    },
                    {
                        "name": "Sebastian Michelmann"
                    },
                    {
                        "name": "Alexander Huth"
                    },
                    {
                        "name": "Mariya Toneva"
                    }
                ],
                "author_detail": {
                    "name": "Mariya Toneva"
                },
                "author": "Mariya Toneva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08130v1",
                "updated": "2024-10-10T17:14:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:14:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning"
                },
                "summary": "This paper presents Dynamic Prompting, a novel framework aimed at improving\nthe reasoning capabilities of Large Language Models (LLMs). In contrast to\nconventional static prompting methods, Dynamic Prompting enables the adaptive\nmodification of prompt sequences and step counts based on real-time task\ncomplexity and model performance. This dynamic adaptation facilitates more\nefficient problem-solving, particularly in smaller models, by reducing\nhallucinations and repetitive cycles. Our empirical evaluations demonstrate\nthat Dynamic Prompting allows smaller LLMs to perform competitively with much\nlarger models, thereby challenging the conventional emphasis on model size as\nthe primary determinant of reasoning efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Dynamic Prompting, a novel framework aimed at improving\nthe reasoning capabilities of Large Language Models (LLMs). In contrast to\nconventional static prompting methods, Dynamic Prompting enables the adaptive\nmodification of prompt sequences and step counts based on real-time task\ncomplexity and model performance. This dynamic adaptation facilitates more\nefficient problem-solving, particularly in smaller models, by reducing\nhallucinations and repetitive cycles. Our empirical evaluations demonstrate\nthat Dynamic Prompting allows smaller LLMs to perform competitively with much\nlarger models, thereby challenging the conventional emphasis on model size as\nthe primary determinant of reasoning efficacy."
                },
                "authors": [
                    {
                        "name": "Kamesh R"
                    }
                ],
                "author_detail": {
                    "name": "Kamesh R"
                },
                "author": "Kamesh R",
                "arxiv_comment": "Submitted to ICLR 2025. This is a preprint version. Future revisions\n  will include additional evaluations and refinements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05515v2",
                "updated": "2024-10-10T17:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-07T21:41:24Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    21,
                    41,
                    24,
                    0,
                    281,
                    0
                ],
                "title": "MSPINN: Multiple scale method integrated physics-informed neural\n  networks for reconstructing transient natural convection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSPINN: Multiple scale method integrated physics-informed neural\n  networks for reconstructing transient natural convection"
                },
                "summary": "This study employs physics-informed neural networks (PINNs) to reconstruct\nmultiple flow fields in a transient natural convection system solely based on\ninstantaneous temperature data at an arbitrary moment. Transient convection\nproblems present reconstruction challenges due to the temporal variability of\nfields across different flow phases. In general, large reconstruction errors\nare observed during the incipient phase, while the quasi-steady phase exhibits\nrelatively smaller errors, reduced by a factor of 2 to 4. We hypothesize that\nreconstruction errors vary across different flow phases due to the changing\nsolution space of a PINN, inferred from the temporal gradients of the fields.\nFurthermore, we find that reconstruction errors tend to accumulate in regions\nwhere the spatial gradients are smaller than the order of $10^{-6}$, likely due\nto the vanishing gradient phenomenon. In convection phenomena, field variations\noften manifest across multiple scales in space. However, PINN-based\nreconstruction tends to preserve larger-scale variations, while smaller-scale\nvariations become less pronounced due to the vanishing gradient problem. To\nmitigate the errors associated with vanishing gradients, we introduce a\nmulti-scale approach that determines scaling constants for the PINN inputs and\nreformulates inputs across multiple scales. This approach improves the maximum\nand mean errors by 72.2% and 6.4%, respectively. Our research provides insights\ninto the behavior of PINNs when applied to transient convection problems with\nlarge solution space and field variations across multiple scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study employs physics-informed neural networks (PINNs) to reconstruct\nmultiple flow fields in a transient natural convection system solely based on\ninstantaneous temperature data at an arbitrary moment. Transient convection\nproblems present reconstruction challenges due to the temporal variability of\nfields across different flow phases. In general, large reconstruction errors\nare observed during the incipient phase, while the quasi-steady phase exhibits\nrelatively smaller errors, reduced by a factor of 2 to 4. We hypothesize that\nreconstruction errors vary across different flow phases due to the changing\nsolution space of a PINN, inferred from the temporal gradients of the fields.\nFurthermore, we find that reconstruction errors tend to accumulate in regions\nwhere the spatial gradients are smaller than the order of $10^{-6}$, likely due\nto the vanishing gradient phenomenon. In convection phenomena, field variations\noften manifest across multiple scales in space. However, PINN-based\nreconstruction tends to preserve larger-scale variations, while smaller-scale\nvariations become less pronounced due to the vanishing gradient problem. To\nmitigate the errors associated with vanishing gradients, we introduce a\nmulti-scale approach that determines scaling constants for the PINN inputs and\nreformulates inputs across multiple scales. This approach improves the maximum\nand mean errors by 72.2% and 6.4%, respectively. Our research provides insights\ninto the behavior of PINNs when applied to transient convection problems with\nlarge solution space and field variations across multiple scales."
                },
                "authors": [
                    {
                        "name": "Nagahiro Ohashi"
                    },
                    {
                        "name": "Nam Phuong Nguyen"
                    },
                    {
                        "name": "Leslie K. Hwang"
                    },
                    {
                        "name": "Beomjin Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Beomjin Kwon"
                },
                "author": "Beomjin Kwon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08126v1",
                "updated": "2024-10-10T17:10:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Mars: Situated Inductive Reasoning in an Open-World Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mars: Situated Inductive Reasoning in an Open-World Environment"
                },
                "summary": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Song-chun Zhu"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08119v1",
                "updated": "2024-10-10T17:02:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    2,
                    48,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:02:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    2,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-VLM: Post-training Quantization for Large Vision-Language Models"
                },
                "summary": "In this paper, we propose a post-training quantization framework of large\nvision-language models (LVLMs) for efficient multi-modal inference.\nConventional quantization methods sequentially search the layer-wise rounding\nfunctions by minimizing activation discretization errors, which fails to\nacquire optimal quantization strategy without considering cross-layer\ndependency. On the contrary, we mine the cross-layer dependency that\nsignificantly influences discretization errors of the entire vision-language\nmodel, and embed this dependency into optimal quantization strategy searching\nwith low search cost. Specifically, we observe the strong correlation between\nthe activation entropy and the cross-layer dependency concerning output\ndiscretization errors. Therefore, we employ the entropy as the proxy to\npartition blocks optimally, which aims to achieve satisfying trade-offs between\ndiscretization errors and the search cost. Moreover, we optimize the visual\nencoder to disentangle the cross-layer dependency for fine-grained\ndecomposition of search space, so that the search cost is further reduced\nwithout harming the quantization accuracy. Experimental results demonstrate\nthat our method compresses the memory by 2.78x and increase generate speed by\n1.44x about 13B LLaVA model without performance degradation on diverse\nmulti-modal reasoning tasks. Code is available at\nhttps://github.com/ChangyuanWang17/QVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a post-training quantization framework of large\nvision-language models (LVLMs) for efficient multi-modal inference.\nConventional quantization methods sequentially search the layer-wise rounding\nfunctions by minimizing activation discretization errors, which fails to\nacquire optimal quantization strategy without considering cross-layer\ndependency. On the contrary, we mine the cross-layer dependency that\nsignificantly influences discretization errors of the entire vision-language\nmodel, and embed this dependency into optimal quantization strategy searching\nwith low search cost. Specifically, we observe the strong correlation between\nthe activation entropy and the cross-layer dependency concerning output\ndiscretization errors. Therefore, we employ the entropy as the proxy to\npartition blocks optimally, which aims to achieve satisfying trade-offs between\ndiscretization errors and the search cost. Moreover, we optimize the visual\nencoder to disentangle the cross-layer dependency for fine-grained\ndecomposition of search space, so that the search cost is further reduced\nwithout harming the quantization accuracy. Experimental results demonstrate\nthat our method compresses the memory by 2.78x and increase generate speed by\n1.44x about 13B LLaVA model without performance degradation on diverse\nmulti-modal reasoning tasks. Code is available at\nhttps://github.com/ChangyuanWang17/QVLM."
                },
                "authors": [
                    {
                        "name": "Changyuan Wang"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08118v1",
                "updated": "2024-10-10T17:01:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    1,
                    57,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:01:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    1,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Medical Image Quality Assessment based on Probability of Necessity and\n  Sufficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Image Quality Assessment based on Probability of Necessity and\n  Sufficiency"
                },
                "summary": "Medical image quality assessment (MIQA) is essential for reliable medical\nimage analysis. While deep learning has shown promise in this field, current\nmodels could be misled by spurious correlations learned from data and struggle\nwith out-of-distribution (OOD) scenarios. To that end, we propose an MIQA\nframework based on a concept from causal inference: Probability of Necessity\nand Sufficiency (PNS). PNS measures how likely a set of features is to be both\nnecessary (always present for an outcome) and sufficient (capable of\nguaranteeing an outcome) for a particular result. Our approach leverages this\nconcept by learning hidden features from medical images with high PNS values\nfor quality prediction. This encourages models to capture more essential\npredictive information, enhancing their robustness to OOD scenarios. We\nevaluate our framework on an Anterior Segment Optical Coherence Tomography\n(AS-OCT) dataset for the MIQA task and experimental results demonstrate the\neffectiveness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image quality assessment (MIQA) is essential for reliable medical\nimage analysis. While deep learning has shown promise in this field, current\nmodels could be misled by spurious correlations learned from data and struggle\nwith out-of-distribution (OOD) scenarios. To that end, we propose an MIQA\nframework based on a concept from causal inference: Probability of Necessity\nand Sufficiency (PNS). PNS measures how likely a set of features is to be both\nnecessary (always present for an outcome) and sufficient (capable of\nguaranteeing an outcome) for a particular result. Our approach leverages this\nconcept by learning hidden features from medical images with high PNS values\nfor quality prediction. This encourages models to capture more essential\npredictive information, enhancing their robustness to OOD scenarios. We\nevaluate our framework on an Anterior Segment Optical Coherence Tomography\n(AS-OCT) dataset for the MIQA task and experimental results demonstrate the\neffectiveness of our framework."
                },
                "authors": [
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Ameenat L. Solebo"
                    },
                    {
                        "name": "Weiye Bao"
                    },
                    {
                        "name": "Paul Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Paul Taylor"
                },
                "author": "Paul Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08116v1",
                "updated": "2024-10-10T17:01:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    1,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:01:53Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    1,
                    53,
                    3,
                    284,
                    0
                ],
                "title": "BOWIE-ALIGN: JWST reveals hints of planetesimal accretion and complex\n  sulphur chemistry in the atmosphere of the misaligned hot Jupiter WASP-15b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOWIE-ALIGN: JWST reveals hints of planetesimal accretion and complex\n  sulphur chemistry in the atmosphere of the misaligned hot Jupiter WASP-15b"
                },
                "summary": "We present a transmission spectrum of the misaligned hot Jupiter WASP-15b\nfrom 2.8--5.2 microns observed with JWST's NIRSpec/G395H grating. Our high\nsignal to noise data, which has negligible red noise, reveals significant\nabsorption by H$_2$O ($4.2\\sigma$) and CO$_2$ ($8.9\\sigma$). From independent\ndata reduction and atmospheric retrieval approaches, we infer that WASP-15b's\natmospheric metallicity is super-solar ($\\gtrsim 15\\times$ solar) and its C/O\nis consistent with solar, that together imply planetesimal accretion. Our GCM\nsimulations for WASP-15b suggest that the C/O we measure at the limb is likely\nrepresentative of the entire photosphere due to the mostly uniform spatial\ndistribution of H$_2$O, CO$_2$ and CO. We additionally see evidence for\nabsorption by SO$_2$ and absorption at 4.9$\\mu$m, for which the current leading\ncandidate is OCS, albeit with several caveats. If confirmed, this would be the\nfirst detection of OCS in an exoplanet atmosphere and point towards complex\nphotochemistry of sulphur-bearing species in the upper atmosphere. These are\nthe first observations from the BOWIE-ALIGN survey which is using JWST's\nNIRSpec/G395H instrument to compare the atmospheric compositions of\naligned/low-obliquity and misaligned/high-obliquity hot Jupiters around F stars\nabove the Kraft break. The goal of our survey is to determine whether the\natmospheric composition differs across two populations of planets that have\nlikely undergone different migration histories (disc versus disc-free) as\nevidenced by their obliquities (aligned versus misaligned).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a transmission spectrum of the misaligned hot Jupiter WASP-15b\nfrom 2.8--5.2 microns observed with JWST's NIRSpec/G395H grating. Our high\nsignal to noise data, which has negligible red noise, reveals significant\nabsorption by H$_2$O ($4.2\\sigma$) and CO$_2$ ($8.9\\sigma$). From independent\ndata reduction and atmospheric retrieval approaches, we infer that WASP-15b's\natmospheric metallicity is super-solar ($\\gtrsim 15\\times$ solar) and its C/O\nis consistent with solar, that together imply planetesimal accretion. Our GCM\nsimulations for WASP-15b suggest that the C/O we measure at the limb is likely\nrepresentative of the entire photosphere due to the mostly uniform spatial\ndistribution of H$_2$O, CO$_2$ and CO. We additionally see evidence for\nabsorption by SO$_2$ and absorption at 4.9$\\mu$m, for which the current leading\ncandidate is OCS, albeit with several caveats. If confirmed, this would be the\nfirst detection of OCS in an exoplanet atmosphere and point towards complex\nphotochemistry of sulphur-bearing species in the upper atmosphere. These are\nthe first observations from the BOWIE-ALIGN survey which is using JWST's\nNIRSpec/G395H instrument to compare the atmospheric compositions of\naligned/low-obliquity and misaligned/high-obliquity hot Jupiters around F stars\nabove the Kraft break. The goal of our survey is to determine whether the\natmospheric composition differs across two populations of planets that have\nlikely undergone different migration histories (disc versus disc-free) as\nevidenced by their obliquities (aligned versus misaligned)."
                },
                "authors": [
                    {
                        "name": "James Kirk"
                    },
                    {
                        "name": "Eva-Maria Ahrer"
                    },
                    {
                        "name": "Alastair B. Claringbold"
                    },
                    {
                        "name": "Maria Zamyatina"
                    },
                    {
                        "name": "Chloe Fisher"
                    },
                    {
                        "name": "Mason McCormack"
                    },
                    {
                        "name": "Vatsal Panwar"
                    },
                    {
                        "name": "Diana Powell"
                    },
                    {
                        "name": "Jake Taylor"
                    },
                    {
                        "name": "Daniel P. Thorngren"
                    },
                    {
                        "name": "Duncan A. Christie"
                    },
                    {
                        "name": "Emma Esparza-Borges"
                    },
                    {
                        "name": "Shang-Min Tsai"
                    },
                    {
                        "name": "Lili Alderson"
                    },
                    {
                        "name": "Richard A. Booth"
                    },
                    {
                        "name": "Charlotte Fairman"
                    },
                    {
                        "name": "Mercedes Lpez-Morales"
                    },
                    {
                        "name": "N. J. Mayne"
                    },
                    {
                        "name": "Annabella Meech"
                    },
                    {
                        "name": "Paul Molliere"
                    },
                    {
                        "name": "James E. Owen"
                    },
                    {
                        "name": "Anna B. T. Penzlin"
                    },
                    {
                        "name": "Denis E. Sergeev"
                    },
                    {
                        "name": "Daniel Valentine"
                    },
                    {
                        "name": "Hannah R. Wakeford"
                    },
                    {
                        "name": "Peter J. Wheatley"
                    }
                ],
                "author_detail": {
                    "name": "Peter J. Wheatley"
                },
                "author": "Peter J. Wheatley",
                "arxiv_comment": "24 pages, 23 figures, 6 tables. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08115v1",
                "updated": "2024-10-10T17:00:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    0,
                    6,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:00:06Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    0,
                    6,
                    3,
                    284,
                    0
                ],
                "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page)."
                },
                "authors": [
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Jiarui Yuan"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09020v2",
                "updated": "2024-10-10T16:58:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    58,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2023-11-15T15:08:38Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    15,
                    8,
                    38,
                    2,
                    319,
                    0
                ],
                "title": "Explaining Explanation: An Empirical Study on Explanation in Code\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Explanation: An Empirical Study on Explanation in Code\n  Reviews"
                },
                "summary": "Code reviews are central for software quality assurance. Ideally, reviewers\nshould explain their feedback to enable authors of code changes to understand\nthe feedback and act accordingly. Different developers might need different\nexplanations in different contexts. Therefore, assisting this process first\nrequires understanding the types of explanations reviewers usually provide. The\ngoal of this paper is to study the types of explanations used in code reviews\nand explore the potential of Large Language Models (LLMs), specifically\nChatGPT, in generating these specific types. We extracted 793 code review\ncomments from Gerrit and manually labeled them based on whether they contained\na suggestion, an explanation, or both. Our analysis shows that 42% of comments\nonly include suggestions without explanations. We categorized the explanations\ninto seven distinct types including rule or principle, similar examples, and\nfuture implications. When measuring their prevalence, we observed that some\nexplanations are used differently by novice and experienced reviewers. Our\nmanual evaluation shows that, when the explanation type is specified, ChatGPT\ncan correctly generate the explanation in 88 out of 90 cases. This foundational\nwork highlights the potential for future automation in code reviews, which can\nassist developers in sharing and obtaining different types of explanations as\nneeded, thereby reducing back-and-forth communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code reviews are central for software quality assurance. Ideally, reviewers\nshould explain their feedback to enable authors of code changes to understand\nthe feedback and act accordingly. Different developers might need different\nexplanations in different contexts. Therefore, assisting this process first\nrequires understanding the types of explanations reviewers usually provide. The\ngoal of this paper is to study the types of explanations used in code reviews\nand explore the potential of Large Language Models (LLMs), specifically\nChatGPT, in generating these specific types. We extracted 793 code review\ncomments from Gerrit and manually labeled them based on whether they contained\na suggestion, an explanation, or both. Our analysis shows that 42% of comments\nonly include suggestions without explanations. We categorized the explanations\ninto seven distinct types including rule or principle, similar examples, and\nfuture implications. When measuring their prevalence, we observed that some\nexplanations are used differently by novice and experienced reviewers. Our\nmanual evaluation shows that, when the explanation type is specified, ChatGPT\ncan correctly generate the explanation in 88 out of 90 cases. This foundational\nwork highlights the potential for future automation in code reviews, which can\nassist developers in sharing and obtaining different types of explanations as\nneeded, thereby reducing back-and-forth communication."
                },
                "authors": [
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Abir Bouraffa"
                    },
                    {
                        "name": "Walid Maalej"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08109v1",
                "updated": "2024-10-10T16:56:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    56,
                    5,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    56,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "A Closer Look at Machine Unlearning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at Machine Unlearning for Large Language Models"
                },
                "summary": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning."
                },
                "authors": [
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08105v1",
                "updated": "2024-10-10T16:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    53,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    53,
                    10,
                    3,
                    284,
                    0
                ],
                "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?"
                },
                "summary": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation."
                },
                "authors": [
                    {
                        "name": "Kunhao Zheng"
                    },
                    {
                        "name": "Juliette Decugis"
                    },
                    {
                        "name": "Jonas Gehring"
                    },
                    {
                        "name": "Taco Cohen"
                    },
                    {
                        "name": "Benjamin Negrevergne"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08102v1",
                "updated": "2024-10-10T16:45:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:45:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining"
                },
                "summary": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhen Hao Wong"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Qiu Jiantao"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05292v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05292v5",
                "updated": "2024-10-10T16:29:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    59,
                    3,
                    284,
                    0
                ],
                "published": "2023-10-08T21:39:47Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    21,
                    39,
                    47,
                    6,
                    281,
                    0
                ],
                "title": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent\n  for Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent\n  for Debugging"
                },
                "summary": "Large Language Models (LLMs) now excel at generative skills and can create\ncontent at impeccable speeds. However, they are imperfect and still make\nvarious mistakes. In a Computer Science education context, as these models are\nwidely recognized as \"AI pair programmers,\" it becomes increasingly important\nto train students on evaluating and debugging the LLM-generated code. In this\nwork, we introduce HypoCompass, a novel system to facilitate deliberate\npractice on debugging, where human novices play the role of Teaching Assistants\nand help LLM-powered teachable agents debug code. We enable effective task\ndelegation between students and LLMs in this learning-by-teaching environment:\nstudents focus on hypothesizing the cause of code errors, while adjacent skills\nlike code completion are offloaded to LLM-agents. Our evaluations demonstrate\nthat HypoCompass generates high-quality training materials (e.g., bugs and\nfixes), outperforming human counterparts fourfold in efficiency, and\nsignificantly improves student performance on debugging by 12% in the\npre-to-post test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) now excel at generative skills and can create\ncontent at impeccable speeds. However, they are imperfect and still make\nvarious mistakes. In a Computer Science education context, as these models are\nwidely recognized as \"AI pair programmers,\" it becomes increasingly important\nto train students on evaluating and debugging the LLM-generated code. In this\nwork, we introduce HypoCompass, a novel system to facilitate deliberate\npractice on debugging, where human novices play the role of Teaching Assistants\nand help LLM-powered teachable agents debug code. We enable effective task\ndelegation between students and LLMs in this learning-by-teaching environment:\nstudents focus on hypothesizing the cause of code errors, while adjacent skills\nlike code completion are offloaded to LLM-agents. Our evaluations demonstrate\nthat HypoCompass generates high-quality training materials (e.g., bugs and\nfixes), outperforming human counterparts fourfold in efficiency, and\nsignificantly improves student performance on debugging by 12% in the\npre-to-post test."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_doi": "10.1007/978-3-031-64302-6_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-64302-6_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.05292v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05292v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures",
                "arxiv_journal_ref": "AIED 2024, LNAI 14829, pp. 1-16",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08085v1",
                "updated": "2024-10-10T16:29:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    21,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:29:21Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    21,
                    3,
                    284,
                    0
                ],
                "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study over Open-ended Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study over Open-ended Question Answering"
                },
                "summary": "Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks mainly focus on closed tasks, leaving a gap in the\nassessment of more complex, real-world scenarios. This gap has also obscured\nthe evaluation of KGs' potential to mitigate the problem of hallucination in\nLLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically\ndesigned to assess LLMs enhanced with KGs under open-ended, real-world question\nanswering scenarios. OKGQA is designed to closely reflect the complexities of\npractical applications using questions from different types, and incorporates\nspecific metrics to measure both the reduction in hallucinations and the\nenhancement in reasoning capabilities. To consider the scenario in which KGs\nmay have varying levels of mistakes, we further propose another experiment\nsetting OKGQA-P to assess model performance when the semantics and structure of\nKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore\nwhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)\nconduct a comparative analysis to shed light on methods and future directions\nfor leveraging KGs to reduce LLMs' hallucination. We believe that this study\ncan facilitate a more complete performance comparison and encourage continuous\nimprovement in integrating KGs with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks mainly focus on closed tasks, leaving a gap in the\nassessment of more complex, real-world scenarios. This gap has also obscured\nthe evaluation of KGs' potential to mitigate the problem of hallucination in\nLLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically\ndesigned to assess LLMs enhanced with KGs under open-ended, real-world question\nanswering scenarios. OKGQA is designed to closely reflect the complexities of\npractical applications using questions from different types, and incorporates\nspecific metrics to measure both the reduction in hallucinations and the\nenhancement in reasoning capabilities. To consider the scenario in which KGs\nmay have varying levels of mistakes, we further propose another experiment\nsetting OKGQA-P to assess model performance when the semantics and structure of\nKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore\nwhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)\nconduct a comparative analysis to shed light on methods and future directions\nfor leveraging KGs to reduce LLMs' hallucination. We believe that this study\ncan facilitate a more complete performance comparison and encourage continuous\nimprovement in integrating KGs with LLMs."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12971v2",
                "updated": "2024-10-10T16:27:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    27,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-02-20T12:40:31Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    12,
                    40,
                    31,
                    1,
                    51,
                    0
                ],
                "title": "Differentiability in Unrolled Training of Neural Physics Simulators on\n  Transient Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiability in Unrolled Training of Neural Physics Simulators on\n  Transient Dynamics"
                },
                "summary": "Unrolling training trajectories over time strongly influences the inference\naccuracy of neural network-augmented physics simulators. We analyze this in\nthree variants of training neural time-steppers. In addition to one-step setups\nand fully differentiable unrolling, we include a third, less widely used\nvariant: unrolling without temporal gradients. Comparing networks trained with\nthese three modalities disentangles the two dominant effects of unrolling,\ntraining distribution shift and long-term gradients. We present detailed study\nacross physical systems, network sizes and architectures, training setups, and\ntest scenarios. It also encompasses two simulation modes: In prediction setups,\nwe rely solely on neural networks to compute a trajectory. In contrast,\ncorrection setups include a numerical solver that is supported by a neural\nnetwork. Spanning these variations, our study provides the empirical basis for\nour main findings: Non-differentiable but unrolled training with a numerical\nsolver in a correction setup can yield substantial improvements over a fully\ndifferentiable prediction setup not utilizing this solver. The accuracy of\nmodels trained in a fully differentiable setup differs compared to their\nnon-differentiable counterparts. Differentiable ones perform best in a\ncomparison among correction networks as well as among prediction setups. For\nboth, the accuracy of non-differentiable unrolling comes close. Furthermore, we\nshow that these behaviors are invariant to the physical system, the network\narchitecture and size, and the numerical scheme. These results motivate\nintegrating non-differentiable numerical simulators into training setups even\nif full differentiability is unavailable. We show the convergence rate of\ncommon architectures to be low compared to numerical algorithms. This motivates\ncorrection setups combining neural and numerical parts which utilize benefits\nof both.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unrolling training trajectories over time strongly influences the inference\naccuracy of neural network-augmented physics simulators. We analyze this in\nthree variants of training neural time-steppers. In addition to one-step setups\nand fully differentiable unrolling, we include a third, less widely used\nvariant: unrolling without temporal gradients. Comparing networks trained with\nthese three modalities disentangles the two dominant effects of unrolling,\ntraining distribution shift and long-term gradients. We present detailed study\nacross physical systems, network sizes and architectures, training setups, and\ntest scenarios. It also encompasses two simulation modes: In prediction setups,\nwe rely solely on neural networks to compute a trajectory. In contrast,\ncorrection setups include a numerical solver that is supported by a neural\nnetwork. Spanning these variations, our study provides the empirical basis for\nour main findings: Non-differentiable but unrolled training with a numerical\nsolver in a correction setup can yield substantial improvements over a fully\ndifferentiable prediction setup not utilizing this solver. The accuracy of\nmodels trained in a fully differentiable setup differs compared to their\nnon-differentiable counterparts. Differentiable ones perform best in a\ncomparison among correction networks as well as among prediction setups. For\nboth, the accuracy of non-differentiable unrolling comes close. Furthermore, we\nshow that these behaviors are invariant to the physical system, the network\narchitecture and size, and the numerical scheme. These results motivate\nintegrating non-differentiable numerical simulators into training setups even\nif full differentiability is unavailable. We show the convergence rate of\ncommon architectures to be low compared to numerical algorithms. This motivates\ncorrection setups combining neural and numerical parts which utilize benefits\nof both."
                },
                "authors": [
                    {
                        "name": "Bjoern List"
                    },
                    {
                        "name": "Li-Wei Chen"
                    },
                    {
                        "name": "Kartik Bali"
                    },
                    {
                        "name": "Nils Thuerey"
                    }
                ],
                "author_detail": {
                    "name": "Nils Thuerey"
                },
                "author": "Nils Thuerey",
                "arxiv_comment": "Project Page:\n  https://ge.in.tum.de/publications/how-temporal-unrolling-supports-neural-physics-simulators/\n  , Github Page: https://github.com/tum-pbs/unrolling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18034v2",
                "updated": "2024-10-10T16:19:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    19,
                    59,
                    3,
                    284,
                    0
                ],
                "published": "2024-01-31T17:58:10Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    17,
                    58,
                    10,
                    2,
                    31,
                    0
                ],
                "title": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages"
                },
                "summary": "We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11915v2",
                "updated": "2024-10-10T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    13,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-16T21:11:23Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    21,
                    11,
                    23,
                    6,
                    168,
                    0
                ],
                "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "miniCodeProps: a Minimal Benchmark for Proving Code Properties"
                },
                "summary": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode."
                },
                "authors": [
                    {
                        "name": "Evan Lohn"
                    },
                    {
                        "name": "Sean Welleck"
                    }
                ],
                "author_detail": {
                    "name": "Sean Welleck"
                },
                "author": "Sean Welleck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09116v2",
                "updated": "2024-10-10T16:09:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    9,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-13T13:43:59Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    13,
                    43,
                    59,
                    3,
                    165,
                    0
                ],
                "title": "Injective flows for star-like manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injective flows for star-like manifolds"
                },
                "summary": "Normalizing Flows (NFs) are powerful and efficient models for density\nestimation. When modeling densities on manifolds, NFs can be generalized to\ninjective flows but the Jacobian determinant becomes computationally\nprohibitive. Current approaches either consider bounds on the log-likelihood or\nrely on some approximations of the Jacobian determinant. In contrast, we\npropose injective flows for star-like manifolds and show that for such\nmanifolds we can compute the Jacobian determinant exactly and efficiently, with\nthe same cost as NFs. This aspect is particularly relevant for variational\ninference settings, where no samples are available and only some unnormalized\ntarget is known. Among many, we showcase the relevance of modeling densities on\nstar-like manifolds in two settings. Firstly, we introduce a novel Objective\nBayesian approach for penalized likelihood models by interpreting level-sets of\nthe penalty as star-like manifolds. Secondly, we consider probabilistic mixing\nmodels and introduce a general method for variational inference by defining the\nposterior of mixture weights on the probability simplex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalizing Flows (NFs) are powerful and efficient models for density\nestimation. When modeling densities on manifolds, NFs can be generalized to\ninjective flows but the Jacobian determinant becomes computationally\nprohibitive. Current approaches either consider bounds on the log-likelihood or\nrely on some approximations of the Jacobian determinant. In contrast, we\npropose injective flows for star-like manifolds and show that for such\nmanifolds we can compute the Jacobian determinant exactly and efficiently, with\nthe same cost as NFs. This aspect is particularly relevant for variational\ninference settings, where no samples are available and only some unnormalized\ntarget is known. Among many, we showcase the relevance of modeling densities on\nstar-like manifolds in two settings. Firstly, we introduce a novel Objective\nBayesian approach for penalized likelihood models by interpreting level-sets of\nthe penalty as star-like manifolds. Secondly, we consider probabilistic mixing\nmodels and introduce a general method for variational inference by defining the\nposterior of mixture weights on the probability simplex."
                },
                "authors": [
                    {
                        "name": "Marcello Massimo Negri"
                    },
                    {
                        "name": "Jonathan Aellen"
                    },
                    {
                        "name": "Volker Roth"
                    }
                ],
                "author_detail": {
                    "name": "Volker Roth"
                },
                "author": "Volker Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16785v2",
                "updated": "2024-10-10T16:09:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    9,
                    22,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-27T03:13:28Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    3,
                    13,
                    28,
                    0,
                    148,
                    0
                ],
                "title": "PromptFix: You Prompt and We Fix the Photo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptFix: You Prompt and We Fix the Photo"
                },
                "summary": "Diffusion models equipped with language models demonstrate excellent\ncontrollability in image generation tasks, allowing image processing to adhere\nto human instructions. However, the lack of diverse instruction-following data\nhampers the development of models that effectively recognize and execute\nuser-customized instructions, particularly in low-level tasks. Moreover, the\nstochastic nature of the diffusion process leads to deficiencies in image\ngeneration or editing tasks that require the detailed preservation of the\ngenerated images. To address these limitations, we propose PromptFix, a\ncomprehensive framework that enables diffusion models to follow human\ninstructions to perform a wide variety of image-processing tasks. First, we\nconstruct a large-scale instruction-following dataset that covers comprehensive\nimage-processing tasks, including low-level tasks, image editing, and object\ncreation. Next, we propose a high-frequency guidance sampling method to\nexplicitly control the denoising process and preserve high-frequency details in\nunprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing\nVision-Language Models (VLMs) to enhance text prompts and improve the model's\ntask generalization. Experimental results show that PromptFix outperforms\nprevious methods in various image-processing tasks. Our proposed model also\nachieves comparable inference efficiency with these baseline models and\nexhibits superior zero-shot capabilities in blind restoration and combination\ntasks. The dataset and code are available at\nhttps://www.yongshengyu.com/PromptFix-Page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models equipped with language models demonstrate excellent\ncontrollability in image generation tasks, allowing image processing to adhere\nto human instructions. However, the lack of diverse instruction-following data\nhampers the development of models that effectively recognize and execute\nuser-customized instructions, particularly in low-level tasks. Moreover, the\nstochastic nature of the diffusion process leads to deficiencies in image\ngeneration or editing tasks that require the detailed preservation of the\ngenerated images. To address these limitations, we propose PromptFix, a\ncomprehensive framework that enables diffusion models to follow human\ninstructions to perform a wide variety of image-processing tasks. First, we\nconstruct a large-scale instruction-following dataset that covers comprehensive\nimage-processing tasks, including low-level tasks, image editing, and object\ncreation. Next, we propose a high-frequency guidance sampling method to\nexplicitly control the denoising process and preserve high-frequency details in\nunprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing\nVision-Language Models (VLMs) to enhance text prompts and improve the model's\ntask generalization. Experimental results show that PromptFix outperforms\nprevious methods in various image-processing tasks. Our proposed model also\nachieves comparable inference efficiency with these baseline models and\nexhibits superior zero-shot capabilities in blind restoration and combination\ntasks. The dataset and code are available at\nhttps://www.yongshengyu.com/PromptFix-Page."
                },
                "authors": [
                    {
                        "name": "Yongsheng Yu"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Jianlong Fu"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08068v1",
                "updated": "2024-10-10T16:02:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    2,
                    36,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:02:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    2,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting."
                },
                "authors": [
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Dongxiao Chen"
                    },
                    {
                        "name": "Jieting Xue"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Taijie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taijie Chen"
                },
                "author": "Taijie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v1",
                "updated": "2024-10-10T16:01:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17665v3",
                "updated": "2024-10-10T15:55:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    55,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-27T21:33:56Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    33,
                    56,
                    0,
                    148,
                    0
                ],
                "title": "Improving Robotic Arms through Natural Language Processing, Computer\n  Vision, and Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Robotic Arms through Natural Language Processing, Computer\n  Vision, and Edge Computing"
                },
                "summary": "This paper introduces a prototype for a new approach to assistive robotics,\nintegrating edge computing with Natural Language Processing (NLP) and computer\nvision to enhance the interaction between humans and robotic systems. Our proof\nof concept demonstrates the feasibility of using large language models (LLMs)\nand vision systems in tandem for interpreting and executing complex commands\nconveyed through natural language. This integration aims to improve the\nintuitiveness and accessibility of assistive robotic systems, making them more\nadaptable to the nuanced needs of users with disabilities. By leveraging the\ncapabilities of edge computing, our system has the potential to minimize\nlatency and support offline capability, enhancing the autonomy and\nresponsiveness of assistive robots. Experimental results from our\nimplementation on a robotic arm show promising outcomes in terms of accurate\nintent interpretation and object manipulation based on verbal commands. This\nresearch lays the groundwork for future developments in assistive robotics,\nfocusing on creating highly responsive, user-centric systems that can\nsignificantly improve the quality of life for individuals with disabilities.\nFor video demonstrations and source code, please refer to:\nhttps://tinyurl.com/EnhancedArmEdgeNLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a prototype for a new approach to assistive robotics,\nintegrating edge computing with Natural Language Processing (NLP) and computer\nvision to enhance the interaction between humans and robotic systems. Our proof\nof concept demonstrates the feasibility of using large language models (LLMs)\nand vision systems in tandem for interpreting and executing complex commands\nconveyed through natural language. This integration aims to improve the\nintuitiveness and accessibility of assistive robotic systems, making them more\nadaptable to the nuanced needs of users with disabilities. By leveraging the\ncapabilities of edge computing, our system has the potential to minimize\nlatency and support offline capability, enhancing the autonomy and\nresponsiveness of assistive robots. Experimental results from our\nimplementation on a robotic arm show promising outcomes in terms of accurate\nintent interpretation and object manipulation based on verbal commands. This\nresearch lays the groundwork for future developments in assistive robotics,\nfocusing on creating highly responsive, user-centric systems that can\nsignificantly improve the quality of life for individuals with disabilities.\nFor video demonstrations and source code, please refer to:\nhttps://tinyurl.com/EnhancedArmEdgeNLP."
                },
                "authors": [
                    {
                        "name": "Pascal Sikorski"
                    },
                    {
                        "name": "Kaleb Yu"
                    },
                    {
                        "name": "Lucy Billadeau"
                    },
                    {
                        "name": "Flavio Esposito"
                    },
                    {
                        "name": "Hadi AliAkbarpour"
                    },
                    {
                        "name": "Madi Babaiasl"
                    }
                ],
                "author_detail": {
                    "name": "Madi Babaiasl"
                },
                "author": "Madi Babaiasl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14438v2",
                "updated": "2024-10-10T15:55:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    55,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-23T11:10:32Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    10,
                    32,
                    3,
                    144,
                    0
                ],
                "title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks"
                },
                "summary": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets."
                },
                "authors": [
                    {
                        "name": "Michelle Halbheer"
                    },
                    {
                        "name": "Dominik J. Mhlematter"
                    },
                    {
                        "name": "Alexander Becker"
                    },
                    {
                        "name": "Dominik Narnhofer"
                    },
                    {
                        "name": "Helge Aasen"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Mehmet Ozgur Turkoglu"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Ozgur Turkoglu"
                },
                "author": "Mehmet Ozgur Turkoglu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17026v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17026v4",
                "updated": "2024-10-10T15:51:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    51,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2023-05-26T15:35:43Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    15,
                    35,
                    43,
                    4,
                    146,
                    0
                ],
                "title": "How Powerful are Decoder-Only Transformer Neural Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Powerful are Decoder-Only Transformer Neural Models?"
                },
                "summary": "In this article we prove that the general transformer neural model\nundergirding modern large language models (LLMs) is Turing complete under\nreasonable assumptions. This is the first work to directly address the Turing\ncompleteness of the underlying technology employed in GPT-x as past work has\nfocused on the more expressive, full auto-encoder transformer architecture.\nFrom this theoretical analysis, we show that the sparsity/compressibility of\nthe word embedding is an important consideration for Turing completeness to\nhold. We also show that Transformers are are a variant of B machines studied by\nHao Wang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we prove that the general transformer neural model\nundergirding modern large language models (LLMs) is Turing complete under\nreasonable assumptions. This is the first work to directly address the Turing\ncompleteness of the underlying technology employed in GPT-x as past work has\nfocused on the more expressive, full auto-encoder transformer architecture.\nFrom this theoretical analysis, we show that the sparsity/compressibility of\nthe word embedding is an important consideration for Turing completeness to\nhold. We also show that Transformers are are a variant of B machines studied by\nHao Wang."
                },
                "authors": [
                    {
                        "name": "Jesse Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Roberts"
                },
                "author": "Jesse Roberts",
                "arxiv_doi": "10.1109/IJCNN60899.2024.10651286",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN60899.2024.10651286",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.17026v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17026v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IJCNN 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08053v1",
                "updated": "2024-10-10T15:46:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    46,
                    27,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:46:27Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    46,
                    27,
                    3,
                    284,
                    0
                ],
                "title": "A Target-Aware Analysis of Data Augmentation for Hate Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Target-Aware Analysis of Data Augmentation for Hate Speech Detection"
                },
                "summary": "Hate speech is one of the main threats posed by the widespread use of social\nnetworks, despite efforts to limit it. Although attention has been devoted to\nthis issue, the lack of datasets and case studies centered around scarcely\nrepresented phenomena, such as ableism or ageism, can lead to hate speech\ndetection systems that do not perform well on underrepresented identity groups.\nGiven the unpreceded capabilities of LLMs in producing high-quality data, we\ninvestigate the possibility of augmenting existing data with generative\nlanguage models, reducing target imbalance. We experiment with augmenting 1,000\nposts from the Measuring Hate Speech corpus, an English dataset annotated with\ntarget identity information, adding around 30,000 synthetic examples using both\nsimple data augmentation methods and different types of generative models,\ncomparing autoregressive and sequence-to-sequence approaches. We find\ntraditional DA methods to often be preferable to generative models, but the\ncombination of the two tends to lead to the best results. Indeed, for some hate\ncategories such as origin, religion, and disability, hate speech classification\nusing augmented data for training improves by more than 10% F1 over the no\naugmentation baseline. This work contributes to the development of systems for\nhate speech detection that are not only better performing but also fairer and\nmore inclusive towards targets that have been neglected so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech is one of the main threats posed by the widespread use of social\nnetworks, despite efforts to limit it. Although attention has been devoted to\nthis issue, the lack of datasets and case studies centered around scarcely\nrepresented phenomena, such as ableism or ageism, can lead to hate speech\ndetection systems that do not perform well on underrepresented identity groups.\nGiven the unpreceded capabilities of LLMs in producing high-quality data, we\ninvestigate the possibility of augmenting existing data with generative\nlanguage models, reducing target imbalance. We experiment with augmenting 1,000\nposts from the Measuring Hate Speech corpus, an English dataset annotated with\ntarget identity information, adding around 30,000 synthetic examples using both\nsimple data augmentation methods and different types of generative models,\ncomparing autoregressive and sequence-to-sequence approaches. We find\ntraditional DA methods to often be preferable to generative models, but the\ncombination of the two tends to lead to the best results. Indeed, for some hate\ncategories such as origin, religion, and disability, hate speech classification\nusing augmented data for training improves by more than 10% F1 over the no\naugmentation baseline. This work contributes to the development of systems for\nhate speech detection that are not only better performing but also fairer and\nmore inclusive towards targets that have been neglected so far."
                },
                "authors": [
                    {
                        "name": "Camilla Casula"
                    },
                    {
                        "name": "Sara Tonelli"
                    }
                ],
                "author_detail": {
                    "name": "Sara Tonelli"
                },
                "author": "Sara Tonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08048v1",
                "updated": "2024-10-10T15:43:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:43:55Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers"
                },
                "summary": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains."
                },
                "authors": [
                    {
                        "name": "Jianing Qi"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Zhigang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhigang Zhu"
                },
                "author": "Zhigang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08049v1",
                "updated": "2024-10-10T15:43:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:43:55Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "title": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards\n  Universal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards\n  Universal Representations"
                },
                "summary": "This paper proposes the paradigm of large convolutional kernels in designing\nmodern Convolutional Neural Networks (ConvNets). We establish that employing a\nfew large kernels, instead of stacking multiple smaller ones, can be a superior\ndesign strategy. Our work introduces a set of architecture design guidelines\nfor large-kernel ConvNets that optimize their efficiency and performance. We\npropose the UniRepLKNet architecture, which offers systematical architecture\ndesign principles specifically crafted for large-kernel ConvNets, emphasizing\ntheir unique ability to capture extensive spatial information without deep\nlayer stacking. This results in a model that not only surpasses its\npredecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a\nCOCO box AP of 56.4% but also demonstrates impressive scalability and\nperformance on various modalities such as time-series forecasting, audio, point\ncloud, and video recognition. These results indicate the universal modeling\nabilities of large-kernel ConvNets with faster inference speed compared with\nvision transformers. Our findings reveal that large-kernel ConvNets possess\nlarger effective receptive fields and a higher shape bias, moving away from the\ntexture bias typical of smaller-kernel CNNs. All codes and models are publicly\navailable at https://github.com/AILab-CVC/UniRepLKNet promoting further\nresearch and development in the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes the paradigm of large convolutional kernels in designing\nmodern Convolutional Neural Networks (ConvNets). We establish that employing a\nfew large kernels, instead of stacking multiple smaller ones, can be a superior\ndesign strategy. Our work introduces a set of architecture design guidelines\nfor large-kernel ConvNets that optimize their efficiency and performance. We\npropose the UniRepLKNet architecture, which offers systematical architecture\ndesign principles specifically crafted for large-kernel ConvNets, emphasizing\ntheir unique ability to capture extensive spatial information without deep\nlayer stacking. This results in a model that not only surpasses its\npredecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a\nCOCO box AP of 56.4% but also demonstrates impressive scalability and\nperformance on various modalities such as time-series forecasting, audio, point\ncloud, and video recognition. These results indicate the universal modeling\nabilities of large-kernel ConvNets with faster inference speed compared with\nvision transformers. Our findings reveal that large-kernel ConvNets possess\nlarger effective receptive fields and a higher shape bias, moving away from the\ntexture bias typical of smaller-kernel CNNs. All codes and models are publicly\navailable at https://github.com/AILab-CVC/UniRepLKNet promoting further\nresearch and development in the community."
                },
                "authors": [
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Xiaohan Ding"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "This is the journal version of arXiv:2203.06717 and arXiv:2311.15599",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17670v3",
                "updated": "2024-10-10T15:43:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    13,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-27T21:48:07Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    48,
                    7,
                    0,
                    148,
                    0
                ],
                "title": "Deployment of Large Language Models to Control Mobile Robots at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of Large Language Models to Control Mobile Robots at the Edge"
                },
                "summary": "This paper investigates the possibility of intuitive human-robot interaction\nthrough the application of Natural Language Processing (NLP) and Large Language\nModels (LLMs) in mobile robotics. This work aims to explore the feasibility of\nusing these technologies for edge-based deployment, where traditional cloud\ndependencies are eliminated. The study specifically contrasts the performance\nof GPT-4-Turbo, which requires cloud connectivity, with an offline-capable,\nquantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). These results show that\nGPT-4-Turbo delivers superior performance in interpreting and executing complex\ncommands accurately, whereas LLaMA 2 exhibits significant limitations in\nconsistency and reliability of command execution. Communication between the\ncontrol computer and the mobile robot is established via a Raspberry Pi Pico W,\nwhich wirelessly receives commands from the computer without internet\ndependency and transmits them through a wired connection to the robot's Arduino\ncontroller. This study highlights the potential and challenges of implementing\nLLMs and NLP at the edge, providing groundwork for future research into fully\nautonomous and network-independent robotic systems. For video demonstrations\nand source code, please refer to: https://tinyurl.com/MobileRobotGPT4LLaMA2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the possibility of intuitive human-robot interaction\nthrough the application of Natural Language Processing (NLP) and Large Language\nModels (LLMs) in mobile robotics. This work aims to explore the feasibility of\nusing these technologies for edge-based deployment, where traditional cloud\ndependencies are eliminated. The study specifically contrasts the performance\nof GPT-4-Turbo, which requires cloud connectivity, with an offline-capable,\nquantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). These results show that\nGPT-4-Turbo delivers superior performance in interpreting and executing complex\ncommands accurately, whereas LLaMA 2 exhibits significant limitations in\nconsistency and reliability of command execution. Communication between the\ncontrol computer and the mobile robot is established via a Raspberry Pi Pico W,\nwhich wirelessly receives commands from the computer without internet\ndependency and transmits them through a wired connection to the robot's Arduino\ncontroller. This study highlights the potential and challenges of implementing\nLLMs and NLP at the edge, providing groundwork for future research into fully\nautonomous and network-independent robotic systems. For video demonstrations\nand source code, please refer to: https://tinyurl.com/MobileRobotGPT4LLaMA2024."
                },
                "authors": [
                    {
                        "name": "Pascal Sikorski"
                    },
                    {
                        "name": "Leendert Schrader"
                    },
                    {
                        "name": "Kaleb Yu"
                    },
                    {
                        "name": "Lucy Billadeau"
                    },
                    {
                        "name": "Jinka Meenakshi"
                    },
                    {
                        "name": "Naveena Mutharasan"
                    },
                    {
                        "name": "Flavio Esposito"
                    },
                    {
                        "name": "Hadi AliAkbarpour"
                    },
                    {
                        "name": "Madi Babaiasl"
                    }
                ],
                "author_detail": {
                    "name": "Madi Babaiasl"
                },
                "author": "Madi Babaiasl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08047v1",
                "updated": "2024-10-10T15:42:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    42,
                    39,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:42:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    42,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning"
                },
                "summary": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Gyeongman Kim"
                    },
                    {
                        "name": "Hyemin S. Lee"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19091v2",
                "updated": "2024-10-10T15:29:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    29,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-27T18:41:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    18,
                    41,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "System-Level Defense against Indirect Prompt Injection Attacks: An\n  Information Flow Control Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-Level Defense against Indirect Prompt Injection Attacks: An\n  Information Flow Control Perspective"
                },
                "summary": "Large Language Model-based systems (LLM systems) are information and query\nprocessing systems that use LLMs to plan operations from natural-language\nprompts and feed the output of each successive step into the LLM to plan the\nnext. This structure results in powerful tools that can process complex\ninformation from diverse sources but raises critical security concerns.\nMalicious information from any source may be processed by the LLM and can\ncompromise the query processing, resulting in nearly arbitrary misbehavior. To\ntackle this problem, we present a system-level defense based on the principles\nof information flow control that we call an f-secure LLM system. An f-secure\nLLM system disaggregates the components of an LLM system into a context-aware\npipeline with dynamically generated structured executable plans, and a security\nmonitor filters out untrusted input into the planning process. This structure\nprevents compromise while maximizing flexibility. We provide formal models for\nboth existing LLM systems and our f-secure LLM system, allowing analysis of\ncritical security guarantees. We further evaluate case studies and benchmarks\nshowing that f-secure LLM systems provide robust security while preserving\nfunctionality and efficiency. Our code is released at\nhttps://github.com/fzwark/Secure_LLM_System.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based systems (LLM systems) are information and query\nprocessing systems that use LLMs to plan operations from natural-language\nprompts and feed the output of each successive step into the LLM to plan the\nnext. This structure results in powerful tools that can process complex\ninformation from diverse sources but raises critical security concerns.\nMalicious information from any source may be processed by the LLM and can\ncompromise the query processing, resulting in nearly arbitrary misbehavior. To\ntackle this problem, we present a system-level defense based on the principles\nof information flow control that we call an f-secure LLM system. An f-secure\nLLM system disaggregates the components of an LLM system into a context-aware\npipeline with dynamically generated structured executable plans, and a security\nmonitor filters out untrusted input into the planning process. This structure\nprevents compromise while maximizing flexibility. We provide formal models for\nboth existing LLM systems and our f-secure LLM system, allowing analysis of\ncritical security guarantees. We further evaluate case studies and benchmarks\nshowing that f-secure LLM systems provide robust security while preserving\nfunctionality and efficiency. Our code is released at\nhttps://github.com/fzwark/Secure_LLM_System."
                },
                "authors": [
                    {
                        "name": "Fangzhou Wu"
                    },
                    {
                        "name": "Ethan Cecchetti"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13873v2",
                "updated": "2024-10-10T15:27:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    27,
                    41,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-22T17:56:53Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    56,
                    53,
                    2,
                    143,
                    0
                ],
                "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering"
                },
                "summary": "Large language models are often challenged by generating erroneous or\n`hallucinated' responses, especially in complex reasoning tasks. To mitigate\nthis, we propose a retrieval augmented reasoning method, FiDeLiS, which\nenhances knowledge graph question answering by anchoring responses to\nstructured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced\nretrieval mechanism that fetches relevant entities and relations from a\nvector-based index of KGs to ensure high-recall retrieval. Once these entities\nand relations are retrieved, our method constructs candidate reasoning paths\nwhich are then refined using a stepwise beam search. This ensures that all the\npaths we create can be confidently linked back to KGs, ensuring they are\naccurate and reliable. A distinctive feature of our approach is its blend of\nnatural language planning with beam search to optimize the selection of\nreasoning paths. Moreover, we redesign the way reasoning paths are scored by\ntransforming this process into a deductive reasoning task, allowing the LLM to\nassess the validity of the paths through deductive reasoning rather than\ntraditional logit-based scoring. This helps avoid misleading reasoning chains\nand reduces unnecessary computational demand. Extensive experiments demonstrate\nthat our method, even as a training-free method which has lower computational\ncosts and superior generality, outperforms established strong baselines across\nthree datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often challenged by generating erroneous or\n`hallucinated' responses, especially in complex reasoning tasks. To mitigate\nthis, we propose a retrieval augmented reasoning method, FiDeLiS, which\nenhances knowledge graph question answering by anchoring responses to\nstructured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced\nretrieval mechanism that fetches relevant entities and relations from a\nvector-based index of KGs to ensure high-recall retrieval. Once these entities\nand relations are retrieved, our method constructs candidate reasoning paths\nwhich are then refined using a stepwise beam search. This ensures that all the\npaths we create can be confidently linked back to KGs, ensuring they are\naccurate and reliable. A distinctive feature of our approach is its blend of\nnatural language planning with beam search to optimize the selection of\nreasoning paths. Moreover, we redesign the way reasoning paths are scored by\ntransforming this process into a deductive reasoning task, allowing the LLM to\nassess the validity of the paths through deductive reasoning rather than\ntraditional logit-based scoring. This helps avoid misleading reasoning chains\nand reduces unnecessary computational demand. Extensive experiments demonstrate\nthat our method, even as a training-free method which has lower computational\ncosts and superior generality, outperforms established strong baselines across\nthree datasets."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Nian Liu"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08020v1",
                "updated": "2024-10-10T15:17:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs"
                },
                "summary": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval."
                },
                "authors": [
                    {
                        "name": "Jonas Hbotter"
                    },
                    {
                        "name": "Sascha Bongni"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08014v1",
                "updated": "2024-10-10T15:09:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    9,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:09:52Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    9,
                    52,
                    3,
                    284,
                    0
                ],
                "title": "LLM Cascade with Multi-Objective Optimal Consideration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cascade with Multi-Objective Optimal Consideration"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generating natural language. However, their high deployment\ncosts often pose a barrier to practical applications, especially. Cascading\nlocal and server models offers a promising solution to this challenge. While\nexisting studies on LLM cascades have primarily focused on the performance-cost\ntrade-off, real-world scenarios often involve more complex requirements. This\npaper introduces a novel LLM Cascade strategy with Multi-Objective\nOptimization, enabling LLM cascades to consider additional objectives (e.g.,\nprivacy) and better align with the specific demands of real-world applications\nwhile maintaining their original cascading abilities. Extensive experiments on\nthree benchmarks validate the effectiveness and superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generating natural language. However, their high deployment\ncosts often pose a barrier to practical applications, especially. Cascading\nlocal and server models offers a promising solution to this challenge. While\nexisting studies on LLM cascades have primarily focused on the performance-cost\ntrade-off, real-world scenarios often involve more complex requirements. This\npaper introduces a novel LLM Cascade strategy with Multi-Objective\nOptimization, enabling LLM cascades to consider additional objectives (e.g.,\nprivacy) and better align with the specific demands of real-world applications\nwhile maintaining their original cascading abilities. Extensive experiments on\nthree benchmarks validate the effectiveness and superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Liqian Peng"
                    },
                    {
                        "name": "Congchao Wang"
                    },
                    {
                        "name": "Alec Go"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02018v2",
                "updated": "2024-10-10T15:07:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    7,
                    42,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-02T15:08:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    15,
                    8,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "Large Language Models for Orchestrating Bimanual Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Orchestrating Bimanual Robots"
                },
                "summary": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io."
                },
                "authors": [
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Accepted in Humanoids 2024. The project website can be found at\n  http://labor-agent.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09039v3",
                "updated": "2024-10-10T15:06:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    6,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2023-12-14T15:37:04Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    15,
                    37,
                    4,
                    3,
                    348,
                    0
                ],
                "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning"
                },
                "summary": "Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08001v1",
                "updated": "2024-10-10T14:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation"
                },
                "summary": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/"
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "Project page: https://opendrivelab.com/RoboDual/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07997v1",
                "updated": "2024-10-10T14:53:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    53,
                    39,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:53:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    53,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "APOLLO: A GPT-based tool to detect phishing emails and generate\n  explanations that warn users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: A GPT-based tool to detect phishing emails and generate\n  explanations that warn users"
                },
                "summary": "Phishing is one of the most prolific cybercriminal activities, with attacks\nbecoming increasingly sophisticated. It is, therefore, imperative to explore\nnovel technologies to improve user protection across both technical and human\ndimensions. Large Language Models (LLMs) offer significant promise for text\nprocessing in various domains, but their use for defense against phishing\nattacks still remains scarcely explored. In this paper, we present APOLLO, a\ntool based on OpenAI's GPT-4o to detect phishing emails and generate\nexplanation messages to users about why a specific email is dangerous, thus\nimproving their decision-making capabilities. We have evaluated the performance\nof APOLLO in classifying phishing emails; the results show that the LLM models\nhave exemplary capabilities in classifying phishing emails (97 percent accuracy\nin the case of GPT-4o) and that this performance can be further improved by\nintegrating data from third-party services, resulting in a near-perfect\nclassification rate (99 percent accuracy). To assess the perception of the\nexplanations generated by this tool, we also conducted a study with 20\nparticipants, comparing four different explanations presented as phishing\nwarnings. We compared the LLM-generated explanations to four baselines: a\nmanually crafted warning, and warnings from Chrome, Firefox, and Edge browsers.\nThe results show that not only the LLM-generated explanations were perceived as\nhigh quality, but also that they can be more understandable, interesting, and\ntrustworthy than the baselines. These findings suggest that using LLMs as a\ndefense against phishing is a very promising approach, with APOLLO representing\na proof of concept in this research direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is one of the most prolific cybercriminal activities, with attacks\nbecoming increasingly sophisticated. It is, therefore, imperative to explore\nnovel technologies to improve user protection across both technical and human\ndimensions. Large Language Models (LLMs) offer significant promise for text\nprocessing in various domains, but their use for defense against phishing\nattacks still remains scarcely explored. In this paper, we present APOLLO, a\ntool based on OpenAI's GPT-4o to detect phishing emails and generate\nexplanation messages to users about why a specific email is dangerous, thus\nimproving their decision-making capabilities. We have evaluated the performance\nof APOLLO in classifying phishing emails; the results show that the LLM models\nhave exemplary capabilities in classifying phishing emails (97 percent accuracy\nin the case of GPT-4o) and that this performance can be further improved by\nintegrating data from third-party services, resulting in a near-perfect\nclassification rate (99 percent accuracy). To assess the perception of the\nexplanations generated by this tool, we also conducted a study with 20\nparticipants, comparing four different explanations presented as phishing\nwarnings. We compared the LLM-generated explanations to four baselines: a\nmanually crafted warning, and warnings from Chrome, Firefox, and Edge browsers.\nThe results show that not only the LLM-generated explanations were perceived as\nhigh quality, but also that they can be more understandable, interesting, and\ntrustworthy than the baselines. These findings suggest that using LLMs as a\ndefense against phishing is a very promising approach, with APOLLO representing\na proof of concept in this research direction."
                },
                "authors": [
                    {
                        "name": "Giuseppe Desolda"
                    },
                    {
                        "name": "Francesco Greco"
                    },
                    {
                        "name": "Luca Vigan"
                    }
                ],
                "author_detail": {
                    "name": "Luca Vigan"
                },
                "author": "Luca Vigan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00269v2",
                "updated": "2024-10-10T14:51:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    51,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-08-30T21:54:13Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    21,
                    54,
                    13,
                    4,
                    243,
                    0
                ],
                "title": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human\n  and GPT-4 Written Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human\n  and GPT-4 Written Content"
                },
                "summary": "Cosine similarity between two documents can be computed using token\nembeddings formed by Large Language Models (LLMs) such as GPT-4, and used to\ncategorize those documents across a range of uses. However, these similarities\nare ultimately dependent on the corpora used to train these LLMs, and may not\nreflect subjective similarity of individuals or how their biases and\nconstraints impact similarity metrics. This lack of cognitively-aware\npersonalization of similarity metrics can be particularly problematic in\neducational and recommendation settings where there is a limited number of\nindividual judgements of category or preference, and biases can be particularly\nrelevant. To address this, we rely on an integration of an Instance-Based\nLearning (IBL) cognitive model with LLM embeddings to develop the\nInstance-Based Individualized Similarity (IBIS) metric. This similarity metric\nis beneficial in that it takes into account individual biases and constraints\nin a manner that is grounded in the cognitive mechanisms of decision making. To\nevaluate the IBIS metric, we also introduce a dataset of human categorizations\nof emails as being either dangerous (phishing) or safe (ham). This dataset is\nused to demonstrate the benefits of leveraging a cognitive model to measure the\nsubjective similarity of human participants in an educational setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosine similarity between two documents can be computed using token\nembeddings formed by Large Language Models (LLMs) such as GPT-4, and used to\ncategorize those documents across a range of uses. However, these similarities\nare ultimately dependent on the corpora used to train these LLMs, and may not\nreflect subjective similarity of individuals or how their biases and\nconstraints impact similarity metrics. This lack of cognitively-aware\npersonalization of similarity metrics can be particularly problematic in\neducational and recommendation settings where there is a limited number of\nindividual judgements of category or preference, and biases can be particularly\nrelevant. To address this, we rely on an integration of an Instance-Based\nLearning (IBL) cognitive model with LLM embeddings to develop the\nInstance-Based Individualized Similarity (IBIS) metric. This similarity metric\nis beneficial in that it takes into account individual biases and constraints\nin a manner that is grounded in the cognitive mechanisms of decision making. To\nevaluate the IBIS metric, we also introduce a dataset of human categorizations\nof emails as being either dangerous (phishing) or safe (ham). This dataset is\nused to demonstrate the benefits of leveraging a cognitive model to measure the\nsubjective similarity of human participants in an educational setting."
                },
                "authors": [
                    {
                        "name": "Tyler Malloy"
                    },
                    {
                        "name": "Maria Jos Ferreira"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Cleotilde Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Cleotilde Gonzalez"
                },
                "author": "Cleotilde Gonzalez",
                "arxiv_comment": "7 Figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06423v2",
                "updated": "2024-10-10T14:50:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    50,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-08T22:06:09Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    22,
                    6,
                    9,
                    0,
                    190,
                    0
                ],
                "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation"
                },
                "summary": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics."
                },
                "authors": [
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Juan Rodriguez"
                    },
                    {
                        "name": "Amirhossein Abaskohi"
                    },
                    {
                        "name": "Mohammad Chegini"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Perouz Taslakian"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Sai Rajeswar Mudumba"
                    },
                    {
                        "name": "Issam Hadj Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam Hadj Laradji"
                },
                "author": "Issam Hadj Laradji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v1",
                "updated": "2024-10-10T14:48:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10719v2",
                "updated": "2024-10-10T14:47:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    47,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-16T20:47:00Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    20,
                    47,
                    0,
                    0,
                    260,
                    0
                ],
                "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images"
                },
                "summary": "Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available."
                },
                "authors": [
                    {
                        "name": "Sina Malakouti"
                    },
                    {
                        "name": "Aysan Aghazadeh"
                    },
                    {
                        "name": "Ashmit Khandelwal"
                    },
                    {
                        "name": "Adriana Kovashka"
                    }
                ],
                "author_detail": {
                    "name": "Adriana Kovashka"
                },
                "author": "Adriana Kovashka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07985v1",
                "updated": "2024-10-10T14:39:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    39,
                    33,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:39:33Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    39,
                    33,
                    3,
                    284,
                    0
                ],
                "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54% and 52.55%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54% and 52.55%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "26 Pages, 17 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06680v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06680v4",
                "updated": "2024-10-10T14:38:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    38,
                    37,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-05T16:35:30Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    16,
                    35,
                    30,
                    6,
                    126,
                    0
                ],
                "title": "Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning"
                },
                "summary": "Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06680v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06680v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18314v2",
                "updated": "2024-10-10T14:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    31,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-28T16:07:17Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    16,
                    7,
                    17,
                    1,
                    149,
                    0
                ],
                "title": "Deriving Causal Order from Single-Variable Interventions: Guarantees &\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving Causal Order from Single-Variable Interventions: Guarantees &\n  Algorithm"
                },
                "summary": "Targeted and uniform interventions to a system are crucial for unveiling\ncausal relationships. While several methods have been developed to leverage\ninterventional data for causal structure learning, their practical application\nin real-world scenarios often remains challenging. Recent benchmark studies\nhave highlighted these difficulties, even when large numbers of single-variable\nintervention samples are available. In this work, we demonstrate, both\ntheoretically and empirically, that such datasets contain a wealth of causal\ninformation that can be effectively extracted under realistic assumptions about\nthe data distribution. More specifically, we introduce the notion of\ninterventional faithfulness, which relies on comparisons between the marginal\ndistributions of each variable across observational and interventional\nsettings, and we introduce a score on causal orders. Under this assumption, we\nare able to prove strong theoretical guarantees on the optimum of our score\nthat also hold for large-scale settings. To empirically verify our theory, we\nintroduce Intersort, an algorithm designed to infer the causal order from\ndatasets containing large numbers of single-variable interventions by\napproximately optimizing our score. Intersort outperforms baselines (GIES,\nDCDI, PC and EASE) on almost all simulated data settings replicating common\nbenchmarks in the field. Our proposed novel approach to modeling interventional\ndatasets thus offers a promising avenue for advancing causal inference,\nhighlighting significant potential for further enhancements under realistic\nassumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted and uniform interventions to a system are crucial for unveiling\ncausal relationships. While several methods have been developed to leverage\ninterventional data for causal structure learning, their practical application\nin real-world scenarios often remains challenging. Recent benchmark studies\nhave highlighted these difficulties, even when large numbers of single-variable\nintervention samples are available. In this work, we demonstrate, both\ntheoretically and empirically, that such datasets contain a wealth of causal\ninformation that can be effectively extracted under realistic assumptions about\nthe data distribution. More specifically, we introduce the notion of\ninterventional faithfulness, which relies on comparisons between the marginal\ndistributions of each variable across observational and interventional\nsettings, and we introduce a score on causal orders. Under this assumption, we\nare able to prove strong theoretical guarantees on the optimum of our score\nthat also hold for large-scale settings. To empirically verify our theory, we\nintroduce Intersort, an algorithm designed to infer the causal order from\ndatasets containing large numbers of single-variable interventions by\napproximately optimizing our score. Intersort outperforms baselines (GIES,\nDCDI, PC and EASE) on almost all simulated data settings replicating common\nbenchmarks in the field. Our proposed novel approach to modeling interventional\ndatasets thus offers a promising avenue for advancing causal inference,\nhighlighting significant potential for further enhancements under realistic\nassumptions."
                },
                "authors": [
                    {
                        "name": "Mathieu Chevalley"
                    },
                    {
                        "name": "Patrick Schwab"
                    },
                    {
                        "name": "Arash Mehrjou"
                    }
                ],
                "author_detail": {
                    "name": "Arash Mehrjou"
                },
                "author": "Arash Mehrjou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07970v1",
                "updated": "2024-10-10T14:28:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    28,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:28:43Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    28,
                    43,
                    3,
                    284,
                    0
                ],
                "title": "Mapping Hong Kong's Financial Ecosystem: A Network Analysis of the SFC's\n  Licensed Professionals and Institutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Hong Kong's Financial Ecosystem: A Network Analysis of the SFC's\n  Licensed Professionals and Institutions"
                },
                "summary": "We present the first study of the Public Register of Licensed Persons and\nRegistered Institutions maintained by the Hong Kong Securities and Futures\nCommission (SFC) through the lens of complex network analysis. This dataset,\nspanning 21 years with daily granularity, provides a unique view of the\nevolving social network between licensed professionals and their affiliated\nfirms in Hong Kong's financial sector. Leveraging large language models, we\nclassify firms (e.g., asset managers, banks) and infer the likely nationality\nand gender of employees based on their names. This application enhances the\ndataset by adding rich demographic and organizational context, enabling more\nprecise network analysis. Our preliminary findings reveal key structural\nfeatures, offering new insights into the dynamics of Hong Kong's financial\nlandscape. We release the structured dataset to enable further research,\nestablishing a foundation for future studies that may inform recruitment\nstrategies, policy-making, and risk management in the financial industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first study of the Public Register of Licensed Persons and\nRegistered Institutions maintained by the Hong Kong Securities and Futures\nCommission (SFC) through the lens of complex network analysis. This dataset,\nspanning 21 years with daily granularity, provides a unique view of the\nevolving social network between licensed professionals and their affiliated\nfirms in Hong Kong's financial sector. Leveraging large language models, we\nclassify firms (e.g., asset managers, banks) and infer the likely nationality\nand gender of employees based on their names. This application enhances the\ndataset by adding rich demographic and organizational context, enabling more\nprecise network analysis. Our preliminary findings reveal key structural\nfeatures, offering new insights into the dynamics of Hong Kong's financial\nlandscape. We release the structured dataset to enable further research,\nestablishing a foundation for future studies that may inform recruitment\nstrategies, policy-making, and risk management in the financial industry."
                },
                "authors": [
                    {
                        "name": "Abdulla AlKetbi"
                    },
                    {
                        "name": "Gautier Marti"
                    },
                    {
                        "name": "Khaled AlNuaimi"
                    },
                    {
                        "name": "Raed Jaradat"
                    },
                    {
                        "name": "Andreas Henschel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Henschel"
                },
                "author": "Andreas Henschel",
                "arxiv_comment": "Complex Networks 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07962v1",
                "updated": "2024-10-10T14:24:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:24:43Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    43,
                    3,
                    284,
                    0
                ],
                "title": "Towards Assurance of LLM Adversarial Robustness using Ontology-Driven\n  Argumentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Assurance of LLM Adversarial Robustness using Ontology-Driven\n  Argumentation"
                },
                "summary": "Despite the impressive adaptability of large language models (LLMs),\nchallenges remain in ensuring their security, transparency, and\ninterpretability. Given their susceptibility to adversarial attacks, LLMs need\nto be defended with an evolving combination of adversarial training and\nguardrails. However, managing the implicit and heterogeneous knowledge for\ncontinuously assuring robustness is difficult. We introduce a novel approach\nfor assurance of the adversarial robustness of LLMs based on formal\nargumentation. Using ontologies for formalization, we structure\nstate-of-the-art attacks and defenses, facilitating the creation of a\nhuman-readable assurance case, and a machine-readable representation. We\ndemonstrate its application with examples in English language and code\ntranslation tasks, and provide implications for theory and practice, by\ntargeting engineers, data scientists, users, and auditors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive adaptability of large language models (LLMs),\nchallenges remain in ensuring their security, transparency, and\ninterpretability. Given their susceptibility to adversarial attacks, LLMs need\nto be defended with an evolving combination of adversarial training and\nguardrails. However, managing the implicit and heterogeneous knowledge for\ncontinuously assuring robustness is difficult. We introduce a novel approach\nfor assurance of the adversarial robustness of LLMs based on formal\nargumentation. Using ontologies for formalization, we structure\nstate-of-the-art attacks and defenses, facilitating the creation of a\nhuman-readable assurance case, and a machine-readable representation. We\ndemonstrate its application with examples in English language and code\ntranslation tasks, and provide implications for theory and practice, by\ntargeting engineers, data scientists, users, and auditors."
                },
                "authors": [
                    {
                        "name": "Tomas Bueno Momcilovic"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Dian Balta"
                    }
                ],
                "author_detail": {
                    "name": "Dian Balta"
                },
                "author": "Dian Balta",
                "arxiv_comment": "To be published in xAI 2024, late-breaking track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07961v1",
                "updated": "2024-10-10T14:24:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "title": "QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm\n  Design"
                },
                "summary": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yuntian Gu"
                    },
                    {
                        "name": "Ziruo Wang"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Tongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongyang Li"
                },
                "author": "Tongyang Li",
                "arxiv_comment": "35 pages, 7 figures, 4 tables, GitHub repository:\n  https://github.com/EstelYang/QCircuitNet_Dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07959v1",
                "updated": "2024-10-10T14:23:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:23:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act"
                },
                "summary": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice."
                },
                "authors": [
                    {
                        "name": "Philipp Guldimann"
                    },
                    {
                        "name": "Alexander Spiridonov"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Velko Vechev"
                    },
                    {
                        "name": "Anna Gueorguieva"
                    },
                    {
                        "name": "Mislav Balunovi"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    },
                    {
                        "name": "Pavol Bielik"
                    },
                    {
                        "name": "Petar Tsankov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16015v2",
                "updated": "2024-10-10T14:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    19,
                    57,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-24T17:48:38Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    17,
                    48,
                    38,
                    2,
                    115,
                    0
                ],
                "title": "Neural Operators Learn the Local Physics of Magnetohydrodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Operators Learn the Local Physics of Magnetohydrodynamics"
                },
                "summary": "Magnetohydrodynamics (MHD) plays a pivotal role in describing the dynamics of\nplasma and conductive fluids, essential for understanding phenomena such as the\nstructure and evolution of stars and galaxies, and in nuclear fusion for plasma\nmotion through ideal MHD equations. Solving these hyperbolic PDEs requires\nsophisticated numerical methods, presenting computational challenges due to\ncomplex structures and high costs. Recent advances introduce neural operators\nlike the Fourier Neural Operator (FNO) as surrogate models for traditional\nnumerical analyses. This study explores a modified Flux Fourier neural operator\nmodel to approximate the numerical flux of ideal MHD, offering a novel approach\nthat outperforms existing neural operator models by enabling continuous\ninference, generalization outside sampled distributions, and faster computation\ncompared to classical numerical schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetohydrodynamics (MHD) plays a pivotal role in describing the dynamics of\nplasma and conductive fluids, essential for understanding phenomena such as the\nstructure and evolution of stars and galaxies, and in nuclear fusion for plasma\nmotion through ideal MHD equations. Solving these hyperbolic PDEs requires\nsophisticated numerical methods, presenting computational challenges due to\ncomplex structures and high costs. Recent advances introduce neural operators\nlike the Fourier Neural Operator (FNO) as surrogate models for traditional\nnumerical analyses. This study explores a modified Flux Fourier neural operator\nmodel to approximate the numerical flux of ideal MHD, offering a novel approach\nthat outperforms existing neural operator models by enabling continuous\ninference, generalization outside sampled distributions, and faster computation\ncompared to classical numerical schemes."
                },
                "authors": [
                    {
                        "name": "Taeyoung Kim"
                    },
                    {
                        "name": "Youngsoo Ha"
                    },
                    {
                        "name": "Myungjoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Myungjoo Kang"
                },
                "author": "Myungjoo Kang",
                "arxiv_comment": "48 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07951v1",
                "updated": "2024-10-10T14:18:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    18,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:18:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    18,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Disease Entity Recognition and Normalization is Improved with Large\n  Language Model Derived Synthetic Normalized Mentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease Entity Recognition and Normalization is Improved with Large\n  Language Model Derived Synthetic Normalized Mentions"
                },
                "summary": "Background: Machine learning methods for clinical named entity recognition\nand entity normalization systems can utilize both labeled corpora and Knowledge\nGraphs (KGs) for learning. However, infrequently occurring concepts may have\nfew mentions in training corpora and lack detailed descriptions or synonyms,\neven in large KGs. For Disease Entity Recognition (DER) and Disease Entity\nNormalization (DEN), this can result in fewer high quality training examples\nrelative to the number of known diseases. Large Language Model (LLM) generation\nof synthetic training examples could improve performance in these information\nextraction tasks.\n  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus\ncontaining normalized mentions of concepts from the Unified Medical Language\nSystem (UMLS) Disease Semantic Group. We measured overall and Out of\nDistribution (OOD) performance for DER and DEN, with and without synthetic data\naugmentation. We evaluated performance on 3 different disease corpora using 4\ndifferent data augmentation strategies, assessed using BioBERT for DER and\nSapBERT and KrissBERT for DEN.\n  Results: Our synthetic data yielded a substantial improvement for DEN, in all\n3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by\n3-9 points in overall performance and by 20-55 points in OOD data. A small\nimprovement (1-2 points) was also seen for DER in overall performance, but only\none dataset showed OOD improvement.\n  Conclusion: LLM generation of normalized disease mentions can improve DEN\nrelative to normalization approaches that do not utilize LLMs to augment data\nwith synthetic mentions. Ablation studies indicate that performance gains for\nDEN were only partially attributable to improvements in OOD performance. The\nsame approach has only a limited ability to improve DER. We make our software\nand dataset publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Machine learning methods for clinical named entity recognition\nand entity normalization systems can utilize both labeled corpora and Knowledge\nGraphs (KGs) for learning. However, infrequently occurring concepts may have\nfew mentions in training corpora and lack detailed descriptions or synonyms,\neven in large KGs. For Disease Entity Recognition (DER) and Disease Entity\nNormalization (DEN), this can result in fewer high quality training examples\nrelative to the number of known diseases. Large Language Model (LLM) generation\nof synthetic training examples could improve performance in these information\nextraction tasks.\n  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus\ncontaining normalized mentions of concepts from the Unified Medical Language\nSystem (UMLS) Disease Semantic Group. We measured overall and Out of\nDistribution (OOD) performance for DER and DEN, with and without synthetic data\naugmentation. We evaluated performance on 3 different disease corpora using 4\ndifferent data augmentation strategies, assessed using BioBERT for DER and\nSapBERT and KrissBERT for DEN.\n  Results: Our synthetic data yielded a substantial improvement for DEN, in all\n3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by\n3-9 points in overall performance and by 20-55 points in OOD data. A small\nimprovement (1-2 points) was also seen for DER in overall performance, but only\none dataset showed OOD improvement.\n  Conclusion: LLM generation of normalized disease mentions can improve DEN\nrelative to normalization approaches that do not utilize LLMs to augment data\nwith synthetic mentions. Ablation studies indicate that performance gains for\nDEN were only partially attributable to improvements in OOD performance. The\nsame approach has only a limited ability to improve DER. We make our software\nand dataset publicly available."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Shinjitha Vadlakonda"
                    },
                    {
                        "name": "Richard E. Kennedy"
                    },
                    {
                        "name": "John D. Osborne"
                    }
                ],
                "author_detail": {
                    "name": "John D. Osborne"
                },
                "author": "John D. Osborne",
                "arxiv_comment": "21 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09265v2",
                "updated": "2024-10-10T14:17:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    17,
                    20,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-13T16:04:11Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    4,
                    11,
                    3,
                    165,
                    0
                ],
                "title": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs"
                },
                "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing (NLP), and recent studies have aimed to understand their\nunderlying mechanisms. However, most of this research is conducted within a\nmonolingual setting, primarily focusing on English. Few studies attempt to\nexplore the internal workings of LLMs in multilingual settings. In this study,\nwe aim to fill the research gap by examining how neuron activation is shared\nacross tasks and languages. We classify neurons into four distinct categories\nbased on their responses to a specific input across different\nlanguages:all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the patterns of neuron sharing are significantly affected by the\ncharacteristics of tasks and examples; (ii) neuron sharing does not fully\ncorrespond with language similarity; (iii) shared neurons play a vital role in\ngenerating responses, especially those shared across all languages. These\nfindings shed light on the internal workings of multilingual LLMs and pave the\nway to the future research. We will release the code to foster research in this\narea.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing (NLP), and recent studies have aimed to understand their\nunderlying mechanisms. However, most of this research is conducted within a\nmonolingual setting, primarily focusing on English. Few studies attempt to\nexplore the internal workings of LLMs in multilingual settings. In this study,\nwe aim to fill the research gap by examining how neuron activation is shared\nacross tasks and languages. We classify neurons into four distinct categories\nbased on their responses to a specific input across different\nlanguages:all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the patterns of neuron sharing are significantly affected by the\ncharacteristics of tasks and examples; (ii) neuron sharing does not fully\ncorrespond with language similarity; (iii) shared neurons play a vital role in\ngenerating responses, especially those shared across all languages. These\nfindings shed light on the internal workings of multilingual LLMs and pave the\nway to the future research. We will release the code to foster research in this\narea."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16562v3",
                "updated": "2024-10-10T14:04:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    4,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-24T11:56:15Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    11,
                    56,
                    15,
                    0,
                    176,
                    0
                ],
                "title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models"
                },
                "summary": "The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment."
                },
                "authors": [
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "Project page: https://sais-fuxi.github.io/projects/evalalign/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07934v1",
                "updated": "2024-10-10T14:01:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    1,
                    5,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:01:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    1,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "panelPomp: Analysis of Panel Data via Partially Observed Markov\n  Processes in R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "panelPomp: Analysis of Panel Data via Partially Observed Markov\n  Processes in R"
                },
                "summary": "Panel data arise when time series measurements are collected from multiple,\ndynamically independent but structurally related systems. In such cases, each\nsystem's time series can be modeled as a partially observed Markov process\n(POMP), and the ensemble of these models is called a PanelPOMP. If the time\nseries are relatively short, statistical inference for each time series must\ndraw information from across the entire panel. Every time series has a name,\ncalled its unit label, which may correspond to an object on which that time\nseries was collected. Differences between units may be of direct inferential\ninterest or may be a nuisance for studying the commonalities. The R package\npanelPomp supports analysis of panel data via a general class of PanelPOMP\nmodels. This includes a suite of tools for manipulation of models and data that\ntake advantage of the panel structure. The panelPomp package currently\nemphasizes recent advances enabling likelihood-based inference via\nsimulation-based algorithms. However, the general framework provided by\npanelPomp supports development of additional, new inference methodology for\npanel data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panel data arise when time series measurements are collected from multiple,\ndynamically independent but structurally related systems. In such cases, each\nsystem's time series can be modeled as a partially observed Markov process\n(POMP), and the ensemble of these models is called a PanelPOMP. If the time\nseries are relatively short, statistical inference for each time series must\ndraw information from across the entire panel. Every time series has a name,\ncalled its unit label, which may correspond to an object on which that time\nseries was collected. Differences between units may be of direct inferential\ninterest or may be a nuisance for studying the commonalities. The R package\npanelPomp supports analysis of panel data via a general class of PanelPOMP\nmodels. This includes a suite of tools for manipulation of models and data that\ntake advantage of the panel structure. The panelPomp package currently\nemphasizes recent advances enabling likelihood-based inference via\nsimulation-based algorithms. However, the general framework provided by\npanelPomp supports development of additional, new inference methodology for\npanel data."
                },
                "authors": [
                    {
                        "name": "Carles Bret"
                    },
                    {
                        "name": "Jesse Wheeler"
                    },
                    {
                        "name": "Aaron A. King"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07930v1",
                "updated": "2024-10-10T13:57:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    57,
                    27,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:57:27Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    57,
                    27,
                    3,
                    284,
                    0
                ],
                "title": "Cost-aware Simulation-based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-aware Simulation-based Inference"
                },
                "summary": "Simulation-based inference (SBI) is the preferred framework for estimating\nparameters of intractable models in science and engineering. A significant\nchallenge in this context is the large computational cost of simulating data\nfrom complex models, and the fact that this cost often depends on parameter\nvalues. We therefore propose \\textit{cost-aware SBI methods} which can\nsignificantly reduce the cost of existing sampling-based SBI methods, such as\nneural SBI and approximate Bayesian computation. This is achieved through a\ncombination of rejection and self-normalised importance sampling, which\nsignificantly reduces the number of expensive simulations needed. Our approach\nis studied extensively on models from epidemiology to telecommunications\nengineering, where we obtain significant reductions in the overall cost of\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference (SBI) is the preferred framework for estimating\nparameters of intractable models in science and engineering. A significant\nchallenge in this context is the large computational cost of simulating data\nfrom complex models, and the fact that this cost often depends on parameter\nvalues. We therefore propose \\textit{cost-aware SBI methods} which can\nsignificantly reduce the cost of existing sampling-based SBI methods, such as\nneural SBI and approximate Bayesian computation. This is achieved through a\ncombination of rejection and self-normalised importance sampling, which\nsignificantly reduces the number of expensive simulations needed. Our approach\nis studied extensively on models from epidemiology to telecommunications\nengineering, where we obtain significant reductions in the overall cost of\ninference."
                },
                "authors": [
                    {
                        "name": "Ayush Bharti"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Franois-Xavier Briol"
                    }
                ],
                "author_detail": {
                    "name": "Franois-Xavier Briol"
                },
                "author": "Franois-Xavier Briol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2003.12408v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2003.12408v5",
                "updated": "2024-10-10T13:55:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    55,
                    55,
                    3,
                    284,
                    0
                ],
                "published": "2020-03-27T13:31:49Z",
                "published_parsed": [
                    2020,
                    3,
                    27,
                    13,
                    31,
                    49,
                    4,
                    87,
                    0
                ],
                "title": "On the role of surrogates in the efficient estimation of treatment\n  effects with limited outcome data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the role of surrogates in the efficient estimation of treatment\n  effects with limited outcome data"
                },
                "summary": "In many experimental and observational studies, the outcome of interest is\noften difficult or expensive to observe, reducing effective sample sizes for\nestimating average treatment effects (ATEs) even when identifiable. We study\nhow incorporating data on units for which only surrogate outcomes not of\nprimary interest are observed can increase the precision of ATE estimation. We\nrefrain from imposing stringent surrogacy conditions, which permit surrogates\nas perfect replacements for the target outcome. Instead, we supplement the\navailable, albeit limited, observations of the target outcome with abundant\nobservations of surrogate outcomes, without any assumptions beyond unconfounded\ntreatment assignment and missingness and corresponding overlap conditions. To\nquantify the potential gains, we derive the difference in efficiency bounds on\nATE estimation with and without surrogates, both when an overwhelming or\ncomparable number of units have missing outcomes. We develop robust ATE\nestimation and inference methods that realize these efficiency gains. We\nempirically demonstrate the gains by studying long-term-earning effects of job\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many experimental and observational studies, the outcome of interest is\noften difficult or expensive to observe, reducing effective sample sizes for\nestimating average treatment effects (ATEs) even when identifiable. We study\nhow incorporating data on units for which only surrogate outcomes not of\nprimary interest are observed can increase the precision of ATE estimation. We\nrefrain from imposing stringent surrogacy conditions, which permit surrogates\nas perfect replacements for the target outcome. Instead, we supplement the\navailable, albeit limited, observations of the target outcome with abundant\nobservations of surrogate outcomes, without any assumptions beyond unconfounded\ntreatment assignment and missingness and corresponding overlap conditions. To\nquantify the potential gains, we derive the difference in efficiency bounds on\nATE estimation with and without surrogates, both when an overwhelming or\ncomparable number of units have missing outcomes. We develop robust ATE\nestimation and inference methods that realize these efficiency gains. We\nempirically demonstrate the gains by studying long-term-earning effects of job\ntraining."
                },
                "authors": [
                    {
                        "name": "Nathan Kallus"
                    },
                    {
                        "name": "Xiaojie Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Mao"
                },
                "author": "Xiaojie Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2003.12408v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2003.12408v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07927v1",
                "updated": "2024-10-10T13:54:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    54,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:54:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    54,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "Efficient Reinforcement Learning with Large Language Model Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reinforcement Learning with Large Language Model Priors"
                },
                "summary": "In sequential decision-making (SDM) tasks, methods like reinforcement\nlearning (RL) and heuristic search have made notable advances in specific\ncases. However, they often require extensive exploration and face challenges in\ngeneralizing across diverse environments due to their limited grasp of the\nunderlying decision dynamics. In contrast, large language models (LLMs) have\nrecently emerged as powerful general-purpose tools, due to their capacity to\nmaintain vast amounts of domain-specific knowledge. To harness this rich prior\nknowledge for efficiently solving complex SDM tasks, we propose treating LLMs\nas prior action distributions and integrating them into RL frameworks through\nBayesian inference methods, making use of variational inference and direct\nposterior sampling. The proposed approaches facilitate the seamless\nincorporation of fixed LLM priors into both policy-based and value-based RL\nframeworks. Our experiments show that incorporating LLM-based action priors\nsignificantly reduces exploration and optimization complexity, substantially\nimproving sample efficiency compared to traditional RL techniques, e.g., using\nLLM priors decreases the number of required samples by over 90% in offline\nlearning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In sequential decision-making (SDM) tasks, methods like reinforcement\nlearning (RL) and heuristic search have made notable advances in specific\ncases. However, they often require extensive exploration and face challenges in\ngeneralizing across diverse environments due to their limited grasp of the\nunderlying decision dynamics. In contrast, large language models (LLMs) have\nrecently emerged as powerful general-purpose tools, due to their capacity to\nmaintain vast amounts of domain-specific knowledge. To harness this rich prior\nknowledge for efficiently solving complex SDM tasks, we propose treating LLMs\nas prior action distributions and integrating them into RL frameworks through\nBayesian inference methods, making use of variational inference and direct\nposterior sampling. The proposed approaches facilitate the seamless\nincorporation of fixed LLM priors into both policy-based and value-based RL\nframeworks. Our experiments show that incorporating LLM-based action priors\nsignificantly reduces exploration and optimization complexity, substantially\nimproving sample efficiency compared to traditional RL techniques, e.g., using\nLLM priors decreases the number of required samples by over 90% in offline\nlearning scenarios."
                },
                "authors": [
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Xidong Feng"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07919v1",
                "updated": "2024-10-10T13:45:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    45,
                    56,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:45:56Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    45,
                    56,
                    3,
                    284,
                    0
                ],
                "title": "InstructBioMol: Advancing Biomolecule Understanding and Design Following\n  Human Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructBioMol: Advancing Biomolecule Understanding and Design Following\n  Human Instructions"
                },
                "summary": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology, and\nenzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nAI's computational power and researchers' intuition, using natural language to\nalign molecular complexity with human intentions. Large Language Models (LLMs)\nhave shown potential to interpret human intentions, yet their application to\nbiomolecular research remains nascent due to challenges including specialized\nknowledge requirements, multimodal data integration, and semantic alignment\nbetween natural language and biomolecules. To address these limitations, we\npresent InstructBioMol, a novel LLM designed to bridge natural language and\nbiomolecules through a comprehensive any-to-any alignment of natural language,\nmolecules, and proteins. This model can integrate multimodal biomolecules as\ninput, and enable researchers to articulate design goals in natural language,\nproviding biomolecular outputs that meet precise biological needs. Experimental\nresults demonstrate InstructBioMol can understand and design biomolecules\nfollowing human instructions. Notably, it can generate drug molecules with a\n10% improvement in binding affinity and design enzymes that achieve an ESP\nScore of 70.4, making it the only method to surpass the enzyme-substrate\ninteraction threshold of 60.0 recommended by the ESP developer. This highlights\nits potential to transform real-world biomolecular research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology, and\nenzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nAI's computational power and researchers' intuition, using natural language to\nalign molecular complexity with human intentions. Large Language Models (LLMs)\nhave shown potential to interpret human intentions, yet their application to\nbiomolecular research remains nascent due to challenges including specialized\nknowledge requirements, multimodal data integration, and semantic alignment\nbetween natural language and biomolecules. To address these limitations, we\npresent InstructBioMol, a novel LLM designed to bridge natural language and\nbiomolecules through a comprehensive any-to-any alignment of natural language,\nmolecules, and proteins. This model can integrate multimodal biomolecules as\ninput, and enable researchers to articulate design goals in natural language,\nproviding biomolecular outputs that meet precise biological needs. Experimental\nresults demonstrate InstructBioMol can understand and design biomolecules\nfollowing human instructions. Notably, it can generate drug molecules with a\n10% improvement in binding affinity and design enzymes that achieve an ESP\nScore of 70.4, making it the only method to surpass the enzyme-substrate\ninteraction threshold of 60.0 recommended by the ESP developer. This highlights\nits potential to transform real-world biomolecular research."
                },
                "authors": [
                    {
                        "name": "Xiang Zhuang"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Tianwen Lyu"
                    },
                    {
                        "name": "Yinuo Jiang"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Zhuoyi Xiang"
                    },
                    {
                        "name": "Zeyuan Wang"
                    },
                    {
                        "name": "Ming Qin"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Jike Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12376v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12376v3",
                "updated": "2024-10-10T13:43:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    43,
                    20,
                    3,
                    284,
                    0
                ],
                "published": "2024-02-19T18:59:07Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    18,
                    59,
                    7,
                    0,
                    50,
                    0
                ],
                "title": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion\n  Model"
                },
                "summary": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo address this limitation, we conceptualize images as sequences of tokens with\ndynamic sizes, rather than traditional methods that perceive images as\nfixed-resolution grids. This perspective enables a flexible training strategy\nthat seamlessly accommodates various aspect ratios during both training and\ninference, thus promoting resolution generalization and eliminating biases\nintroduced by image cropping. On this basis, we present the Flexible Vision\nTransformer (FiT), a transformer architecture specifically designed for\ngenerating images with unrestricted resolutions and aspect ratios. We further\nupgrade the FiT to FiTv2 with several innovative designs, includingthe\nQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flow\nscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted\nnetwork structure, FiTv2 exhibits 2x convergence speed of FiT. When\nincorporating advanced training-free extrapolation techniques, FiTv2\ndemonstrates remarkable adaptability in both resolution extrapolation and\ndiverse resolution generation. Additionally, our exploration of the scalability\nof the FiTv2 model reveals that larger models exhibit better computational\nefficiency. Furthermore, we introduce an efficient post-training strategy to\nadapt a pre-trained model for the high-resolution generation. Comprehensive\nexperiments demonstrate the exceptional performance of FiTv2 across a broad\nrange of resolutions. We have released all the codes and models at\nhttps://github.com/whlzy/FiT to promote the exploration of diffusion\ntransformer models for arbitrary-resolution image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo address this limitation, we conceptualize images as sequences of tokens with\ndynamic sizes, rather than traditional methods that perceive images as\nfixed-resolution grids. This perspective enables a flexible training strategy\nthat seamlessly accommodates various aspect ratios during both training and\ninference, thus promoting resolution generalization and eliminating biases\nintroduced by image cropping. On this basis, we present the Flexible Vision\nTransformer (FiT), a transformer architecture specifically designed for\ngenerating images with unrestricted resolutions and aspect ratios. We further\nupgrade the FiT to FiTv2 with several innovative designs, includingthe\nQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flow\nscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted\nnetwork structure, FiTv2 exhibits 2x convergence speed of FiT. When\nincorporating advanced training-free extrapolation techniques, FiTv2\ndemonstrates remarkable adaptability in both resolution extrapolation and\ndiverse resolution generation. Additionally, our exploration of the scalability\nof the FiTv2 model reveals that larger models exhibit better computational\nefficiency. Furthermore, we introduce an efficient post-training strategy to\nadapt a pre-trained model for the high-resolution generation. Comprehensive\nexperiments demonstrate the exceptional performance of FiTv2 across a broad\nrange of resolutions. We have released all the codes and models at\nhttps://github.com/whlzy/FiT to promote the exploration of diffusion\ntransformer models for arbitrary-resolution image generation."
                },
                "authors": [
                    {
                        "name": "Zidong Wang"
                    },
                    {
                        "name": "Zeyu Lu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "arxiv_comment": "This work is not very well-developed, and there are some errors in\n  the writing. Additionally, it uses too much of the previous content. I hope\n  to withdraw it for improvement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12376v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12376v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05567v2",
                "updated": "2024-10-10T13:37:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    37,
                    29,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-08T00:02:04Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    0,
                    2,
                    4,
                    1,
                    282,
                    0
                ],
                "title": "With random regressors, least squares inference is robust to correlated\n  errors with unknown correlation structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With random regressors, least squares inference is robust to correlated\n  errors with unknown correlation structure"
                },
                "summary": "Linear regression is arguably the most widely used statistical method. With\nfixed regressors and correlated errors, the conventional wisdom is to modify\nthe variance-covariance estimator to accommodate the known correlation\nstructure of the errors. We depart from the literature by showing that with\nrandom regressors, linear regression inference is robust to correlated errors\nwith unknown correlation structure. The existing theoretical analyses for\nlinear regression are no longer valid because even the asymptotic normality of\nthe least-squares coefficients breaks down in this regime. We first prove the\nasymptotic normality of the t statistics by establishing their Berry-Esseen\nbounds based on a novel probabilistic analysis of self-normalized statistics.\nWe then study the local power of the corresponding t tests and show that,\nperhaps surprisingly, error correlation can even enhance power in the regime of\nweak signals. Overall, our results show that linear regression is applicable\nmore broadly than the conventional theory suggests, and further demonstrate the\nvalue of randomization to ensure robustness of inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear regression is arguably the most widely used statistical method. With\nfixed regressors and correlated errors, the conventional wisdom is to modify\nthe variance-covariance estimator to accommodate the known correlation\nstructure of the errors. We depart from the literature by showing that with\nrandom regressors, linear regression inference is robust to correlated errors\nwith unknown correlation structure. The existing theoretical analyses for\nlinear regression are no longer valid because even the asymptotic normality of\nthe least-squares coefficients breaks down in this regime. We first prove the\nasymptotic normality of the t statistics by establishing their Berry-Esseen\nbounds based on a novel probabilistic analysis of self-normalized statistics.\nWe then study the local power of the corresponding t tests and show that,\nperhaps surprisingly, error correlation can even enhance power in the regime of\nweak signals. Overall, our results show that linear regression is applicable\nmore broadly than the conventional theory suggests, and further demonstrate the\nvalue of randomization to ensure robustness of inference."
                },
                "authors": [
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Wen Zhou"
                    },
                    {
                        "name": "Haonan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Wang"
                },
                "author": "Haonan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15368v3",
                "updated": "2024-10-10T13:35:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    35,
                    50,
                    3,
                    284,
                    0
                ],
                "published": "2024-02-23T15:02:44Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    15,
                    2,
                    44,
                    4,
                    54,
                    0
                ],
                "title": "Safe Task Planning for Language-Instructed Multi-Robot Systems using\n  Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Task Planning for Language-Instructed Multi-Robot Systems using\n  Conformal Prediction"
                },
                "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack mission completion guarantees. To address this challenge,\nwe introduce a new distributed LLM-based planner, called S-ATLAS for Safe\nplAnning for Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates while minimizing the\noverall number of help requests. We provide comparative experiments against\nrelated works showing that our method is significantly more computational\nefficient and achieves lower help rates. The advantage of our algorithm over\nbaselines becomes more pronounced with increasing robot team size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack mission completion guarantees. To address this challenge,\nwe introduce a new distributed LLM-based planner, called S-ATLAS for Safe\nplAnning for Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates while minimizing the\noverall number of help requests. We provide comparative experiments against\nrelated works showing that our method is significantly more computational\nefficient and achieves lower help rates. The advantage of our algorithm over\nbaselines becomes more pronounced with increasing robot team size."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guocheng He"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07896v1",
                "updated": "2024-10-10T13:23:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    23,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:23:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    23,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing\n  Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing\n  Machines"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing and reasoning tasks. However, their\nperformance in the foundational domain of arithmetic remains unsatisfactory.\nWhen dealing with arithmetic tasks, LLMs often memorize specific examples\nrather than learning the underlying computational logic, limiting their ability\nto generalize to new problems. In this paper, we propose a Composable\nArithmetic Execution Framework (CAEF) that enables LLMs to learn to execute\nstep-by-step computations by emulating Turing Machines, thereby gaining a\ngenuine understanding of computational logic. Moreover, the proposed framework\nis highly scalable, allowing composing learned operators to significantly\nreduce the difficulty of learning complex operators. In our evaluation, CAEF\nachieves nearly 100% accuracy across seven common mathematical operations on\nthe LLaMA 3.1-8B model, effectively supporting computations involving operands\nwith up to 100 digits, a level where GPT-4o falls short noticeably in some\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing and reasoning tasks. However, their\nperformance in the foundational domain of arithmetic remains unsatisfactory.\nWhen dealing with arithmetic tasks, LLMs often memorize specific examples\nrather than learning the underlying computational logic, limiting their ability\nto generalize to new problems. In this paper, we propose a Composable\nArithmetic Execution Framework (CAEF) that enables LLMs to learn to execute\nstep-by-step computations by emulating Turing Machines, thereby gaining a\ngenuine understanding of computational logic. Moreover, the proposed framework\nis highly scalable, allowing composing learned operators to significantly\nreduce the difficulty of learning complex operators. In our evaluation, CAEF\nachieves nearly 100% accuracy across seven common mathematical operations on\nthe LLaMA 3.1-8B model, effectively supporting computations involving operands\nwith up to 100 digits, a level where GPT-4o falls short noticeably in some\nsettings."
                },
                "authors": [
                    {
                        "name": "Junyu Lai"
                    },
                    {
                        "name": "Jiahe Xu"
                    },
                    {
                        "name": "Yao Yang"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13968v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13968v3",
                "updated": "2024-10-10T13:22:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    22,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    16,
                    7,
                    0,
                    113,
                    0
                ],
                "title": "Protecting Your LLMs with Information Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Your LLMs with Information Bottleneck"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Zefan Wang"
                    },
                    {
                        "name": "Linjie Xu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Tianchun Wang"
                    },
                    {
                        "name": "Chunlin Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Accepted by Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13968v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13968v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10248v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10248v5",
                "updated": "2024-10-10T13:20:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    20,
                    13,
                    3,
                    284,
                    0
                ],
                "published": "2023-08-20T12:21:05Z",
                "published_parsed": [
                    2023,
                    8,
                    20,
                    12,
                    21,
                    5,
                    6,
                    232,
                    0
                ],
                "title": "Steering Language Models With Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models With Activation Engineering"
                },
                "summary": "Prompt engineering and finetuning aim to maximize language model performance\non a given metric (like toxicity reduction). However, these methods do not\nfully elicit a model's capabilities. To reduce this gap, we introduce\nactivation engineering: the inference-time modification of activations in order\nto control (or steer) model outputs. Specifically, we introduce the Activation\nAddition (ActAdd) technique, which contrasts the intermediate activations on\nprompt pairs (such as \"Love\" versus \"Hate\") to compute a steering vector\n(Subramani et al. 2022). By tactically adding in e.g. the \"Love\" - \"Hate\"\nsteering vector during the forward pass, we achieve SOTA on\nnegative-to-positive sentiment shift and detoxification using models including\nLLaMA-3 and OPT. ActAdd yields inference-time control over high-level output\nproperties (like topic and sentiment) while preserving performance on\noff-target tasks. ActAdd is lightweight: it does not require any machine\noptimization and works with a single pair of data points, which enables rapid\niteration over steering. ActAdd demonstrates the power of activation\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering and finetuning aim to maximize language model performance\non a given metric (like toxicity reduction). However, these methods do not\nfully elicit a model's capabilities. To reduce this gap, we introduce\nactivation engineering: the inference-time modification of activations in order\nto control (or steer) model outputs. Specifically, we introduce the Activation\nAddition (ActAdd) technique, which contrasts the intermediate activations on\nprompt pairs (such as \"Love\" versus \"Hate\") to compute a steering vector\n(Subramani et al. 2022). By tactically adding in e.g. the \"Love\" - \"Hate\"\nsteering vector during the forward pass, we achieve SOTA on\nnegative-to-positive sentiment shift and detoxification using models including\nLLaMA-3 and OPT. ActAdd yields inference-time control over high-level output\nproperties (like topic and sentiment) while preserving performance on\noff-target tasks. ActAdd is lightweight: it does not require any machine\noptimization and works with a single pair of data points, which enables rapid\niteration over steering. ActAdd demonstrates the power of activation\nengineering."
                },
                "authors": [
                    {
                        "name": "Alexander Matt Turner"
                    },
                    {
                        "name": "Lisa Thiergart"
                    },
                    {
                        "name": "Gavin Leech"
                    },
                    {
                        "name": "David Udell"
                    },
                    {
                        "name": "Juan J. Vazquez"
                    },
                    {
                        "name": "Ulisse Mini"
                    },
                    {
                        "name": "Monte MacDiarmid"
                    }
                ],
                "author_detail": {
                    "name": "Monte MacDiarmid"
                },
                "author": "Monte MacDiarmid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10248v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10248v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06992v2",
                "updated": "2024-10-10T13:13:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    13,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T15:38:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
                },
                "summary": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues."
                },
                "authors": [
                    {
                        "name": "Reem Aleithan"
                    },
                    {
                        "name": "Haoran Xue"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajer"
                    },
                    {
                        "name": "Elijah Nnorom"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Song Wang"
                    }
                ],
                "author_detail": {
                    "name": "Song Wang"
                },
                "author": "Song Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07890v1",
                "updated": "2024-10-10T13:12:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    12,
                    14,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:12:14Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    12,
                    14,
                    3,
                    284,
                    0
                ],
                "title": "Identifying latent disease factors differently expressed in patient\n  subgroups using group factor analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying latent disease factors differently expressed in patient\n  subgroups using group factor analysis"
                },
                "summary": "In this study, we propose a novel approach to uncover subgroup-specific and\nsubgroup-common latent factors addressing the challenges posed by the\nheterogeneity of neurological and mental disorders, which hinder disease\nunderstanding, treatment development, and outcome prediction. The proposed\napproach, sparse Group Factor Analysis (GFA) with regularised horseshoe priors,\nwas implemented with probabilistic programming and can uncover associations (or\nlatent factors) among multiple data modalities differentially expressed in\nsample subgroups. Synthetic data experiments showed the robustness of our\nsparse GFA by correctly inferring latent factors and model parameters. When\napplied to the Genetic Frontotemporal Dementia Initiative (GENFI) dataset,\nwhich comprises patients with frontotemporal dementia (FTD) with genetically\ndefined subgroups, the sparse GFA identified latent disease factors\ndifferentially expressed across the subgroups, distinguishing between\n\"subgroup-specific\" latent factors within homogeneous groups and \"subgroup\ncommon\" latent factors shared across subgroups. The latent disease factors\ncaptured associations between brain structure and non-imaging variables (i.e.,\nquestionnaires assessing behaviour and disease severity) across the different\ngenetic subgroups, offering insights into disease profiles. Importantly, two\nlatent factors were more pronounced in the two more homogeneous FTD patient\nsubgroups (progranulin (GRN) and microtubule-associated protein tau (MAPT)\nmutation), showcasing the method's ability to reveal subgroup-specific\ncharacteristics. These findings underscore the potential of sparse GFA for\nintegrating multiple data modalities and identifying interpretable latent\ndisease factors that can improve the characterization and stratification of\npatients with neurological and mental health disorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a novel approach to uncover subgroup-specific and\nsubgroup-common latent factors addressing the challenges posed by the\nheterogeneity of neurological and mental disorders, which hinder disease\nunderstanding, treatment development, and outcome prediction. The proposed\napproach, sparse Group Factor Analysis (GFA) with regularised horseshoe priors,\nwas implemented with probabilistic programming and can uncover associations (or\nlatent factors) among multiple data modalities differentially expressed in\nsample subgroups. Synthetic data experiments showed the robustness of our\nsparse GFA by correctly inferring latent factors and model parameters. When\napplied to the Genetic Frontotemporal Dementia Initiative (GENFI) dataset,\nwhich comprises patients with frontotemporal dementia (FTD) with genetically\ndefined subgroups, the sparse GFA identified latent disease factors\ndifferentially expressed across the subgroups, distinguishing between\n\"subgroup-specific\" latent factors within homogeneous groups and \"subgroup\ncommon\" latent factors shared across subgroups. The latent disease factors\ncaptured associations between brain structure and non-imaging variables (i.e.,\nquestionnaires assessing behaviour and disease severity) across the different\ngenetic subgroups, offering insights into disease profiles. Importantly, two\nlatent factors were more pronounced in the two more homogeneous FTD patient\nsubgroups (progranulin (GRN) and microtubule-associated protein tau (MAPT)\nmutation), showcasing the method's ability to reveal subgroup-specific\ncharacteristics. These findings underscore the potential of sparse GFA for\nintegrating multiple data modalities and identifying interpretable latent\ndisease factors that can improve the characterization and stratification of\npatients with neurological and mental health disorders."
                },
                "authors": [
                    {
                        "name": "Fabio S. Ferreira"
                    },
                    {
                        "name": "John Ashburner"
                    },
                    {
                        "name": "Arabella Bouzigues"
                    },
                    {
                        "name": "Chatrin Suksasilp"
                    },
                    {
                        "name": "Lucy L. Russell"
                    },
                    {
                        "name": "Phoebe H. Foster"
                    },
                    {
                        "name": "Eve Ferry-Bolder"
                    },
                    {
                        "name": "John C. van Swieten"
                    },
                    {
                        "name": "Lize C. Jiskoot"
                    },
                    {
                        "name": "Harro Seelaar"
                    },
                    {
                        "name": "Raquel Sanchez-Valle"
                    },
                    {
                        "name": "Robert Laforce"
                    },
                    {
                        "name": "Caroline Graff"
                    },
                    {
                        "name": "Daniela Galimberti"
                    },
                    {
                        "name": "Rik Vandenberghe"
                    },
                    {
                        "name": "Alexandre de Mendonca"
                    },
                    {
                        "name": "Pietro Tiraboschi"
                    },
                    {
                        "name": "Isabel Santana"
                    },
                    {
                        "name": "Alexander Gerhard"
                    },
                    {
                        "name": "Johannes Levin"
                    },
                    {
                        "name": "Sandro Sorbi"
                    },
                    {
                        "name": "Markus Otto"
                    },
                    {
                        "name": "Florence Pasquier"
                    },
                    {
                        "name": "Simon Ducharme"
                    },
                    {
                        "name": "Chris R. Butler"
                    },
                    {
                        "name": "Isabelle Le Ber"
                    },
                    {
                        "name": "Elizabeth Finger"
                    },
                    {
                        "name": "Maria C. Tartaglia"
                    },
                    {
                        "name": "Mario Masellis"
                    },
                    {
                        "name": "James B. Rowe"
                    },
                    {
                        "name": "Matthis Synofzik"
                    },
                    {
                        "name": "Fermin Moreno"
                    },
                    {
                        "name": "Barbara Borroni"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Jonathan D. Rohrer"
                    },
                    {
                        "name": "Janaina Mourao-Miranda"
                    }
                ],
                "author_detail": {
                    "name": "Janaina Mourao-Miranda"
                },
                "author": "Janaina Mourao-Miranda",
                "arxiv_comment": "38 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07872v1",
                "updated": "2024-10-10T12:46:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    46,
                    3,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:46:03Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    46,
                    3,
                    3,
                    284,
                    0
                ],
                "title": "L-VITeX: Light-weight Visual Intuition for Terrain Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L-VITeX: Light-weight Visual Intuition for Terrain Exploration"
                },
                "summary": "This paper presents L-VITeX, a lightweight visual intuition system for\nterrain exploration designed for resource-constrained robots and swarms.\nL-VITeX aims to provide a hint of Regions of Interest (RoIs) without\ncomputationally expensive processing. By utilizing the Faster Objects, More\nObjects (FOMO) tinyML architecture, the system achieves high accuracy (>99%) in\nRoI detection while operating on minimal hardware resources (Peak RAM usage <\n50 KB) with near real-time inference (<200 ms). The paper evaluates L-VITeX's\nperformance across various terrains, including mountainous areas, underwater\nshipwreck debris regions, and Martian rocky surfaces. Additionally, it\ndemonstrates the system's application in 3D mapping using a small mobile robot\nrun by ESP32-Cam and Gaussian Splats (GS), showcasing its potential to enhance\nexploration efficiency and decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents L-VITeX, a lightweight visual intuition system for\nterrain exploration designed for resource-constrained robots and swarms.\nL-VITeX aims to provide a hint of Regions of Interest (RoIs) without\ncomputationally expensive processing. By utilizing the Faster Objects, More\nObjects (FOMO) tinyML architecture, the system achieves high accuracy (>99%) in\nRoI detection while operating on minimal hardware resources (Peak RAM usage <\n50 KB) with near real-time inference (<200 ms). The paper evaluates L-VITeX's\nperformance across various terrains, including mountainous areas, underwater\nshipwreck debris regions, and Martian rocky surfaces. Additionally, it\ndemonstrates the system's application in 3D mapping using a small mobile robot\nrun by ESP32-Cam and Gaussian Splats (GS), showcasing its potential to enhance\nexploration efficiency and decision-making."
                },
                "authors": [
                    {
                        "name": "Antar Mazumder"
                    },
                    {
                        "name": "Zarin Anjum Madhiha"
                    }
                ],
                "author_detail": {
                    "name": "Zarin Anjum Madhiha"
                },
                "author": "Zarin Anjum Madhiha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05864v2",
                "updated": "2024-10-10T12:41:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    26,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-08T09:53:35Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    35,
                    1,
                    282,
                    0
                ],
                "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Words: On the Inner Lexicon of LLMs"
                },
                "summary": "Natural language is composed of words, but modern LLMs process sub-words as\ninput. A natural question raised by this discrepancy is whether LLMs encode\nwords internally, and if so how. We present evidence that LLMs engage in an\nintrinsic detokenization process, where sub-word sequences are combined into\ncoherent word representations. Our experiments show that this process takes\nplace primarily within the early and middle layers of the model. They also show\nthat it is robust to non-morphemic splits, typos and perhaps importantly-to\nout-of-vocabulary words: when feeding the inner representation of such words to\nthe model as input vectors, it can \"understand\" them despite never seeing them\nduring training. Our findings suggest that LLMs maintain a latent vocabulary\nbeyond the tokenizer's scope. These insights provide a practical,\nfinetuning-free application for expanding the vocabulary of pre-trained models.\nBy enabling the addition of new vocabulary words, we reduce input length and\ninference iterations, which reduces both space and model latency, with little\nto no loss in model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language is composed of words, but modern LLMs process sub-words as\ninput. A natural question raised by this discrepancy is whether LLMs encode\nwords internally, and if so how. We present evidence that LLMs engage in an\nintrinsic detokenization process, where sub-word sequences are combined into\ncoherent word representations. Our experiments show that this process takes\nplace primarily within the early and middle layers of the model. They also show\nthat it is robust to non-morphemic splits, typos and perhaps importantly-to\nout-of-vocabulary words: when feeding the inner representation of such words to\nthe model as input vectors, it can \"understand\" them despite never seeing them\nduring training. Our findings suggest that LLMs maintain a latent vocabulary\nbeyond the tokenizer's scope. These insights provide a practical,\nfinetuning-free application for expanding the vocabulary of pre-trained models.\nBy enabling the addition of new vocabulary words, we reduce input length and\ninference iterations, which reduces both space and model latency, with little\nto no loss in model accuracy."
                },
                "authors": [
                    {
                        "name": "Guy Kaplan"
                    },
                    {
                        "name": "Matanel Oren"
                    },
                    {
                        "name": "Yuval Reif"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07869v1",
                "updated": "2024-10-10T12:41:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "Benchmarking Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Agentic Workflow Generation"
                },
                "summary": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset will be available at\nhttps://github.com/zjunlp/WorFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset will be available at\nhttps://github.com/zjunlp/WorFBench."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07863v1",
                "updated": "2024-10-10T12:30:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    30,
                    56,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:30:56Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    30,
                    56,
                    3,
                    284,
                    0
                ],
                "title": "Learning to Balance Altruism and Self-interest Based on Empathy in\n  Mixed-Motive Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Balance Altruism and Self-interest Based on Empathy in\n  Mixed-Motive Games"
                },
                "summary": "Real-world multi-agent scenarios often involve mixed motives, demanding\naltruistic agents capable of self-protection against potential exploitation.\nHowever, existing approaches often struggle to achieve both objectives. In this\npaper, based on that empathic responses are modulated by inferred social\nrelationships between agents, we propose LASE Learning to balance Altruism and\nSelf-interest based on Empathy), a distributed multi-agent reinforcement\nlearning algorithm that fosters altruistic cooperation through gifting while\navoiding exploitation by other agents in mixed-motive games. LASE allocates a\nportion of its rewards to co-players as gifts, with this allocation adapting\ndynamically based on the social relationship -- a metric evaluating the\nfriendliness of co-players estimated by counterfactual reasoning. In\nparticular, social relationship measures each co-player by comparing the\nestimated $Q$-function of current joint action to a counterfactual baseline\nwhich marginalizes the co-player's action, with its action distribution\ninferred by a perspective-taking module. Comprehensive experiments are\nperformed in spatially and temporally extended mixed-motive games,\ndemonstrating LASE's ability to promote group collaboration without\ncompromising fairness and its capacity to adapt policies to various types of\ninteractive co-players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world multi-agent scenarios often involve mixed motives, demanding\naltruistic agents capable of self-protection against potential exploitation.\nHowever, existing approaches often struggle to achieve both objectives. In this\npaper, based on that empathic responses are modulated by inferred social\nrelationships between agents, we propose LASE Learning to balance Altruism and\nSelf-interest based on Empathy), a distributed multi-agent reinforcement\nlearning algorithm that fosters altruistic cooperation through gifting while\navoiding exploitation by other agents in mixed-motive games. LASE allocates a\nportion of its rewards to co-players as gifts, with this allocation adapting\ndynamically based on the social relationship -- a metric evaluating the\nfriendliness of co-players estimated by counterfactual reasoning. In\nparticular, social relationship measures each co-player by comparing the\nestimated $Q$-function of current joint action to a counterfactual baseline\nwhich marginalizes the co-player's action, with its action distribution\ninferred by a perspective-taking module. Comprehensive experiments are\nperformed in spatially and temporally extended mixed-motive games,\ndemonstrating LASE's ability to promote group collaboration without\ncompromising fairness and its capacity to adapt policies to various types of\ninteractive co-players."
                },
                "authors": [
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07851v1",
                "updated": "2024-10-10T12:18:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    18,
                    42,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:18:42Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    18,
                    42,
                    3,
                    284,
                    0
                ],
                "title": "Scalable Representation Learning for Multimodal Tabular Transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Representation Learning for Multimodal Tabular Transactions"
                },
                "summary": "Large language models (LLMs) are primarily designed to understand\nunstructured text. When directly applied to structured formats such as tabular\ndata, they may struggle to discern inherent relationships and overlook critical\npatterns. While tabular representation learning methods can address some of\nthese limitations, existing efforts still face challenges with sparse\nhigh-cardinality fields, precise numerical reasoning, and column-heavy tables.\nFurthermore, leveraging these learned representations for downstream tasks\nthrough a language based interface is not apparent. In this paper, we present\nan innovative and scalable solution to these challenges. Concretely, our\napproach introduces a multi-tier partitioning mechanism that utilizes power-law\ndynamics to handle large vocabularies, an adaptive quantization mechanism to\nimpose priors on numerical continuity, and a distinct treatment of core-columns\nand meta-information columns. To facilitate instruction tuning on LLMs, we\npropose a parameter efficient decoder that interleaves transaction and text\nmodalities using a series of adapter layers, thereby exploiting rich cross-task\nknowledge. We validate the efficacy of our solution on a large-scale dataset of\nsynthetic payments transactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are primarily designed to understand\nunstructured text. When directly applied to structured formats such as tabular\ndata, they may struggle to discern inherent relationships and overlook critical\npatterns. While tabular representation learning methods can address some of\nthese limitations, existing efforts still face challenges with sparse\nhigh-cardinality fields, precise numerical reasoning, and column-heavy tables.\nFurthermore, leveraging these learned representations for downstream tasks\nthrough a language based interface is not apparent. In this paper, we present\nan innovative and scalable solution to these challenges. Concretely, our\napproach introduces a multi-tier partitioning mechanism that utilizes power-law\ndynamics to handle large vocabularies, an adaptive quantization mechanism to\nimpose priors on numerical continuity, and a distinct treatment of core-columns\nand meta-information columns. To facilitate instruction tuning on LLMs, we\npropose a parameter efficient decoder that interleaves transaction and text\nmodalities using a series of adapter layers, thereby exploiting rich cross-task\nknowledge. We validate the efficacy of our solution on a large-scale dataset of\nsynthetic payments transactions."
                },
                "authors": [
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07840v1",
                "updated": "2024-10-10T11:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    59,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    59,
                    58,
                    3,
                    284,
                    0
                ],
                "title": "Protect Before Generate: Error Correcting Codes within Discrete Deep\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protect Before Generate: Error Correcting Codes within Discrete Deep\n  Generative Models"
                },
                "summary": "Despite significant advancements in deep probabilistic models, learning\nlow-dimensional discrete latent representations remains a challenging task. In\nthis paper, we introduce a novel method that enhances variational inference in\ndiscrete latent variable models by leveraging Error Correcting Codes (ECCs) to\nintroduce redundancy in the latent representations. This redundancy is then\nexploited by the variational posterior to yield more accurate estimates,\nthereby narrowing the variational gap. Inspired by ECCs commonly used in\ndigital communications and data storage, we demonstrate proof-of-concept using\na Discrete Variational Autoencoder (DVAE) with binary latent variables and\nblock repetition codes. We further extend this idea to a hierarchical structure\nbased on polar codes, where certain latent bits are more robustly protected.\nOur method improves generation quality, data reconstruction, and uncertainty\ncalibration compared to the uncoded DVAE, even when trained with tighter bounds\nsuch as the Importance Weighted Autoencoder (IWAE) objective. In particular, we\ndemonstrate superior performance on MNIST, FMNIST, CIFAR10, and Tiny ImageNet\ndatasets. The general approach of integrating ECCs into variational inference\nis compatible with existing techniques to boost variational inference, such as\nimportance sampling or Hamiltonian Monte Carlo. We also outline the key\nproperties ECCs must have to effectively enhance discrete variational\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in deep probabilistic models, learning\nlow-dimensional discrete latent representations remains a challenging task. In\nthis paper, we introduce a novel method that enhances variational inference in\ndiscrete latent variable models by leveraging Error Correcting Codes (ECCs) to\nintroduce redundancy in the latent representations. This redundancy is then\nexploited by the variational posterior to yield more accurate estimates,\nthereby narrowing the variational gap. Inspired by ECCs commonly used in\ndigital communications and data storage, we demonstrate proof-of-concept using\na Discrete Variational Autoencoder (DVAE) with binary latent variables and\nblock repetition codes. We further extend this idea to a hierarchical structure\nbased on polar codes, where certain latent bits are more robustly protected.\nOur method improves generation quality, data reconstruction, and uncertainty\ncalibration compared to the uncoded DVAE, even when trained with tighter bounds\nsuch as the Importance Weighted Autoencoder (IWAE) objective. In particular, we\ndemonstrate superior performance on MNIST, FMNIST, CIFAR10, and Tiny ImageNet\ndatasets. The general approach of integrating ECCs into variational inference\nis compatible with existing techniques to boost variational inference, such as\nimportance sampling or Hamiltonian Monte Carlo. We also outline the key\nproperties ECCs must have to effectively enhance discrete variational\ninference."
                },
                "authors": [
                    {
                        "name": "Mara Martnez-Garca"
                    },
                    {
                        "name": "Grace Villacrs"
                    },
                    {
                        "name": "David Mitchell"
                    },
                    {
                        "name": "Pablo M. Olmos"
                    }
                ],
                "author_detail": {
                    "name": "Pablo M. Olmos"
                },
                "author": "Pablo M. Olmos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07839v1",
                "updated": "2024-10-10T11:58:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:58:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Enhancing Language Model Reasoning via Weighted Reasoning in\n  Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Language Model Reasoning via Weighted Reasoning in\n  Self-Consistency"
                },
                "summary": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al's self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the detailed step-by-step reasoning paths applied by these\npaths. Our work enhances this approach by incorporating and analyzing both the\nreasoning paths of these rationales in addition to their final decisions before\ntaking a majority vote. These methods not only improve the reliability of\nreasoning paths but also cause more robust performance on complex reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al's self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the detailed step-by-step reasoning paths applied by these\npaths. Our work enhances this approach by incorporating and analyzing both the\nreasoning paths of these rationales in addition to their final decisions before\ntaking a majority vote. These methods not only improve the reliability of\nreasoning paths but also cause more robust performance on complex reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tim Knappe"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Ayush Chauhan"
                    },
                    {
                        "name": "Kaylee Chhua"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted to MATH-AI at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07838v1",
                "updated": "2024-10-10T11:56:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    56,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:56:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    56,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "MinorityPrompt: Text to Minority Image Generation via Prompt\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MinorityPrompt: Text to Minority Image Generation via Prompt\n  Optimization"
                },
                "summary": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers."
                },
                "authors": [
                    {
                        "name": "Soobin Um"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "23 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17515v2",
                "updated": "2024-10-10T11:53:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    53,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-26T03:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection"
                },
                "summary": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data."
                },
                "authors": [
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Maike Feng"
                    },
                    {
                        "name": "Jing Qiu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13009v2",
                "updated": "2024-10-10T11:51:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    51,
                    24,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-13T10:51:43Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    51,
                    43,
                    0,
                    134,
                    0
                ],
                "title": "MetaReflection: Learning Instructions for Language Agents using Past\n  Reflections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaReflection: Learning Instructions for Language Agents using Past\n  Reflections"
                },
                "summary": "The popularity of Large Language Models (LLMs) have unleashed a new age\nofLanguage Agents for solving a diverse range of tasks. While contemporary\nfrontier LLMs are capable enough to power reasonably good Language agents, the\nclosed-API model makes it hard to improve in cases they perform sub-optimally.\nTo address this, recent works have explored ways to improve their performance\nusing techniques like self-reflection and prompt optimization. Unfortunately,\ntechniques like self-reflection can be used only in an online setup, while\ncontemporary prompt optimization techniques are designed and tested to work on\nsimple tasks. To this end, we introduce MetaReflection, a novel offline\nreinforcement learning technique that enhances the performance of Language\nAgents by augmenting a semantic memory based on experiential learnings from\npast trials. We demonstrate the efficacy of MetaReflection by evaluating across\nmultiple domains, including complex logical reasoning, biomedical semantic\nsimilarity, open world question answering, and vulnerability threat detection,\nin Infrastructure-as-Code, spanning different agent designs. MetaReflection\nboosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline\nand performs on par with existing state-of-the-art prompt optimization\ntechniques while requiring fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of Large Language Models (LLMs) have unleashed a new age\nofLanguage Agents for solving a diverse range of tasks. While contemporary\nfrontier LLMs are capable enough to power reasonably good Language agents, the\nclosed-API model makes it hard to improve in cases they perform sub-optimally.\nTo address this, recent works have explored ways to improve their performance\nusing techniques like self-reflection and prompt optimization. Unfortunately,\ntechniques like self-reflection can be used only in an online setup, while\ncontemporary prompt optimization techniques are designed and tested to work on\nsimple tasks. To this end, we introduce MetaReflection, a novel offline\nreinforcement learning technique that enhances the performance of Language\nAgents by augmenting a semantic memory based on experiential learnings from\npast trials. We demonstrate the efficacy of MetaReflection by evaluating across\nmultiple domains, including complex logical reasoning, biomedical semantic\nsimilarity, open world question answering, and vulnerability threat detection,\nin Infrastructure-as-Code, spanning different agent designs. MetaReflection\nboosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline\nand performs on par with existing state-of-the-art prompt optimization\ntechniques while requiring fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Priyanshu Gupta"
                    },
                    {
                        "name": "Shashank Kirtania"
                    },
                    {
                        "name": "Ananya Singha"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sherry Shi"
                    },
                    {
                        "name": "Gustavo Soares"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Soares"
                },
                "author": "Gustavo Soares",
                "arxiv_comment": "We release our experimental code at:\n  https://aka.ms/metareflection-code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02952v3",
                "updated": "2024-10-10T11:41:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    41,
                    35,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-03T19:52:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    52,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications"
                },
                "summary": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation."
                },
                "authors": [
                    {
                        "name": "Oren Sultan"
                    },
                    {
                        "name": "Alex Khasin"
                    },
                    {
                        "name": "Guy Shiran"
                    },
                    {
                        "name": "Asnat Greenstein-Messica"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15934v2",
                "updated": "2024-10-10T11:37:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    37,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-24T09:57:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents"
                },
                "summary": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains."
                },
                "authors": [
                    {
                        "name": "Samuel Arcadinho"
                    },
                    {
                        "name": "David Aparicio"
                    },
                    {
                        "name": "Mariana Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Almeida"
                },
                "author": "Mariana Almeida",
                "arxiv_comment": "14 pages, 5 figures, Submitted to GenBench@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.06645v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.06645v6",
                "updated": "2024-10-10T11:35:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    35,
                    50,
                    3,
                    284,
                    0
                ],
                "published": "2023-05-11T08:23:43Z",
                "published_parsed": [
                    2023,
                    5,
                    11,
                    8,
                    23,
                    43,
                    3,
                    131,
                    0
                ],
                "title": "Causal Inference for Continuous Multiple Time Point Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference for Continuous Multiple Time Point Interventions"
                },
                "summary": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$<13$ years in Zambia/Uganda. Simulations show in which situations a standard\ng-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$<13$ years in Zambia/Uganda. Simulations show in which situations a standard\ng-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest."
                },
                "authors": [
                    {
                        "name": "Michael Schomaker"
                    },
                    {
                        "name": "Helen McIlleron"
                    },
                    {
                        "name": "Paolo Denti"
                    },
                    {
                        "name": "Ivn Daz"
                    }
                ],
                "author_detail": {
                    "name": "Ivn Daz"
                },
                "author": "Ivn Daz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.06645v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.06645v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07830v1",
                "updated": "2024-10-10T11:33:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    33,
                    25,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:33:25Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    33,
                    25,
                    3,
                    284,
                    0
                ],
                "title": "NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional promise in\ntranslation tasks for high-resource languages. However, their performance in\nlow-resource languages is limited by the scarcity of both parallel and\nmonolingual corpora, as well as the presence of noise. Consequently, such LLMs\nsuffer with alignment and have lagged behind State-of-The-Art (SoTA) neural\nmachine translation (NMT) models in these settings. This paper introduces\nNusaMT-7B, an LLM-based machine translation model for low-resource Indonesian\nlanguages, starting with Balinese and Minangkabau. Leveraging the pretrained\nLLaMA2-7B, our approach integrates continued pre-training on monolingual data,\nSupervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to\nreduce noise in parallel sentences. In the FLORES-200 multilingual translation\nbenchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to\n+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms\nby up to -3.38 spBLEU in translations into higher-resource languages. Our\nresults show that fine-tuned LLMs can enhance translation quality for\nlow-resource languages, aiding in linguistic preservation and cross-cultural\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional promise in\ntranslation tasks for high-resource languages. However, their performance in\nlow-resource languages is limited by the scarcity of both parallel and\nmonolingual corpora, as well as the presence of noise. Consequently, such LLMs\nsuffer with alignment and have lagged behind State-of-The-Art (SoTA) neural\nmachine translation (NMT) models in these settings. This paper introduces\nNusaMT-7B, an LLM-based machine translation model for low-resource Indonesian\nlanguages, starting with Balinese and Minangkabau. Leveraging the pretrained\nLLaMA2-7B, our approach integrates continued pre-training on monolingual data,\nSupervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to\nreduce noise in parallel sentences. In the FLORES-200 multilingual translation\nbenchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to\n+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms\nby up to -3.38 spBLEU in translations into higher-resource languages. Our\nresults show that fine-tuned LLMs can enhance translation quality for\nlow-resource languages, aiding in linguistic preservation and cross-cultural\ncommunication."
                },
                "authors": [
                    {
                        "name": "William Tan"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Accepted to SoLaR @ NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07826v1",
                "updated": "2024-10-10T11:24:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    24,
                    4,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:24:04Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    24,
                    4,
                    3,
                    284,
                    0
                ],
                "title": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study\n  of Alignment with Human Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study\n  of Alignment with Human Responses"
                },
                "summary": "Language models often misinterpret human intentions due to their handling of\nambiguity, a limitation well-recognized in NLP research. While morally clear\nscenarios are more discernible to LLMs, greater difficulty is encountered in\nmorally ambiguous contexts. In this investigation, we explored LLM calibration\nto show that human and LLM judgments are poorly aligned in such scenarios. We\nused two curated datasets from the Scruples project for evaluation: DILEMMAS,\nwhich involves pairs of distinct moral scenarios to assess the model's ability\nto compare and contrast ethical situations, and ANECDOTES, which presents\nindividual narratives to evaluate the model's skill in drawing out details,\ninterpreting, and analyzing distinct moral scenarios. Model answer\nprobabilities were extracted for all possible choices and compared with human\nannotations to benchmark the alignment of three models: Llama-3.1-8b,\nZephyr-7b-beta, and Mistral-7b. Significant improvements were observed after\nfine-tuning, with notable enhancements in both cross-entropy and Dirichlet\nscores, particularly in the latter. Notably, after fine-tuning, the performance\nof Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental\nmodels that were examined were all still outperformed by the BERT and RoBERTa\nmodels in terms of cross-entropy scores. Our fine-tuning approach, which\nimproves the model's understanding of text distributions in a text-to-text\nformat, effectively enhances performance and alignment in complex\ndecision-making contexts, underscoring the need for further research to refine\nethical reasoning techniques and capture human judgment nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models often misinterpret human intentions due to their handling of\nambiguity, a limitation well-recognized in NLP research. While morally clear\nscenarios are more discernible to LLMs, greater difficulty is encountered in\nmorally ambiguous contexts. In this investigation, we explored LLM calibration\nto show that human and LLM judgments are poorly aligned in such scenarios. We\nused two curated datasets from the Scruples project for evaluation: DILEMMAS,\nwhich involves pairs of distinct moral scenarios to assess the model's ability\nto compare and contrast ethical situations, and ANECDOTES, which presents\nindividual narratives to evaluate the model's skill in drawing out details,\ninterpreting, and analyzing distinct moral scenarios. Model answer\nprobabilities were extracted for all possible choices and compared with human\nannotations to benchmark the alignment of three models: Llama-3.1-8b,\nZephyr-7b-beta, and Mistral-7b. Significant improvements were observed after\nfine-tuning, with notable enhancements in both cross-entropy and Dirichlet\nscores, particularly in the latter. Notably, after fine-tuning, the performance\nof Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental\nmodels that were examined were all still outperformed by the BERT and RoBERTa\nmodels in terms of cross-entropy scores. Our fine-tuning approach, which\nimproves the model's understanding of text distributions in a text-to-text\nformat, effectively enhances performance and alignment in complex\ndecision-making contexts, underscoring the need for further research to refine\nethical reasoning techniques and capture human judgment nuances."
                },
                "authors": [
                    {
                        "name": "Pranav Senthilkumar"
                    },
                    {
                        "name": "Visshwa Balasubramanian"
                    },
                    {
                        "name": "Prisha Jain"
                    },
                    {
                        "name": "Aneesa Maity"
                    },
                    {
                        "name": "Jonathan Lu"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Accepted to NeurIPS 2024, SoLaR workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07825v1",
                "updated": "2024-10-10T11:23:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:23:18Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "title": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models"
                },
                "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "18 Pages. Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10167v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10167v4",
                "updated": "2024-10-10T11:17:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    17,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-14T11:41:03Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    11,
                    41,
                    3,
                    6,
                    196,
                    0
                ],
                "title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs."
                },
                "authors": [
                    {
                        "name": "Xunyu Zhu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Can Ma"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Major Updates:1.fix faults in the error analysis, 2. improve our\n  method, 3. use ChatGPT as teacher LLMs to ensure fairness in performance\n  comparisons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10167v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10167v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.01460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.01460v4",
                "updated": "2024-10-10T11:16:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    16,
                    28,
                    3,
                    284,
                    0
                ],
                "published": "2023-06-02T11:37:22Z",
                "published_parsed": [
                    2023,
                    6,
                    2,
                    11,
                    37,
                    22,
                    4,
                    153,
                    0
                ],
                "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive\n  Advantages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive\n  Advantages"
                },
                "summary": "This paper proposes a step toward approximate Bayesian inference in on-policy\nactor-critic deep reinforcement learning. It is implemented through three\nchanges to the Asynchronous Advantage Actor-Critic (A3C) algorithm: (1)\napplying a ReLU function to advantage estimates, (2) spectral normalization of\nactor-critic weights, and (3) incorporating \\emph{dropout as a Bayesian\napproximation}. We prove under standard assumptions that restricting policy\nupdates to positive advantages optimizes for value by maximizing a lower bound\non the value function plus an additive term. We show that the additive term is\nbounded proportional to the Lipschitz constant of the value function, which\noffers theoretical grounding for spectral normalization of critic weights.\nFinally, our application of dropout corresponds to approximate Bayesian\ninference over both the actor and critic parameters, which enables\n\\textit{adaptive state-aware} exploration around the modes of the actor via\nThompson sampling. We demonstrate significant improvements for median and\ninterquartile mean metrics over A3C, PPO, SAC, and TD3 on the MuJoCo continuous\ncontrol benchmark and improvement over PPO in the challenging ProcGen\ngeneralization benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a step toward approximate Bayesian inference in on-policy\nactor-critic deep reinforcement learning. It is implemented through three\nchanges to the Asynchronous Advantage Actor-Critic (A3C) algorithm: (1)\napplying a ReLU function to advantage estimates, (2) spectral normalization of\nactor-critic weights, and (3) incorporating \\emph{dropout as a Bayesian\napproximation}. We prove under standard assumptions that restricting policy\nupdates to positive advantages optimizes for value by maximizing a lower bound\non the value function plus an additive term. We show that the additive term is\nbounded proportional to the Lipschitz constant of the value function, which\noffers theoretical grounding for spectral normalization of critic weights.\nFinally, our application of dropout corresponds to approximate Bayesian\ninference over both the actor and critic parameters, which enables\n\\textit{adaptive state-aware} exploration around the modes of the actor via\nThompson sampling. We demonstrate significant improvements for median and\ninterquartile mean metrics over A3C, PPO, SAC, and TD3 on the MuJoCo continuous\ncontrol benchmark and improvement over PPO in the challenging ProcGen\ngeneralization benchmark."
                },
                "authors": [
                    {
                        "name": "Andrew Jesson"
                    },
                    {
                        "name": "Chris Lu"
                    },
                    {
                        "name": "Gunshi Gupta"
                    },
                    {
                        "name": "Nicolas Beltran-Velez"
                    },
                    {
                        "name": "Angelos Filos"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.01460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.01460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07820v1",
                "updated": "2024-10-10T11:11:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    11,
                    32,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:11:32Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    11,
                    32,
                    3,
                    284,
                    0
                ],
                "title": "Mitigating Gender Bias in Code Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Gender Bias in Code Large Language Models via Model Editing"
                },
                "summary": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity."
                },
                "authors": [
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Zecheng Wang"
                    },
                    {
                        "name": "Deyuan Liu"
                    },
                    {
                        "name": "Cunhang Fan"
                    },
                    {
                        "name": "Zhao Lv"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    },
                    {
                        "name": "Dianbo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Sui"
                },
                "author": "Dianbo Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07819v1",
                "updated": "2024-10-10T11:09:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    9,
                    0,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:09:00Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    9,
                    0,
                    3,
                    284,
                    0
                ],
                "title": "Uncovering Overfitting in Large Language Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Overfitting in Large Language Model Editing"
                },
                "summary": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nof limited effectiveness in knowledge editing. To overcome this, inspired by\nLLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy\ncalled Learn to Inference (LTI), which introduce a Multi-stage Inference\nConstraint module to guide the edited models in recalling new knowledge\nsimilarly to how unedited LLMs leverage knowledge through in-context learning.\nExtensive experimental results across a wide range of tasks validate the\neffectiveness of LTI in mitigating Editing Overfit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nof limited effectiveness in knowledge editing. To overcome this, inspired by\nLLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy\ncalled Learn to Inference (LTI), which introduce a Multi-stage Inference\nConstraint module to guide the edited models in recalling new knowledge\nsimilarly to how unedited LLMs leverage knowledge through in-context learning.\nExtensive experimental results across a wide range of tasks validate the\neffectiveness of LTI in mitigating Editing Overfit."
                },
                "authors": [
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06930v2",
                "updated": "2024-10-10T11:03:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    3,
                    20,
                    3,
                    284,
                    0
                ],
                "published": "2024-01-12T23:33:01Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    23,
                    33,
                    1,
                    4,
                    12,
                    0
                ],
                "title": "PizzaCommonSense: Learning to Model Commonsense Reasoning about\n  Intermediate Steps in Cooking Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PizzaCommonSense: Learning to Model Commonsense Reasoning about\n  Intermediate Steps in Cooking Recipes"
                },
                "summary": "Understanding procedural texts, such as cooking recipes, is essential for\nenabling machines to follow instructions and reason about tasks, a key aspect\nof intelligent reasoning. In cooking, these instructions can be interpreted as\na series of modifications to a food preparation. For a model to effectively\nreason about cooking recipes, it must accurately discern and understand the\ninputs and outputs of intermediate steps within the recipe. We present a new\ncorpus of cooking recipes enriched with descriptions of intermediate steps that\ndescribe the input and output for each step. PizzaCommonsense serves as a\nbenchmark for the reasoning capabilities of LLMs because it demands rigorous\nexplicit input-output descriptions to demonstrate the acquisition of implicit\ncommonsense knowledge, which is unlikely to be easily memorized. GPT-4 achieves\nonly 26\\% human-evaluated preference for generations, leaving room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding procedural texts, such as cooking recipes, is essential for\nenabling machines to follow instructions and reason about tasks, a key aspect\nof intelligent reasoning. In cooking, these instructions can be interpreted as\na series of modifications to a food preparation. For a model to effectively\nreason about cooking recipes, it must accurately discern and understand the\ninputs and outputs of intermediate steps within the recipe. We present a new\ncorpus of cooking recipes enriched with descriptions of intermediate steps that\ndescribe the input and output for each step. PizzaCommonsense serves as a\nbenchmark for the reasoning capabilities of LLMs because it demands rigorous\nexplicit input-output descriptions to demonstrate the acquisition of implicit\ncommonsense knowledge, which is unlikely to be easily memorized. GPT-4 achieves\nonly 26\\% human-evaluated preference for generations, leaving room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "arxiv_comment": "Findings of EMNLP 2024. The data is available at:\n  https://github.com/adiallo07/PizzaCommonsense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07815v1",
                "updated": "2024-10-10T11:00:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    0,
                    55,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    0,
                    55,
                    3,
                    284,
                    0
                ],
                "title": "Simple ReFlow: Improved Techniques for Fast Flow Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple ReFlow: Improved Techniques for Fast Flow Models"
                },
                "summary": "Diffusion and flow-matching models achieve remarkable generative performance\nbut at the cost of many sampling steps, this slows inference and limits\napplicability to time-critical tasks. The ReFlow procedure can accelerate\nsampling by straightening generation trajectories. However, ReFlow is an\niterative procedure, typically requiring training on simulated data, and\nresults in reduced sample quality. To mitigate sample deterioration, we examine\nthe design space of ReFlow and highlight potential pitfalls in prior heuristic\npractices. We then propose seven improvements for training dynamics, learning\nand inference, which are verified with thorough ablation studies on CIFAR10 $32\n\\times 32$, AFHQv2 $64 \\times 64$, and FFHQ $64 \\times 64$. Combining all our\ntechniques, we achieve state-of-the-art FID scores (without / with guidance,\nresp.) for fast generation via neural ODEs: $2.23$ / $1.98$ on CIFAR10, $2.30$\n/ $1.91$ on AFHQv2, $2.84$ / $2.67$ on FFHQ, and $3.49$ / $1.74$ on\nImageNet-64, all with merely $9$ neural function evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion and flow-matching models achieve remarkable generative performance\nbut at the cost of many sampling steps, this slows inference and limits\napplicability to time-critical tasks. The ReFlow procedure can accelerate\nsampling by straightening generation trajectories. However, ReFlow is an\niterative procedure, typically requiring training on simulated data, and\nresults in reduced sample quality. To mitigate sample deterioration, we examine\nthe design space of ReFlow and highlight potential pitfalls in prior heuristic\npractices. We then propose seven improvements for training dynamics, learning\nand inference, which are verified with thorough ablation studies on CIFAR10 $32\n\\times 32$, AFHQv2 $64 \\times 64$, and FFHQ $64 \\times 64$. Combining all our\ntechniques, we achieve state-of-the-art FID scores (without / with guidance,\nresp.) for fast generation via neural ODEs: $2.23$ / $1.98$ on CIFAR10, $2.30$\n/ $1.91$ on AFHQv2, $2.84$ / $2.67$ on FFHQ, and $3.49$ / $1.74$ on\nImageNet-64, all with merely $9$ neural function evaluations."
                },
                "authors": [
                    {
                        "name": "Beomsu Kim"
                    },
                    {
                        "name": "Yu-Guan Hsieh"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Jong Chul Ye"
                    },
                    {
                        "name": "Bahjat Kawar"
                    },
                    {
                        "name": "James Thornton"
                    }
                ],
                "author_detail": {
                    "name": "James Thornton"
                },
                "author": "James Thornton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17183v2",
                "updated": "2024-10-10T11:00:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    0,
                    2,
                    3,
                    284,
                    0
                ],
                "published": "2024-08-30T10:34:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    34,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "Causal Reasoning in Software Quality Assurance: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning in Software Quality Assurance: A Systematic Review"
                },
                "summary": "Context: Software Quality Assurance (SQA) is a fundamental part of software\nengineering to ensure stakeholders that software products work as expected\nafter release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software\nsystems. In this context, Causal Reasoning is gaining increasing interest as a\nmethodology to go beyond a purely data-driven approach by exploiting the use of\ncausality for more effective SQA strategies. Objective: Provide a broad and\ndetailed overview of the use of causal reasoning for SQA activities, in order\nto support researchers to access this research field, identifying room for\napplication, main challenges and research opportunities. Methods: A systematic\nreview of the scientific literature on causal reasoning for SQA. The study has\nfound, classified, and analyzed 86 articles, according to established\nguidelines for software engineering secondary studies. Results: Results\nhighlight the primary areas within SQA where causal reasoning has been applied,\nthe predominant methodologies used, and the level of maturity of the proposed\nsolutions. Fault localization is the activity where causal reasoning is more\nexploited, especially in the web services/microservices domain, but other tasks\nlike testing are rapidly gaining popularity. Both causal inference and causal\ndiscovery are exploited, with the Pearl's graphical formulation of causality\nbeing preferred, likely due to its intuitiveness. Tools to favour their\napplication are appearing at a fast pace - most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for\nSQA tasks with respect to multiple quality attributes, especially during V&V,\nevolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Software Quality Assurance (SQA) is a fundamental part of software\nengineering to ensure stakeholders that software products work as expected\nafter release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software\nsystems. In this context, Causal Reasoning is gaining increasing interest as a\nmethodology to go beyond a purely data-driven approach by exploiting the use of\ncausality for more effective SQA strategies. Objective: Provide a broad and\ndetailed overview of the use of causal reasoning for SQA activities, in order\nto support researchers to access this research field, identifying room for\napplication, main challenges and research opportunities. Methods: A systematic\nreview of the scientific literature on causal reasoning for SQA. The study has\nfound, classified, and analyzed 86 articles, according to established\nguidelines for software engineering secondary studies. Results: Results\nhighlight the primary areas within SQA where causal reasoning has been applied,\nthe predominant methodologies used, and the level of maturity of the proposed\nsolutions. Fault localization is the activity where causal reasoning is more\nexploited, especially in the web services/microservices domain, but other tasks\nlike testing are rapidly gaining popularity. Both causal inference and causal\ndiscovery are exploited, with the Pearl's graphical formulation of causality\nbeing preferred, likely due to its intuitiveness. Tools to favour their\napplication are appearing at a fast pace - most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for\nSQA tasks with respect to multiple quality attributes, especially during V&V,\nevolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like ..."
                },
                "authors": [
                    {
                        "name": "Luca Giamattei"
                    },
                    {
                        "name": "Antonio Guerriero"
                    },
                    {
                        "name": "Roberto Pietrantuono"
                    },
                    {
                        "name": "Stefano Russo"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Russo"
                },
                "author": "Stefano Russo",
                "arxiv_comment": "Accepted to Information and Software Technology journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.08202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08202v1",
                "updated": "2024-10-10T17:59:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:59:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%."
                },
                "authors": [
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xizhou Zhu"
                },
                "author": "Xizhou Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08197v1",
                "updated": "2024-10-10T17:58:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:58:44Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    44,
                    3,
                    284,
                    0
                ],
                "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions"
                },
                "summary": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08193v1",
                "updated": "2024-10-10T17:58:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:58:24Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    58,
                    24,
                    3,
                    284,
                    0
                ],
                "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining."
                },
                "authors": [
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Alec Koppel"
                    },
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Ganesh"
                },
                "author": "Sumitra Ganesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08189v1",
                "updated": "2024-10-10T17:57:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    57,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:57:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    57,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object\n  Navigation"
                },
                "summary": "In this paper, we propose a new framework for zero-shot object navigation.\nExisting zero-shot object navigation methods prompt LLM with the text of\nspatially closed objects, which lacks enough scene context for in-depth\nreasoning. To better preserve the information of environment and fully exploit\nthe reasoning ability of LLM, we propose to represent the observed scene with\n3D scene graph. The scene graph encodes the relationships between objects,\ngroups and rooms with a LLM-friendly structure, for which we design a\nhierarchical chain-of-thought prompt to help LLM reason the goal location\naccording to scene context by traversing the nodes and edges. Moreover, benefit\nfrom the scene graph representation, we further design a re-perception\nmechanism to empower the object navigation framework with the ability to\ncorrect perception error. We conduct extensive experiments on MP3D, HM3D and\nRoboTHOR environments, where SG-Nav surpasses previous state-of-the-art\nzero-shot methods by more than 10% SR on all benchmarks, while the decision\nprocess is explainable. To the best of our knowledge, SG-Nav is the first\nzero-shot method that achieves even higher performance than supervised object\nnavigation methods on the challenging MP3D benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new framework for zero-shot object navigation.\nExisting zero-shot object navigation methods prompt LLM with the text of\nspatially closed objects, which lacks enough scene context for in-depth\nreasoning. To better preserve the information of environment and fully exploit\nthe reasoning ability of LLM, we propose to represent the observed scene with\n3D scene graph. The scene graph encodes the relationships between objects,\ngroups and rooms with a LLM-friendly structure, for which we design a\nhierarchical chain-of-thought prompt to help LLM reason the goal location\naccording to scene context by traversing the nodes and edges. Moreover, benefit\nfrom the scene graph representation, we further design a re-perception\nmechanism to empower the object navigation framework with the ability to\ncorrect perception error. We conduct extensive experiments on MP3D, HM3D and\nRoboTHOR environments, where SG-Nav surpasses previous state-of-the-art\nzero-shot methods by more than 10% SR on all benchmarks, while the decision\nprocess is explainable. To the best of our knowledge, SG-Nav is the first\nzero-shot method that achieves even higher performance than supervised object\nnavigation methods on the challenging MP3D benchmark."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to NeurIPS 2024. Project page:\n  https://bagh2178.github.io/SG-Nav/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08146v1",
                "updated": "2024-10-10T17:31:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:31:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning"
                },
                "summary": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Xinyang Geng"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Alekh Agarwal"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08145v1",
                "updated": "2024-10-10T17:31:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    17,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:31:17Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    31,
                    17,
                    3,
                    284,
                    0
                ],
                "title": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs"
                },
                "summary": "This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08143v1",
                "updated": "2024-10-10T17:30:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:30:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory"
                },
                "summary": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\ntranslation accuracy, and the summary component of the agent also shows promise\nas a tool for query-based summarization tasks. We release our code and data at\nhttps://github.com/YutongWang1216/DocMTAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\ntranslation accuracy, and the summary component of the agent also shows promise\nas a tool for query-based summarization tasks. We release our code and data at\nhttps://github.com/YutongWang1216/DocMTAgent."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19580v2",
                "updated": "2024-10-10T17:25:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    25,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-28T20:39:16Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    20,
                    39,
                    16,
                    6,
                    210,
                    0
                ],
                "title": "Mini-batch Coresets for Memory-efficient Training of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-batch Coresets for Memory-efficient Training of Large Language\n  Models"
                },
                "summary": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced nature of the sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V -projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr with\nLoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the\nmemory requirement of fine-tuning by 2x and even outperforms training with 4x\nlarger mini-batches. Notably, CoLM easily stack with existing memory-efficient\ntraining methods, such as LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced nature of the sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V -projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr with\nLoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the\nmemory requirement of fine-tuning by 2x and even outperforms training with 4x\nlarger mini-batches. Notably, CoLM easily stack with existing memory-efficient\ntraining methods, such as LoRA."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Rathul Anand"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Baharan Mirzasoleiman"
                    }
                ],
                "author_detail": {
                    "name": "Baharan Mirzasoleiman"
                },
                "author": "Baharan Mirzasoleiman",
                "arxiv_comment": "18 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v2",
                "updated": "2024-10-10T17:24:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    24,
                    1,
                    3,
                    284,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08133v1",
                "updated": "2024-10-10T17:17:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    17,
                    38,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:17:38Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    17,
                    38,
                    3,
                    284,
                    0
                ],
                "title": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks"
                },
                "summary": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models."
                },
                "authors": [
                    {
                        "name": "Mathis Pink"
                    },
                    {
                        "name": "Vy A. Vo"
                    },
                    {
                        "name": "Qinyuan Wu"
                    },
                    {
                        "name": "Jianing Mu"
                    },
                    {
                        "name": "Javier S. Turek"
                    },
                    {
                        "name": "Uri Hasson"
                    },
                    {
                        "name": "Kenneth A. Norman"
                    },
                    {
                        "name": "Sebastian Michelmann"
                    },
                    {
                        "name": "Alexander Huth"
                    },
                    {
                        "name": "Mariya Toneva"
                    }
                ],
                "author_detail": {
                    "name": "Mariya Toneva"
                },
                "author": "Mariya Toneva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08130v1",
                "updated": "2024-10-10T17:14:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:14:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning"
                },
                "summary": "This paper presents Dynamic Prompting, a novel framework aimed at improving\nthe reasoning capabilities of Large Language Models (LLMs). In contrast to\nconventional static prompting methods, Dynamic Prompting enables the adaptive\nmodification of prompt sequences and step counts based on real-time task\ncomplexity and model performance. This dynamic adaptation facilitates more\nefficient problem-solving, particularly in smaller models, by reducing\nhallucinations and repetitive cycles. Our empirical evaluations demonstrate\nthat Dynamic Prompting allows smaller LLMs to perform competitively with much\nlarger models, thereby challenging the conventional emphasis on model size as\nthe primary determinant of reasoning efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Dynamic Prompting, a novel framework aimed at improving\nthe reasoning capabilities of Large Language Models (LLMs). In contrast to\nconventional static prompting methods, Dynamic Prompting enables the adaptive\nmodification of prompt sequences and step counts based on real-time task\ncomplexity and model performance. This dynamic adaptation facilitates more\nefficient problem-solving, particularly in smaller models, by reducing\nhallucinations and repetitive cycles. Our empirical evaluations demonstrate\nthat Dynamic Prompting allows smaller LLMs to perform competitively with much\nlarger models, thereby challenging the conventional emphasis on model size as\nthe primary determinant of reasoning efficacy."
                },
                "authors": [
                    {
                        "name": "Kamesh R"
                    }
                ],
                "author_detail": {
                    "name": "Kamesh R"
                },
                "author": "Kamesh R",
                "arxiv_comment": "Submitted to ICLR 2025. This is a preprint version. Future revisions\n  will include additional evaluations and refinements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08126v1",
                "updated": "2024-10-10T17:10:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    10,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Mars: Situated Inductive Reasoning in an Open-World Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mars: Situated Inductive Reasoning in an Open-World Environment"
                },
                "summary": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Song-chun Zhu"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04640v2",
                "updated": "2024-10-10T17:09:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    9,
                    24,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-06T22:13:30Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    30,
                    6,
                    280,
                    0
                ],
                "title": "Unpacking Failure Modes of Generative Policies: Runtime Monitoring of\n  Consistency and Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unpacking Failure Modes of Generative Policies: Runtime Monitoring of\n  Consistency and Progress"
                },
                "summary": "Robot behavior policies trained via imitation learning are prone to failure\nunder conditions that deviate from their training data. Thus, algorithms that\nmonitor learned policies at test time and provide early warnings of failure are\nnecessary to facilitate scalable deployment. We propose Sentinel, a runtime\nmonitoring framework that splits the detection of failures into two\ncomplementary categories: 1) Erratic failures, which we detect using\nstatistical measures of temporal action consistency, and 2) task progression\nfailures, where we use Vision Language Models (VLMs) to detect when the policy\nconfidently and consistently takes actions that do not solve the task. Our\napproach has two key strengths. First, because learned policies exhibit diverse\nfailure modes, combining complementary detectors leads to significantly higher\naccuracy at failure detection. Second, using a statistical temporal action\nconsistency measure ensures that we quickly detect when multimodal, generative\npolicies exhibit erratic behavior at negligible computational cost. In\ncontrast, we only use VLMs to detect failure modes that are less\ntime-sensitive. We demonstrate our approach in the context of diffusion\npolicies trained on robotic mobile manipulation domains in both simulation and\nthe real world. By unifying temporal consistency detection and VLM runtime\nmonitoring, Sentinel detects 18% more failures than using either of the two\ndetectors alone and significantly outperforms baselines, thus highlighting the\nimportance of assigning specialized detectors to complementary categories of\nfailure. Qualitative results are made available at\nhttps://sites.google.com/stanford.edu/sentinel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot behavior policies trained via imitation learning are prone to failure\nunder conditions that deviate from their training data. Thus, algorithms that\nmonitor learned policies at test time and provide early warnings of failure are\nnecessary to facilitate scalable deployment. We propose Sentinel, a runtime\nmonitoring framework that splits the detection of failures into two\ncomplementary categories: 1) Erratic failures, which we detect using\nstatistical measures of temporal action consistency, and 2) task progression\nfailures, where we use Vision Language Models (VLMs) to detect when the policy\nconfidently and consistently takes actions that do not solve the task. Our\napproach has two key strengths. First, because learned policies exhibit diverse\nfailure modes, combining complementary detectors leads to significantly higher\naccuracy at failure detection. Second, using a statistical temporal action\nconsistency measure ensures that we quickly detect when multimodal, generative\npolicies exhibit erratic behavior at negligible computational cost. In\ncontrast, we only use VLMs to detect failure modes that are less\ntime-sensitive. We demonstrate our approach in the context of diffusion\npolicies trained on robotic mobile manipulation domains in both simulation and\nthe real world. By unifying temporal consistency detection and VLM runtime\nmonitoring, Sentinel detects 18% more failures than using either of the two\ndetectors alone and significantly outperforms baselines, thus highlighting the\nimportance of assigning specialized detectors to complementary categories of\nfailure. Qualitative results are made available at\nhttps://sites.google.com/stanford.edu/sentinel."
                },
                "authors": [
                    {
                        "name": "Christopher Agia"
                    },
                    {
                        "name": "Rohan Sinha"
                    },
                    {
                        "name": "Jingyun Yang"
                    },
                    {
                        "name": "Zi-ang Cao"
                    },
                    {
                        "name": "Rika Antonova"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Jeannette Bohg"
                    }
                ],
                "author_detail": {
                    "name": "Jeannette Bohg"
                },
                "author": "Jeannette Bohg",
                "arxiv_comment": "Project page: https://sites.google.com/stanford.edu/sentinel. 35\n  pages, 9 figures. Accepted to the Conference on Robot Learning (CoRL) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.2.9; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08115v1",
                "updated": "2024-10-10T17:00:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    0,
                    6,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T17:00:06Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    0,
                    6,
                    3,
                    284,
                    0
                ],
                "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page)."
                },
                "authors": [
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Jiarui Yuan"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09020v2",
                "updated": "2024-10-10T16:58:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    58,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2023-11-15T15:08:38Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    15,
                    8,
                    38,
                    2,
                    319,
                    0
                ],
                "title": "Explaining Explanation: An Empirical Study on Explanation in Code\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Explanation: An Empirical Study on Explanation in Code\n  Reviews"
                },
                "summary": "Code reviews are central for software quality assurance. Ideally, reviewers\nshould explain their feedback to enable authors of code changes to understand\nthe feedback and act accordingly. Different developers might need different\nexplanations in different contexts. Therefore, assisting this process first\nrequires understanding the types of explanations reviewers usually provide. The\ngoal of this paper is to study the types of explanations used in code reviews\nand explore the potential of Large Language Models (LLMs), specifically\nChatGPT, in generating these specific types. We extracted 793 code review\ncomments from Gerrit and manually labeled them based on whether they contained\na suggestion, an explanation, or both. Our analysis shows that 42% of comments\nonly include suggestions without explanations. We categorized the explanations\ninto seven distinct types including rule or principle, similar examples, and\nfuture implications. When measuring their prevalence, we observed that some\nexplanations are used differently by novice and experienced reviewers. Our\nmanual evaluation shows that, when the explanation type is specified, ChatGPT\ncan correctly generate the explanation in 88 out of 90 cases. This foundational\nwork highlights the potential for future automation in code reviews, which can\nassist developers in sharing and obtaining different types of explanations as\nneeded, thereby reducing back-and-forth communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code reviews are central for software quality assurance. Ideally, reviewers\nshould explain their feedback to enable authors of code changes to understand\nthe feedback and act accordingly. Different developers might need different\nexplanations in different contexts. Therefore, assisting this process first\nrequires understanding the types of explanations reviewers usually provide. The\ngoal of this paper is to study the types of explanations used in code reviews\nand explore the potential of Large Language Models (LLMs), specifically\nChatGPT, in generating these specific types. We extracted 793 code review\ncomments from Gerrit and manually labeled them based on whether they contained\na suggestion, an explanation, or both. Our analysis shows that 42% of comments\nonly include suggestions without explanations. We categorized the explanations\ninto seven distinct types including rule or principle, similar examples, and\nfuture implications. When measuring their prevalence, we observed that some\nexplanations are used differently by novice and experienced reviewers. Our\nmanual evaluation shows that, when the explanation type is specified, ChatGPT\ncan correctly generate the explanation in 88 out of 90 cases. This foundational\nwork highlights the potential for future automation in code reviews, which can\nassist developers in sharing and obtaining different types of explanations as\nneeded, thereby reducing back-and-forth communication."
                },
                "authors": [
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Abir Bouraffa"
                    },
                    {
                        "name": "Walid Maalej"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08111v1",
                "updated": "2024-10-10T16:57:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    1,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    1,
                    3,
                    284,
                    0
                ],
                "title": "Active Fourier Auditor for Estimating Distributional Properties of ML\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Fourier Auditor for Estimating Distributional Properties of ML\n  Models"
                },
                "summary": "With the pervasive deployment of Machine Learning (ML) models in real-world\napplications, verifying and auditing properties of ML models have become a\ncentral concern. In this work, we focus on three properties: robustness,\nindividual fairness, and group fairness. We discuss two approaches for auditing\nML model properties: estimation with and without reconstruction of the target\nmodel under audit. Though the first approach is studied in the literature, the\nsecond approach remains unexplored. For this purpose, we develop a new\nframework that quantifies different properties in terms of the Fourier\ncoefficients of the ML model under audit but does not parametrically\nreconstruct it. We propose the Active Fourier Auditor (AFA), which queries\nsample points according to the Fourier coefficients of the ML model, and\nfurther estimates the properties. We derive high probability error bounds on\nAFA's estimates, along with the worst-case lower bounds on the sample\ncomplexity to audit them. Numerically we demonstrate on multiple datasets and\nmodels that AFA is more accurate and sample-efficient to estimate the\nproperties of interest than the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the pervasive deployment of Machine Learning (ML) models in real-world\napplications, verifying and auditing properties of ML models have become a\ncentral concern. In this work, we focus on three properties: robustness,\nindividual fairness, and group fairness. We discuss two approaches for auditing\nML model properties: estimation with and without reconstruction of the target\nmodel under audit. Though the first approach is studied in the literature, the\nsecond approach remains unexplored. For this purpose, we develop a new\nframework that quantifies different properties in terms of the Fourier\ncoefficients of the ML model under audit but does not parametrically\nreconstruct it. We propose the Active Fourier Auditor (AFA), which queries\nsample points according to the Fourier coefficients of the ML model, and\nfurther estimates the properties. We derive high probability error bounds on\nAFA's estimates, along with the worst-case lower bounds on the sample\ncomplexity to audit them. Numerically we demonstrate on multiple datasets and\nmodels that AFA is more accurate and sample-efficient to estimate the\nproperties of interest than the baselines."
                },
                "authors": [
                    {
                        "name": "Ayoub Ajarra"
                    },
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Debabrota Basu"
                    }
                ],
                "author_detail": {
                    "name": "Debabrota Basu"
                },
                "author": "Debabrota Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08109v1",
                "updated": "2024-10-10T16:56:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    56,
                    5,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    56,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "A Closer Look at Machine Unlearning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at Machine Unlearning for Large Language Models"
                },
                "summary": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning."
                },
                "authors": [
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08105v1",
                "updated": "2024-10-10T16:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    53,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    53,
                    10,
                    3,
                    284,
                    0
                ],
                "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?"
                },
                "summary": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation."
                },
                "authors": [
                    {
                        "name": "Kunhao Zheng"
                    },
                    {
                        "name": "Juliette Decugis"
                    },
                    {
                        "name": "Jonas Gehring"
                    },
                    {
                        "name": "Taco Cohen"
                    },
                    {
                        "name": "Benjamin Negrevergne"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08102v1",
                "updated": "2024-10-10T16:45:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:45:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    45,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining"
                },
                "summary": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhen Hao Wong"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Qiu Jiantao"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05292v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05292v5",
                "updated": "2024-10-10T16:29:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    59,
                    3,
                    284,
                    0
                ],
                "published": "2023-10-08T21:39:47Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    21,
                    39,
                    47,
                    6,
                    281,
                    0
                ],
                "title": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent\n  for Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent\n  for Debugging"
                },
                "summary": "Large Language Models (LLMs) now excel at generative skills and can create\ncontent at impeccable speeds. However, they are imperfect and still make\nvarious mistakes. In a Computer Science education context, as these models are\nwidely recognized as \"AI pair programmers,\" it becomes increasingly important\nto train students on evaluating and debugging the LLM-generated code. In this\nwork, we introduce HypoCompass, a novel system to facilitate deliberate\npractice on debugging, where human novices play the role of Teaching Assistants\nand help LLM-powered teachable agents debug code. We enable effective task\ndelegation between students and LLMs in this learning-by-teaching environment:\nstudents focus on hypothesizing the cause of code errors, while adjacent skills\nlike code completion are offloaded to LLM-agents. Our evaluations demonstrate\nthat HypoCompass generates high-quality training materials (e.g., bugs and\nfixes), outperforming human counterparts fourfold in efficiency, and\nsignificantly improves student performance on debugging by 12% in the\npre-to-post test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) now excel at generative skills and can create\ncontent at impeccable speeds. However, they are imperfect and still make\nvarious mistakes. In a Computer Science education context, as these models are\nwidely recognized as \"AI pair programmers,\" it becomes increasingly important\nto train students on evaluating and debugging the LLM-generated code. In this\nwork, we introduce HypoCompass, a novel system to facilitate deliberate\npractice on debugging, where human novices play the role of Teaching Assistants\nand help LLM-powered teachable agents debug code. We enable effective task\ndelegation between students and LLMs in this learning-by-teaching environment:\nstudents focus on hypothesizing the cause of code errors, while adjacent skills\nlike code completion are offloaded to LLM-agents. Our evaluations demonstrate\nthat HypoCompass generates high-quality training materials (e.g., bugs and\nfixes), outperforming human counterparts fourfold in efficiency, and\nsignificantly improves student performance on debugging by 12% in the\npre-to-post test."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_doi": "10.1007/978-3-031-64302-6_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-64302-6_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.05292v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05292v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures",
                "arxiv_journal_ref": "AIED 2024, LNAI 14829, pp. 1-16",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08085v1",
                "updated": "2024-10-10T16:29:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    21,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:29:21Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    29,
                    21,
                    3,
                    284,
                    0
                ],
                "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study over Open-ended Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study over Open-ended Question Answering"
                },
                "summary": "Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks mainly focus on closed tasks, leaving a gap in the\nassessment of more complex, real-world scenarios. This gap has also obscured\nthe evaluation of KGs' potential to mitigate the problem of hallucination in\nLLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically\ndesigned to assess LLMs enhanced with KGs under open-ended, real-world question\nanswering scenarios. OKGQA is designed to closely reflect the complexities of\npractical applications using questions from different types, and incorporates\nspecific metrics to measure both the reduction in hallucinations and the\nenhancement in reasoning capabilities. To consider the scenario in which KGs\nmay have varying levels of mistakes, we further propose another experiment\nsetting OKGQA-P to assess model performance when the semantics and structure of\nKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore\nwhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)\nconduct a comparative analysis to shed light on methods and future directions\nfor leveraging KGs to reduce LLMs' hallucination. We believe that this study\ncan facilitate a more complete performance comparison and encourage continuous\nimprovement in integrating KGs with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks mainly focus on closed tasks, leaving a gap in the\nassessment of more complex, real-world scenarios. This gap has also obscured\nthe evaluation of KGs' potential to mitigate the problem of hallucination in\nLLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically\ndesigned to assess LLMs enhanced with KGs under open-ended, real-world question\nanswering scenarios. OKGQA is designed to closely reflect the complexities of\npractical applications using questions from different types, and incorporates\nspecific metrics to measure both the reduction in hallucinations and the\nenhancement in reasoning capabilities. To consider the scenario in which KGs\nmay have varying levels of mistakes, we further propose another experiment\nsetting OKGQA-P to assess model performance when the semantics and structure of\nKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore\nwhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)\nconduct a comparative analysis to shed light on methods and future directions\nfor leveraging KGs to reduce LLMs' hallucination. We believe that this study\ncan facilitate a more complete performance comparison and encourage continuous\nimprovement in integrating KGs with LLMs."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18034v2",
                "updated": "2024-10-10T16:19:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    19,
                    59,
                    3,
                    284,
                    0
                ],
                "published": "2024-01-31T17:58:10Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    17,
                    58,
                    10,
                    2,
                    31,
                    0
                ],
                "title": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages"
                },
                "summary": "We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11915v2",
                "updated": "2024-10-10T16:13:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    13,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-16T21:11:23Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    21,
                    11,
                    23,
                    6,
                    168,
                    0
                ],
                "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "miniCodeProps: a Minimal Benchmark for Proving Code Properties"
                },
                "summary": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode."
                },
                "authors": [
                    {
                        "name": "Evan Lohn"
                    },
                    {
                        "name": "Sean Welleck"
                    }
                ],
                "author_detail": {
                    "name": "Sean Welleck"
                },
                "author": "Sean Welleck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08068v1",
                "updated": "2024-10-10T16:02:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    2,
                    36,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:02:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    2,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting."
                },
                "authors": [
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Dongxiao Chen"
                    },
                    {
                        "name": "Jieting Xue"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Taijie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taijie Chen"
                },
                "author": "Taijie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v1",
                "updated": "2024-10-10T16:01:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17665v3",
                "updated": "2024-10-10T15:55:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    55,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-27T21:33:56Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    33,
                    56,
                    0,
                    148,
                    0
                ],
                "title": "Improving Robotic Arms through Natural Language Processing, Computer\n  Vision, and Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Robotic Arms through Natural Language Processing, Computer\n  Vision, and Edge Computing"
                },
                "summary": "This paper introduces a prototype for a new approach to assistive robotics,\nintegrating edge computing with Natural Language Processing (NLP) and computer\nvision to enhance the interaction between humans and robotic systems. Our proof\nof concept demonstrates the feasibility of using large language models (LLMs)\nand vision systems in tandem for interpreting and executing complex commands\nconveyed through natural language. This integration aims to improve the\nintuitiveness and accessibility of assistive robotic systems, making them more\nadaptable to the nuanced needs of users with disabilities. By leveraging the\ncapabilities of edge computing, our system has the potential to minimize\nlatency and support offline capability, enhancing the autonomy and\nresponsiveness of assistive robots. Experimental results from our\nimplementation on a robotic arm show promising outcomes in terms of accurate\nintent interpretation and object manipulation based on verbal commands. This\nresearch lays the groundwork for future developments in assistive robotics,\nfocusing on creating highly responsive, user-centric systems that can\nsignificantly improve the quality of life for individuals with disabilities.\nFor video demonstrations and source code, please refer to:\nhttps://tinyurl.com/EnhancedArmEdgeNLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a prototype for a new approach to assistive robotics,\nintegrating edge computing with Natural Language Processing (NLP) and computer\nvision to enhance the interaction between humans and robotic systems. Our proof\nof concept demonstrates the feasibility of using large language models (LLMs)\nand vision systems in tandem for interpreting and executing complex commands\nconveyed through natural language. This integration aims to improve the\nintuitiveness and accessibility of assistive robotic systems, making them more\nadaptable to the nuanced needs of users with disabilities. By leveraging the\ncapabilities of edge computing, our system has the potential to minimize\nlatency and support offline capability, enhancing the autonomy and\nresponsiveness of assistive robots. Experimental results from our\nimplementation on a robotic arm show promising outcomes in terms of accurate\nintent interpretation and object manipulation based on verbal commands. This\nresearch lays the groundwork for future developments in assistive robotics,\nfocusing on creating highly responsive, user-centric systems that can\nsignificantly improve the quality of life for individuals with disabilities.\nFor video demonstrations and source code, please refer to:\nhttps://tinyurl.com/EnhancedArmEdgeNLP."
                },
                "authors": [
                    {
                        "name": "Pascal Sikorski"
                    },
                    {
                        "name": "Kaleb Yu"
                    },
                    {
                        "name": "Lucy Billadeau"
                    },
                    {
                        "name": "Flavio Esposito"
                    },
                    {
                        "name": "Hadi AliAkbarpour"
                    },
                    {
                        "name": "Madi Babaiasl"
                    }
                ],
                "author_detail": {
                    "name": "Madi Babaiasl"
                },
                "author": "Madi Babaiasl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14438v2",
                "updated": "2024-10-10T15:55:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    55,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-23T11:10:32Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    10,
                    32,
                    3,
                    144,
                    0
                ],
                "title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks"
                },
                "summary": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets."
                },
                "authors": [
                    {
                        "name": "Michelle Halbheer"
                    },
                    {
                        "name": "Dominik J. Mhlematter"
                    },
                    {
                        "name": "Alexander Becker"
                    },
                    {
                        "name": "Dominik Narnhofer"
                    },
                    {
                        "name": "Helge Aasen"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Mehmet Ozgur Turkoglu"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Ozgur Turkoglu"
                },
                "author": "Mehmet Ozgur Turkoglu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17026v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17026v4",
                "updated": "2024-10-10T15:51:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    51,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2023-05-26T15:35:43Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    15,
                    35,
                    43,
                    4,
                    146,
                    0
                ],
                "title": "How Powerful are Decoder-Only Transformer Neural Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Powerful are Decoder-Only Transformer Neural Models?"
                },
                "summary": "In this article we prove that the general transformer neural model\nundergirding modern large language models (LLMs) is Turing complete under\nreasonable assumptions. This is the first work to directly address the Turing\ncompleteness of the underlying technology employed in GPT-x as past work has\nfocused on the more expressive, full auto-encoder transformer architecture.\nFrom this theoretical analysis, we show that the sparsity/compressibility of\nthe word embedding is an important consideration for Turing completeness to\nhold. We also show that Transformers are are a variant of B machines studied by\nHao Wang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we prove that the general transformer neural model\nundergirding modern large language models (LLMs) is Turing complete under\nreasonable assumptions. This is the first work to directly address the Turing\ncompleteness of the underlying technology employed in GPT-x as past work has\nfocused on the more expressive, full auto-encoder transformer architecture.\nFrom this theoretical analysis, we show that the sparsity/compressibility of\nthe word embedding is an important consideration for Turing completeness to\nhold. We also show that Transformers are are a variant of B machines studied by\nHao Wang."
                },
                "authors": [
                    {
                        "name": "Jesse Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Roberts"
                },
                "author": "Jesse Roberts",
                "arxiv_doi": "10.1109/IJCNN60899.2024.10651286",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN60899.2024.10651286",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.17026v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17026v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IJCNN 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08053v1",
                "updated": "2024-10-10T15:46:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    46,
                    27,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:46:27Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    46,
                    27,
                    3,
                    284,
                    0
                ],
                "title": "A Target-Aware Analysis of Data Augmentation for Hate Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Target-Aware Analysis of Data Augmentation for Hate Speech Detection"
                },
                "summary": "Hate speech is one of the main threats posed by the widespread use of social\nnetworks, despite efforts to limit it. Although attention has been devoted to\nthis issue, the lack of datasets and case studies centered around scarcely\nrepresented phenomena, such as ableism or ageism, can lead to hate speech\ndetection systems that do not perform well on underrepresented identity groups.\nGiven the unpreceded capabilities of LLMs in producing high-quality data, we\ninvestigate the possibility of augmenting existing data with generative\nlanguage models, reducing target imbalance. We experiment with augmenting 1,000\nposts from the Measuring Hate Speech corpus, an English dataset annotated with\ntarget identity information, adding around 30,000 synthetic examples using both\nsimple data augmentation methods and different types of generative models,\ncomparing autoregressive and sequence-to-sequence approaches. We find\ntraditional DA methods to often be preferable to generative models, but the\ncombination of the two tends to lead to the best results. Indeed, for some hate\ncategories such as origin, religion, and disability, hate speech classification\nusing augmented data for training improves by more than 10% F1 over the no\naugmentation baseline. This work contributes to the development of systems for\nhate speech detection that are not only better performing but also fairer and\nmore inclusive towards targets that have been neglected so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech is one of the main threats posed by the widespread use of social\nnetworks, despite efforts to limit it. Although attention has been devoted to\nthis issue, the lack of datasets and case studies centered around scarcely\nrepresented phenomena, such as ableism or ageism, can lead to hate speech\ndetection systems that do not perform well on underrepresented identity groups.\nGiven the unpreceded capabilities of LLMs in producing high-quality data, we\ninvestigate the possibility of augmenting existing data with generative\nlanguage models, reducing target imbalance. We experiment with augmenting 1,000\nposts from the Measuring Hate Speech corpus, an English dataset annotated with\ntarget identity information, adding around 30,000 synthetic examples using both\nsimple data augmentation methods and different types of generative models,\ncomparing autoregressive and sequence-to-sequence approaches. We find\ntraditional DA methods to often be preferable to generative models, but the\ncombination of the two tends to lead to the best results. Indeed, for some hate\ncategories such as origin, religion, and disability, hate speech classification\nusing augmented data for training improves by more than 10% F1 over the no\naugmentation baseline. This work contributes to the development of systems for\nhate speech detection that are not only better performing but also fairer and\nmore inclusive towards targets that have been neglected so far."
                },
                "authors": [
                    {
                        "name": "Camilla Casula"
                    },
                    {
                        "name": "Sara Tonelli"
                    }
                ],
                "author_detail": {
                    "name": "Sara Tonelli"
                },
                "author": "Sara Tonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08048v1",
                "updated": "2024-10-10T15:43:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:43:55Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    55,
                    3,
                    284,
                    0
                ],
                "title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers"
                },
                "summary": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains."
                },
                "authors": [
                    {
                        "name": "Jianing Qi"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Zhigang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhigang Zhu"
                },
                "author": "Zhigang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17670v3",
                "updated": "2024-10-10T15:43:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    43,
                    13,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-27T21:48:07Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    48,
                    7,
                    0,
                    148,
                    0
                ],
                "title": "Deployment of Large Language Models to Control Mobile Robots at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of Large Language Models to Control Mobile Robots at the Edge"
                },
                "summary": "This paper investigates the possibility of intuitive human-robot interaction\nthrough the application of Natural Language Processing (NLP) and Large Language\nModels (LLMs) in mobile robotics. This work aims to explore the feasibility of\nusing these technologies for edge-based deployment, where traditional cloud\ndependencies are eliminated. The study specifically contrasts the performance\nof GPT-4-Turbo, which requires cloud connectivity, with an offline-capable,\nquantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). These results show that\nGPT-4-Turbo delivers superior performance in interpreting and executing complex\ncommands accurately, whereas LLaMA 2 exhibits significant limitations in\nconsistency and reliability of command execution. Communication between the\ncontrol computer and the mobile robot is established via a Raspberry Pi Pico W,\nwhich wirelessly receives commands from the computer without internet\ndependency and transmits them through a wired connection to the robot's Arduino\ncontroller. This study highlights the potential and challenges of implementing\nLLMs and NLP at the edge, providing groundwork for future research into fully\nautonomous and network-independent robotic systems. For video demonstrations\nand source code, please refer to: https://tinyurl.com/MobileRobotGPT4LLaMA2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the possibility of intuitive human-robot interaction\nthrough the application of Natural Language Processing (NLP) and Large Language\nModels (LLMs) in mobile robotics. This work aims to explore the feasibility of\nusing these technologies for edge-based deployment, where traditional cloud\ndependencies are eliminated. The study specifically contrasts the performance\nof GPT-4-Turbo, which requires cloud connectivity, with an offline-capable,\nquantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). These results show that\nGPT-4-Turbo delivers superior performance in interpreting and executing complex\ncommands accurately, whereas LLaMA 2 exhibits significant limitations in\nconsistency and reliability of command execution. Communication between the\ncontrol computer and the mobile robot is established via a Raspberry Pi Pico W,\nwhich wirelessly receives commands from the computer without internet\ndependency and transmits them through a wired connection to the robot's Arduino\ncontroller. This study highlights the potential and challenges of implementing\nLLMs and NLP at the edge, providing groundwork for future research into fully\nautonomous and network-independent robotic systems. For video demonstrations\nand source code, please refer to: https://tinyurl.com/MobileRobotGPT4LLaMA2024."
                },
                "authors": [
                    {
                        "name": "Pascal Sikorski"
                    },
                    {
                        "name": "Leendert Schrader"
                    },
                    {
                        "name": "Kaleb Yu"
                    },
                    {
                        "name": "Lucy Billadeau"
                    },
                    {
                        "name": "Jinka Meenakshi"
                    },
                    {
                        "name": "Naveena Mutharasan"
                    },
                    {
                        "name": "Flavio Esposito"
                    },
                    {
                        "name": "Hadi AliAkbarpour"
                    },
                    {
                        "name": "Madi Babaiasl"
                    }
                ],
                "author_detail": {
                    "name": "Madi Babaiasl"
                },
                "author": "Madi Babaiasl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08047v1",
                "updated": "2024-10-10T15:42:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    42,
                    39,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:42:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    42,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning"
                },
                "summary": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Gyeongman Kim"
                    },
                    {
                        "name": "Hyemin S. Lee"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19091v2",
                "updated": "2024-10-10T15:29:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    29,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-27T18:41:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    18,
                    41,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "System-Level Defense against Indirect Prompt Injection Attacks: An\n  Information Flow Control Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-Level Defense against Indirect Prompt Injection Attacks: An\n  Information Flow Control Perspective"
                },
                "summary": "Large Language Model-based systems (LLM systems) are information and query\nprocessing systems that use LLMs to plan operations from natural-language\nprompts and feed the output of each successive step into the LLM to plan the\nnext. This structure results in powerful tools that can process complex\ninformation from diverse sources but raises critical security concerns.\nMalicious information from any source may be processed by the LLM and can\ncompromise the query processing, resulting in nearly arbitrary misbehavior. To\ntackle this problem, we present a system-level defense based on the principles\nof information flow control that we call an f-secure LLM system. An f-secure\nLLM system disaggregates the components of an LLM system into a context-aware\npipeline with dynamically generated structured executable plans, and a security\nmonitor filters out untrusted input into the planning process. This structure\nprevents compromise while maximizing flexibility. We provide formal models for\nboth existing LLM systems and our f-secure LLM system, allowing analysis of\ncritical security guarantees. We further evaluate case studies and benchmarks\nshowing that f-secure LLM systems provide robust security while preserving\nfunctionality and efficiency. Our code is released at\nhttps://github.com/fzwark/Secure_LLM_System.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based systems (LLM systems) are information and query\nprocessing systems that use LLMs to plan operations from natural-language\nprompts and feed the output of each successive step into the LLM to plan the\nnext. This structure results in powerful tools that can process complex\ninformation from diverse sources but raises critical security concerns.\nMalicious information from any source may be processed by the LLM and can\ncompromise the query processing, resulting in nearly arbitrary misbehavior. To\ntackle this problem, we present a system-level defense based on the principles\nof information flow control that we call an f-secure LLM system. An f-secure\nLLM system disaggregates the components of an LLM system into a context-aware\npipeline with dynamically generated structured executable plans, and a security\nmonitor filters out untrusted input into the planning process. This structure\nprevents compromise while maximizing flexibility. We provide formal models for\nboth existing LLM systems and our f-secure LLM system, allowing analysis of\ncritical security guarantees. We further evaluate case studies and benchmarks\nshowing that f-secure LLM systems provide robust security while preserving\nfunctionality and efficiency. Our code is released at\nhttps://github.com/fzwark/Secure_LLM_System."
                },
                "authors": [
                    {
                        "name": "Fangzhou Wu"
                    },
                    {
                        "name": "Ethan Cecchetti"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13873v2",
                "updated": "2024-10-10T15:27:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    27,
                    41,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-22T17:56:53Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    56,
                    53,
                    2,
                    143,
                    0
                ],
                "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n  Question Answering"
                },
                "summary": "Large language models are often challenged by generating erroneous or\n`hallucinated' responses, especially in complex reasoning tasks. To mitigate\nthis, we propose a retrieval augmented reasoning method, FiDeLiS, which\nenhances knowledge graph question answering by anchoring responses to\nstructured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced\nretrieval mechanism that fetches relevant entities and relations from a\nvector-based index of KGs to ensure high-recall retrieval. Once these entities\nand relations are retrieved, our method constructs candidate reasoning paths\nwhich are then refined using a stepwise beam search. This ensures that all the\npaths we create can be confidently linked back to KGs, ensuring they are\naccurate and reliable. A distinctive feature of our approach is its blend of\nnatural language planning with beam search to optimize the selection of\nreasoning paths. Moreover, we redesign the way reasoning paths are scored by\ntransforming this process into a deductive reasoning task, allowing the LLM to\nassess the validity of the paths through deductive reasoning rather than\ntraditional logit-based scoring. This helps avoid misleading reasoning chains\nand reduces unnecessary computational demand. Extensive experiments demonstrate\nthat our method, even as a training-free method which has lower computational\ncosts and superior generality, outperforms established strong baselines across\nthree datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often challenged by generating erroneous or\n`hallucinated' responses, especially in complex reasoning tasks. To mitigate\nthis, we propose a retrieval augmented reasoning method, FiDeLiS, which\nenhances knowledge graph question answering by anchoring responses to\nstructured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced\nretrieval mechanism that fetches relevant entities and relations from a\nvector-based index of KGs to ensure high-recall retrieval. Once these entities\nand relations are retrieved, our method constructs candidate reasoning paths\nwhich are then refined using a stepwise beam search. This ensures that all the\npaths we create can be confidently linked back to KGs, ensuring they are\naccurate and reliable. A distinctive feature of our approach is its blend of\nnatural language planning with beam search to optimize the selection of\nreasoning paths. Moreover, we redesign the way reasoning paths are scored by\ntransforming this process into a deductive reasoning task, allowing the LLM to\nassess the validity of the paths through deductive reasoning rather than\ntraditional logit-based scoring. This helps avoid misleading reasoning chains\nand reduces unnecessary computational demand. Extensive experiments demonstrate\nthat our method, even as a training-free method which has lower computational\ncosts and superior generality, outperforms established strong baselines across\nthree datasets."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Nian Liu"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05426v2",
                "updated": "2024-10-10T15:18:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    18,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-01-03T22:04:40Z",
                "published_parsed": [
                    2024,
                    1,
                    3,
                    22,
                    4,
                    40,
                    2,
                    3,
                    0
                ],
                "title": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in\n  Human Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in\n  Human Activity Recognition"
                },
                "summary": "Recent advancements in Artificial Neural Networks have significantly improved\nhuman activity recognition using multiple time-series sensors. While employing\nnumerous sensors with high-frequency sampling rates usually improves the\nresults, it often leads to data inefficiency and unnecessary expansion of the\nANN, posing a challenge for their practical deployment on edge devices.\nAddressing these issues, our work introduces a pragmatic framework for\ndata-efficient utilization in HAR tasks, considering the optimization of both\nsensor modalities and sampling rate simultaneously. Central to our approach are\nthe designed trainable parameters, termed 'Weight Scores,' which assess the\nsignificance of each sensor modality and sampling rate during the training\nphase. These scores guide the sensor modalities and sampling rate selection.\nThe pruning method allows users to make a trade-off between computational\nbudgets and performance by selecting the sensor modalities and sampling rates\naccording to the weight score ranking. We tested our framework's effectiveness\nin optimizing sensor modality and sampling rate selection using three public\nHAR benchmark datasets. The results show that the sensor and sampling rate\ncombination selected via CoSS achieves similar classification performance to\nconfigurations using the highest sampling rate with all sensors but at a\nreduced hardware cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Artificial Neural Networks have significantly improved\nhuman activity recognition using multiple time-series sensors. While employing\nnumerous sensors with high-frequency sampling rates usually improves the\nresults, it often leads to data inefficiency and unnecessary expansion of the\nANN, posing a challenge for their practical deployment on edge devices.\nAddressing these issues, our work introduces a pragmatic framework for\ndata-efficient utilization in HAR tasks, considering the optimization of both\nsensor modalities and sampling rate simultaneously. Central to our approach are\nthe designed trainable parameters, termed 'Weight Scores,' which assess the\nsignificance of each sensor modality and sampling rate during the training\nphase. These scores guide the sensor modalities and sampling rate selection.\nThe pruning method allows users to make a trade-off between computational\nbudgets and performance by selecting the sensor modalities and sampling rates\naccording to the weight score ranking. We tested our framework's effectiveness\nin optimizing sensor modality and sampling rate selection using three public\nHAR benchmark datasets. The results show that the sensor and sampling rate\ncombination selected via CoSS achieves similar classification performance to\nconfigurations using the highest sampling rate with all sensors but at a\nreduced hardware cost."
                },
                "authors": [
                    {
                        "name": "Mengxi Liu"
                    },
                    {
                        "name": "Zimin Zhao"
                    },
                    {
                        "name": "Daniel Geiler"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Sungho Suh"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "arxiv_comment": "Accepeted by the 2nd Workshop on Sustainable AI (AAAI24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08020v1",
                "updated": "2024-10-10T15:17:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs"
                },
                "summary": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval."
                },
                "authors": [
                    {
                        "name": "Jonas Hbotter"
                    },
                    {
                        "name": "Sascha Bongni"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08014v1",
                "updated": "2024-10-10T15:09:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    9,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:09:52Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    9,
                    52,
                    3,
                    284,
                    0
                ],
                "title": "LLM Cascade with Multi-Objective Optimal Consideration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cascade with Multi-Objective Optimal Consideration"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generating natural language. However, their high deployment\ncosts often pose a barrier to practical applications, especially. Cascading\nlocal and server models offers a promising solution to this challenge. While\nexisting studies on LLM cascades have primarily focused on the performance-cost\ntrade-off, real-world scenarios often involve more complex requirements. This\npaper introduces a novel LLM Cascade strategy with Multi-Objective\nOptimization, enabling LLM cascades to consider additional objectives (e.g.,\nprivacy) and better align with the specific demands of real-world applications\nwhile maintaining their original cascading abilities. Extensive experiments on\nthree benchmarks validate the effectiveness and superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generating natural language. However, their high deployment\ncosts often pose a barrier to practical applications, especially. Cascading\nlocal and server models offers a promising solution to this challenge. While\nexisting studies on LLM cascades have primarily focused on the performance-cost\ntrade-off, real-world scenarios often involve more complex requirements. This\npaper introduces a novel LLM Cascade strategy with Multi-Objective\nOptimization, enabling LLM cascades to consider additional objectives (e.g.,\nprivacy) and better align with the specific demands of real-world applications\nwhile maintaining their original cascading abilities. Extensive experiments on\nthree benchmarks validate the effectiveness and superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Liqian Peng"
                    },
                    {
                        "name": "Congchao Wang"
                    },
                    {
                        "name": "Alec Go"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02018v2",
                "updated": "2024-10-10T15:07:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    7,
                    42,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-02T15:08:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    15,
                    8,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "Large Language Models for Orchestrating Bimanual Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Orchestrating Bimanual Robots"
                },
                "summary": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have demonstrated promising potential in a variety of\nrobotic tasks. However, the nature of language communication via a single\nsequence of discrete symbols makes LLM-based coordination in continuous space a\nparticular challenge for bimanual tasks. To tackle this challenge, we present\nLAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM\nto analyze task configurations and devise coordination control policies for\naddressing long-horizon bimanual tasks. We evaluate our method through\nsimulated experiments involving two classes of long-horizon tasks using the\nNICOL humanoid robot. Our results demonstrate that our method outperforms the\nbaseline in terms of success rate. Additionally, we thoroughly analyze failure\ncases, offering insights into LLM-based approaches in bimanual robotic control\nand revealing future research trends. The project website can be found at\nhttp://labor-agent.github.io."
                },
                "authors": [
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Accepted in Humanoids 2024. The project website can be found at\n  http://labor-agent.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09039v3",
                "updated": "2024-10-10T15:06:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    6,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2023-12-14T15:37:04Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    15,
                    37,
                    4,
                    3,
                    348,
                    0
                ],
                "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning"
                },
                "summary": "Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08001v1",
                "updated": "2024-10-10T14:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation"
                },
                "summary": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/"
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "Project page: https://opendrivelab.com/RoboDual/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07997v1",
                "updated": "2024-10-10T14:53:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    53,
                    39,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:53:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    53,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "APOLLO: A GPT-based tool to detect phishing emails and generate\n  explanations that warn users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: A GPT-based tool to detect phishing emails and generate\n  explanations that warn users"
                },
                "summary": "Phishing is one of the most prolific cybercriminal activities, with attacks\nbecoming increasingly sophisticated. It is, therefore, imperative to explore\nnovel technologies to improve user protection across both technical and human\ndimensions. Large Language Models (LLMs) offer significant promise for text\nprocessing in various domains, but their use for defense against phishing\nattacks still remains scarcely explored. In this paper, we present APOLLO, a\ntool based on OpenAI's GPT-4o to detect phishing emails and generate\nexplanation messages to users about why a specific email is dangerous, thus\nimproving their decision-making capabilities. We have evaluated the performance\nof APOLLO in classifying phishing emails; the results show that the LLM models\nhave exemplary capabilities in classifying phishing emails (97 percent accuracy\nin the case of GPT-4o) and that this performance can be further improved by\nintegrating data from third-party services, resulting in a near-perfect\nclassification rate (99 percent accuracy). To assess the perception of the\nexplanations generated by this tool, we also conducted a study with 20\nparticipants, comparing four different explanations presented as phishing\nwarnings. We compared the LLM-generated explanations to four baselines: a\nmanually crafted warning, and warnings from Chrome, Firefox, and Edge browsers.\nThe results show that not only the LLM-generated explanations were perceived as\nhigh quality, but also that they can be more understandable, interesting, and\ntrustworthy than the baselines. These findings suggest that using LLMs as a\ndefense against phishing is a very promising approach, with APOLLO representing\na proof of concept in this research direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is one of the most prolific cybercriminal activities, with attacks\nbecoming increasingly sophisticated. It is, therefore, imperative to explore\nnovel technologies to improve user protection across both technical and human\ndimensions. Large Language Models (LLMs) offer significant promise for text\nprocessing in various domains, but their use for defense against phishing\nattacks still remains scarcely explored. In this paper, we present APOLLO, a\ntool based on OpenAI's GPT-4o to detect phishing emails and generate\nexplanation messages to users about why a specific email is dangerous, thus\nimproving their decision-making capabilities. We have evaluated the performance\nof APOLLO in classifying phishing emails; the results show that the LLM models\nhave exemplary capabilities in classifying phishing emails (97 percent accuracy\nin the case of GPT-4o) and that this performance can be further improved by\nintegrating data from third-party services, resulting in a near-perfect\nclassification rate (99 percent accuracy). To assess the perception of the\nexplanations generated by this tool, we also conducted a study with 20\nparticipants, comparing four different explanations presented as phishing\nwarnings. We compared the LLM-generated explanations to four baselines: a\nmanually crafted warning, and warnings from Chrome, Firefox, and Edge browsers.\nThe results show that not only the LLM-generated explanations were perceived as\nhigh quality, but also that they can be more understandable, interesting, and\ntrustworthy than the baselines. These findings suggest that using LLMs as a\ndefense against phishing is a very promising approach, with APOLLO representing\na proof of concept in this research direction."
                },
                "authors": [
                    {
                        "name": "Giuseppe Desolda"
                    },
                    {
                        "name": "Francesco Greco"
                    },
                    {
                        "name": "Luca Vigan"
                    }
                ],
                "author_detail": {
                    "name": "Luca Vigan"
                },
                "author": "Luca Vigan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00269v2",
                "updated": "2024-10-10T14:51:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    51,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-08-30T21:54:13Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    21,
                    54,
                    13,
                    4,
                    243,
                    0
                ],
                "title": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human\n  and GPT-4 Written Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human\n  and GPT-4 Written Content"
                },
                "summary": "Cosine similarity between two documents can be computed using token\nembeddings formed by Large Language Models (LLMs) such as GPT-4, and used to\ncategorize those documents across a range of uses. However, these similarities\nare ultimately dependent on the corpora used to train these LLMs, and may not\nreflect subjective similarity of individuals or how their biases and\nconstraints impact similarity metrics. This lack of cognitively-aware\npersonalization of similarity metrics can be particularly problematic in\neducational and recommendation settings where there is a limited number of\nindividual judgements of category or preference, and biases can be particularly\nrelevant. To address this, we rely on an integration of an Instance-Based\nLearning (IBL) cognitive model with LLM embeddings to develop the\nInstance-Based Individualized Similarity (IBIS) metric. This similarity metric\nis beneficial in that it takes into account individual biases and constraints\nin a manner that is grounded in the cognitive mechanisms of decision making. To\nevaluate the IBIS metric, we also introduce a dataset of human categorizations\nof emails as being either dangerous (phishing) or safe (ham). This dataset is\nused to demonstrate the benefits of leveraging a cognitive model to measure the\nsubjective similarity of human participants in an educational setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosine similarity between two documents can be computed using token\nembeddings formed by Large Language Models (LLMs) such as GPT-4, and used to\ncategorize those documents across a range of uses. However, these similarities\nare ultimately dependent on the corpora used to train these LLMs, and may not\nreflect subjective similarity of individuals or how their biases and\nconstraints impact similarity metrics. This lack of cognitively-aware\npersonalization of similarity metrics can be particularly problematic in\neducational and recommendation settings where there is a limited number of\nindividual judgements of category or preference, and biases can be particularly\nrelevant. To address this, we rely on an integration of an Instance-Based\nLearning (IBL) cognitive model with LLM embeddings to develop the\nInstance-Based Individualized Similarity (IBIS) metric. This similarity metric\nis beneficial in that it takes into account individual biases and constraints\nin a manner that is grounded in the cognitive mechanisms of decision making. To\nevaluate the IBIS metric, we also introduce a dataset of human categorizations\nof emails as being either dangerous (phishing) or safe (ham). This dataset is\nused to demonstrate the benefits of leveraging a cognitive model to measure the\nsubjective similarity of human participants in an educational setting."
                },
                "authors": [
                    {
                        "name": "Tyler Malloy"
                    },
                    {
                        "name": "Maria Jos Ferreira"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Cleotilde Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Cleotilde Gonzalez"
                },
                "author": "Cleotilde Gonzalez",
                "arxiv_comment": "7 Figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06423v2",
                "updated": "2024-10-10T14:50:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    50,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-08T22:06:09Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    22,
                    6,
                    9,
                    0,
                    190,
                    0
                ],
                "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step\n  Insight Generation"
                },
                "summary": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data analytics is essential for extracting valuable insights from data that\ncan assist organizations in making effective decisions. We introduce\nInsightBench, a benchmark dataset with three key features. First, it consists\nof 100 datasets representing diverse business use cases such as finance and\nincident management, each accompanied by a carefully curated set of insights\nplanted in the datasets. Second, unlike existing benchmarks focusing on\nanswering single queries, InsightBench evaluates agents based on their ability\nto perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable\nsteps. Third, we conducted comprehensive quality assurance to ensure that each\ndataset in the benchmark had clear goals and included relevant and meaningful\nquestions and analysis. Furthermore, we implement a two-way evaluation\nmechanism using LLaMA-3 as an effective, open-source evaluator to assess\nagents' ability to extract insights. We also propose AgentPoirot, our baseline\ndata analysis agent capable of performing end-to-end data analytics. Our\nevaluation on InsightBench shows that AgentPoirot outperforms existing\napproaches (such as Pandas Agent) that focus on resolving single queries. We\nalso compare the performance of open- and closed-source LLMs and various\nevaluation strategies. Overall, this benchmark serves as a testbed to motivate\nfurther development in comprehensive automated data analytics."
                },
                "authors": [
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Juan Rodriguez"
                    },
                    {
                        "name": "Amirhossein Abaskohi"
                    },
                    {
                        "name": "Mohammad Chegini"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Perouz Taslakian"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Sai Rajeswar Mudumba"
                    },
                    {
                        "name": "Issam Hadj Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam Hadj Laradji"
                },
                "author": "Issam Hadj Laradji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v1",
                "updated": "2024-10-10T14:48:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10719v2",
                "updated": "2024-10-10T14:47:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    47,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-16T20:47:00Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    20,
                    47,
                    0,
                    0,
                    260,
                    0
                ],
                "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images"
                },
                "summary": "Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available."
                },
                "authors": [
                    {
                        "name": "Sina Malakouti"
                    },
                    {
                        "name": "Aysan Aghazadeh"
                    },
                    {
                        "name": "Ashmit Khandelwal"
                    },
                    {
                        "name": "Adriana Kovashka"
                    }
                ],
                "author_detail": {
                    "name": "Adriana Kovashka"
                },
                "author": "Adriana Kovashka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07985v1",
                "updated": "2024-10-10T14:39:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    39,
                    33,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:39:33Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    39,
                    33,
                    3,
                    284,
                    0
                ],
                "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54% and 52.55%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54% and 52.55%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "26 Pages, 17 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06680v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06680v4",
                "updated": "2024-10-10T14:38:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    38,
                    37,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-05T16:35:30Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    16,
                    35,
                    30,
                    6,
                    126,
                    0
                ],
                "title": "Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Compositional Deficiency of Large Language Models in\n  Mathematical Reasoning"
                },
                "summary": "Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06680v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06680v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07962v1",
                "updated": "2024-10-10T14:24:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:24:43Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    43,
                    3,
                    284,
                    0
                ],
                "title": "Towards Assurance of LLM Adversarial Robustness using Ontology-Driven\n  Argumentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Assurance of LLM Adversarial Robustness using Ontology-Driven\n  Argumentation"
                },
                "summary": "Despite the impressive adaptability of large language models (LLMs),\nchallenges remain in ensuring their security, transparency, and\ninterpretability. Given their susceptibility to adversarial attacks, LLMs need\nto be defended with an evolving combination of adversarial training and\nguardrails. However, managing the implicit and heterogeneous knowledge for\ncontinuously assuring robustness is difficult. We introduce a novel approach\nfor assurance of the adversarial robustness of LLMs based on formal\nargumentation. Using ontologies for formalization, we structure\nstate-of-the-art attacks and defenses, facilitating the creation of a\nhuman-readable assurance case, and a machine-readable representation. We\ndemonstrate its application with examples in English language and code\ntranslation tasks, and provide implications for theory and practice, by\ntargeting engineers, data scientists, users, and auditors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive adaptability of large language models (LLMs),\nchallenges remain in ensuring their security, transparency, and\ninterpretability. Given their susceptibility to adversarial attacks, LLMs need\nto be defended with an evolving combination of adversarial training and\nguardrails. However, managing the implicit and heterogeneous knowledge for\ncontinuously assuring robustness is difficult. We introduce a novel approach\nfor assurance of the adversarial robustness of LLMs based on formal\nargumentation. Using ontologies for formalization, we structure\nstate-of-the-art attacks and defenses, facilitating the creation of a\nhuman-readable assurance case, and a machine-readable representation. We\ndemonstrate its application with examples in English language and code\ntranslation tasks, and provide implications for theory and practice, by\ntargeting engineers, data scientists, users, and auditors."
                },
                "authors": [
                    {
                        "name": "Tomas Bueno Momcilovic"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Dian Balta"
                    }
                ],
                "author_detail": {
                    "name": "Dian Balta"
                },
                "author": "Dian Balta",
                "arxiv_comment": "To be published in xAI 2024, late-breaking track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07961v1",
                "updated": "2024-10-10T14:24:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "title": "QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm\n  Design"
                },
                "summary": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yuntian Gu"
                    },
                    {
                        "name": "Ziruo Wang"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Tongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongyang Li"
                },
                "author": "Tongyang Li",
                "arxiv_comment": "35 pages, 7 figures, 4 tables, GitHub repository:\n  https://github.com/EstelYang/QCircuitNet_Dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07959v1",
                "updated": "2024-10-10T14:23:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:23:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act"
                },
                "summary": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice."
                },
                "authors": [
                    {
                        "name": "Philipp Guldimann"
                    },
                    {
                        "name": "Alexander Spiridonov"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Velko Vechev"
                    },
                    {
                        "name": "Anna Gueorguieva"
                    },
                    {
                        "name": "Mislav Balunovi"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    },
                    {
                        "name": "Pavol Bielik"
                    },
                    {
                        "name": "Petar Tsankov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07951v1",
                "updated": "2024-10-10T14:18:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    18,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:18:34Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    18,
                    34,
                    3,
                    284,
                    0
                ],
                "title": "Disease Entity Recognition and Normalization is Improved with Large\n  Language Model Derived Synthetic Normalized Mentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease Entity Recognition and Normalization is Improved with Large\n  Language Model Derived Synthetic Normalized Mentions"
                },
                "summary": "Background: Machine learning methods for clinical named entity recognition\nand entity normalization systems can utilize both labeled corpora and Knowledge\nGraphs (KGs) for learning. However, infrequently occurring concepts may have\nfew mentions in training corpora and lack detailed descriptions or synonyms,\neven in large KGs. For Disease Entity Recognition (DER) and Disease Entity\nNormalization (DEN), this can result in fewer high quality training examples\nrelative to the number of known diseases. Large Language Model (LLM) generation\nof synthetic training examples could improve performance in these information\nextraction tasks.\n  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus\ncontaining normalized mentions of concepts from the Unified Medical Language\nSystem (UMLS) Disease Semantic Group. We measured overall and Out of\nDistribution (OOD) performance for DER and DEN, with and without synthetic data\naugmentation. We evaluated performance on 3 different disease corpora using 4\ndifferent data augmentation strategies, assessed using BioBERT for DER and\nSapBERT and KrissBERT for DEN.\n  Results: Our synthetic data yielded a substantial improvement for DEN, in all\n3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by\n3-9 points in overall performance and by 20-55 points in OOD data. A small\nimprovement (1-2 points) was also seen for DER in overall performance, but only\none dataset showed OOD improvement.\n  Conclusion: LLM generation of normalized disease mentions can improve DEN\nrelative to normalization approaches that do not utilize LLMs to augment data\nwith synthetic mentions. Ablation studies indicate that performance gains for\nDEN were only partially attributable to improvements in OOD performance. The\nsame approach has only a limited ability to improve DER. We make our software\nand dataset publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Machine learning methods for clinical named entity recognition\nand entity normalization systems can utilize both labeled corpora and Knowledge\nGraphs (KGs) for learning. However, infrequently occurring concepts may have\nfew mentions in training corpora and lack detailed descriptions or synonyms,\neven in large KGs. For Disease Entity Recognition (DER) and Disease Entity\nNormalization (DEN), this can result in fewer high quality training examples\nrelative to the number of known diseases. Large Language Model (LLM) generation\nof synthetic training examples could improve performance in these information\nextraction tasks.\n  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus\ncontaining normalized mentions of concepts from the Unified Medical Language\nSystem (UMLS) Disease Semantic Group. We measured overall and Out of\nDistribution (OOD) performance for DER and DEN, with and without synthetic data\naugmentation. We evaluated performance on 3 different disease corpora using 4\ndifferent data augmentation strategies, assessed using BioBERT for DER and\nSapBERT and KrissBERT for DEN.\n  Results: Our synthetic data yielded a substantial improvement for DEN, in all\n3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by\n3-9 points in overall performance and by 20-55 points in OOD data. A small\nimprovement (1-2 points) was also seen for DER in overall performance, but only\none dataset showed OOD improvement.\n  Conclusion: LLM generation of normalized disease mentions can improve DEN\nrelative to normalization approaches that do not utilize LLMs to augment data\nwith synthetic mentions. Ablation studies indicate that performance gains for\nDEN were only partially attributable to improvements in OOD performance. The\nsame approach has only a limited ability to improve DER. We make our software\nand dataset publicly available."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Shinjitha Vadlakonda"
                    },
                    {
                        "name": "Richard E. Kennedy"
                    },
                    {
                        "name": "John D. Osborne"
                    }
                ],
                "author_detail": {
                    "name": "John D. Osborne"
                },
                "author": "John D. Osborne",
                "arxiv_comment": "21 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09265v2",
                "updated": "2024-10-10T14:17:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    17,
                    20,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-13T16:04:11Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    4,
                    11,
                    3,
                    165,
                    0
                ],
                "title": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs"
                },
                "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing (NLP), and recent studies have aimed to understand their\nunderlying mechanisms. However, most of this research is conducted within a\nmonolingual setting, primarily focusing on English. Few studies attempt to\nexplore the internal workings of LLMs in multilingual settings. In this study,\nwe aim to fill the research gap by examining how neuron activation is shared\nacross tasks and languages. We classify neurons into four distinct categories\nbased on their responses to a specific input across different\nlanguages:all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the patterns of neuron sharing are significantly affected by the\ncharacteristics of tasks and examples; (ii) neuron sharing does not fully\ncorrespond with language similarity; (iii) shared neurons play a vital role in\ngenerating responses, especially those shared across all languages. These\nfindings shed light on the internal workings of multilingual LLMs and pave the\nway to the future research. We will release the code to foster research in this\narea.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing (NLP), and recent studies have aimed to understand their\nunderlying mechanisms. However, most of this research is conducted within a\nmonolingual setting, primarily focusing on English. Few studies attempt to\nexplore the internal workings of LLMs in multilingual settings. In this study,\nwe aim to fill the research gap by examining how neuron activation is shared\nacross tasks and languages. We classify neurons into four distinct categories\nbased on their responses to a specific input across different\nlanguages:all-shared, partial-shared, specific, and non-activated. This\ncategorization is combined with a study of neuron attribution, i.e. the\nimportance of a neuron w.r.t an output. Our analysis reveals the following\ninsights: (i) the patterns of neuron sharing are significantly affected by the\ncharacteristics of tasks and examples; (ii) neuron sharing does not fully\ncorrespond with language similarity; (iii) shared neurons play a vital role in\ngenerating responses, especially those shared across all languages. These\nfindings shed light on the internal workings of multilingual LLMs and pave the\nway to the future research. We will release the code to foster research in this\narea."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16562v3",
                "updated": "2024-10-10T14:04:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    4,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-24T11:56:15Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    11,
                    56,
                    15,
                    0,
                    176,
                    0
                ],
                "title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models"
                },
                "summary": "The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive data. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We supervised fine-tune (SFT) the MLLM to\nalign with human evaluative judgments, resulting in a robust evaluation model.\nOur evaluation across 24 text-to-image generation models demonstrate that\nEvalAlign not only provides superior metric stability but also aligns more\nclosely with human preferences than existing metrics, confirming its\neffectiveness and utility in model assessment."
                },
                "authors": [
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "Project page: https://sais-fuxi.github.io/projects/evalalign/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07933v1",
                "updated": "2024-10-10T14:00:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    0,
                    21,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T14:00:21Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    0,
                    21,
                    3,
                    284,
                    0
                ],
                "title": "Offline Hierarchical Reinforcement Learning via Inverse Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Hierarchical Reinforcement Learning via Inverse Optimization"
                },
                "summary": "Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the inverse\nproblem, recovering the unobservable high-level actions that likely generated\nthe observed data under our hierarchical policy. This approach constructs a\ndataset suitable for off-the-shelf offline training. We demonstrate our\nframework on robotic and network optimization problems and show that it\nsubstantially outperforms end-to-end RL methods and improves robustness. We\ninvestigate a variety of instantiations of our framework, both in direct\ndeployment of policies trained offline and when online fine-tuning is\nperformed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the inverse\nproblem, recovering the unobservable high-level actions that likely generated\nthe observed data under our hierarchical policy. This approach constructs a\ndataset suitable for off-the-shelf offline training. We demonstrate our\nframework on robotic and network optimization problems and show that it\nsubstantially outperforms end-to-end RL methods and improves robustness. We\ninvestigate a variety of instantiations of our framework, both in direct\ndeployment of policies trained offline and when online fine-tuning is\nperformed."
                },
                "authors": [
                    {
                        "name": "Carolin Schmidt"
                    },
                    {
                        "name": "Daniele Gammelli"
                    },
                    {
                        "name": "James Harrison"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Filipe Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Filipe Rodrigues"
                },
                "author": "Filipe Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07927v1",
                "updated": "2024-10-10T13:54:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    54,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:54:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    54,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "Efficient Reinforcement Learning with Large Language Model Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reinforcement Learning with Large Language Model Priors"
                },
                "summary": "In sequential decision-making (SDM) tasks, methods like reinforcement\nlearning (RL) and heuristic search have made notable advances in specific\ncases. However, they often require extensive exploration and face challenges in\ngeneralizing across diverse environments due to their limited grasp of the\nunderlying decision dynamics. In contrast, large language models (LLMs) have\nrecently emerged as powerful general-purpose tools, due to their capacity to\nmaintain vast amounts of domain-specific knowledge. To harness this rich prior\nknowledge for efficiently solving complex SDM tasks, we propose treating LLMs\nas prior action distributions and integrating them into RL frameworks through\nBayesian inference methods, making use of variational inference and direct\nposterior sampling. The proposed approaches facilitate the seamless\nincorporation of fixed LLM priors into both policy-based and value-based RL\nframeworks. Our experiments show that incorporating LLM-based action priors\nsignificantly reduces exploration and optimization complexity, substantially\nimproving sample efficiency compared to traditional RL techniques, e.g., using\nLLM priors decreases the number of required samples by over 90% in offline\nlearning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In sequential decision-making (SDM) tasks, methods like reinforcement\nlearning (RL) and heuristic search have made notable advances in specific\ncases. However, they often require extensive exploration and face challenges in\ngeneralizing across diverse environments due to their limited grasp of the\nunderlying decision dynamics. In contrast, large language models (LLMs) have\nrecently emerged as powerful general-purpose tools, due to their capacity to\nmaintain vast amounts of domain-specific knowledge. To harness this rich prior\nknowledge for efficiently solving complex SDM tasks, we propose treating LLMs\nas prior action distributions and integrating them into RL frameworks through\nBayesian inference methods, making use of variational inference and direct\nposterior sampling. The proposed approaches facilitate the seamless\nincorporation of fixed LLM priors into both policy-based and value-based RL\nframeworks. Our experiments show that incorporating LLM-based action priors\nsignificantly reduces exploration and optimization complexity, substantially\nimproving sample efficiency compared to traditional RL techniques, e.g., using\nLLM priors decreases the number of required samples by over 90% in offline\nlearning scenarios."
                },
                "authors": [
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Xidong Feng"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07919v1",
                "updated": "2024-10-10T13:45:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    45,
                    56,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:45:56Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    45,
                    56,
                    3,
                    284,
                    0
                ],
                "title": "InstructBioMol: Advancing Biomolecule Understanding and Design Following\n  Human Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructBioMol: Advancing Biomolecule Understanding and Design Following\n  Human Instructions"
                },
                "summary": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology, and\nenzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nAI's computational power and researchers' intuition, using natural language to\nalign molecular complexity with human intentions. Large Language Models (LLMs)\nhave shown potential to interpret human intentions, yet their application to\nbiomolecular research remains nascent due to challenges including specialized\nknowledge requirements, multimodal data integration, and semantic alignment\nbetween natural language and biomolecules. To address these limitations, we\npresent InstructBioMol, a novel LLM designed to bridge natural language and\nbiomolecules through a comprehensive any-to-any alignment of natural language,\nmolecules, and proteins. This model can integrate multimodal biomolecules as\ninput, and enable researchers to articulate design goals in natural language,\nproviding biomolecular outputs that meet precise biological needs. Experimental\nresults demonstrate InstructBioMol can understand and design biomolecules\nfollowing human instructions. Notably, it can generate drug molecules with a\n10% improvement in binding affinity and design enzymes that achieve an ESP\nScore of 70.4, making it the only method to surpass the enzyme-substrate\ninteraction threshold of 60.0 recommended by the ESP developer. This highlights\nits potential to transform real-world biomolecular research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology, and\nenzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nAI's computational power and researchers' intuition, using natural language to\nalign molecular complexity with human intentions. Large Language Models (LLMs)\nhave shown potential to interpret human intentions, yet their application to\nbiomolecular research remains nascent due to challenges including specialized\nknowledge requirements, multimodal data integration, and semantic alignment\nbetween natural language and biomolecules. To address these limitations, we\npresent InstructBioMol, a novel LLM designed to bridge natural language and\nbiomolecules through a comprehensive any-to-any alignment of natural language,\nmolecules, and proteins. This model can integrate multimodal biomolecules as\ninput, and enable researchers to articulate design goals in natural language,\nproviding biomolecular outputs that meet precise biological needs. Experimental\nresults demonstrate InstructBioMol can understand and design biomolecules\nfollowing human instructions. Notably, it can generate drug molecules with a\n10% improvement in binding affinity and design enzymes that achieve an ESP\nScore of 70.4, making it the only method to surpass the enzyme-substrate\ninteraction threshold of 60.0 recommended by the ESP developer. This highlights\nits potential to transform real-world biomolecular research."
                },
                "authors": [
                    {
                        "name": "Xiang Zhuang"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Tianwen Lyu"
                    },
                    {
                        "name": "Yinuo Jiang"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Zhuoyi Xiang"
                    },
                    {
                        "name": "Zeyuan Wang"
                    },
                    {
                        "name": "Ming Qin"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Jike Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15368v3",
                "updated": "2024-10-10T13:35:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    35,
                    50,
                    3,
                    284,
                    0
                ],
                "published": "2024-02-23T15:02:44Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    15,
                    2,
                    44,
                    4,
                    54,
                    0
                ],
                "title": "Safe Task Planning for Language-Instructed Multi-Robot Systems using\n  Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Task Planning for Language-Instructed Multi-Robot Systems using\n  Conformal Prediction"
                },
                "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack mission completion guarantees. To address this challenge,\nwe introduce a new distributed LLM-based planner, called S-ATLAS for Safe\nplAnning for Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates while minimizing the\noverall number of help requests. We provide comparative experiments against\nrelated works showing that our method is significantly more computational\nefficient and achieves lower help rates. The advantage of our algorithm over\nbaselines becomes more pronounced with increasing robot team size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack mission completion guarantees. To address this challenge,\nwe introduce a new distributed LLM-based planner, called S-ATLAS for Safe\nplAnning for Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates while minimizing the\noverall number of help requests. We provide comparative experiments against\nrelated works showing that our method is significantly more computational\nefficient and achieves lower help rates. The advantage of our algorithm over\nbaselines becomes more pronounced with increasing robot team size."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guocheng He"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07896v1",
                "updated": "2024-10-10T13:23:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    23,
                    49,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:23:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    23,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing\n  Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing\n  Machines"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing and reasoning tasks. However, their\nperformance in the foundational domain of arithmetic remains unsatisfactory.\nWhen dealing with arithmetic tasks, LLMs often memorize specific examples\nrather than learning the underlying computational logic, limiting their ability\nto generalize to new problems. In this paper, we propose a Composable\nArithmetic Execution Framework (CAEF) that enables LLMs to learn to execute\nstep-by-step computations by emulating Turing Machines, thereby gaining a\ngenuine understanding of computational logic. Moreover, the proposed framework\nis highly scalable, allowing composing learned operators to significantly\nreduce the difficulty of learning complex operators. In our evaluation, CAEF\nachieves nearly 100% accuracy across seven common mathematical operations on\nthe LLaMA 3.1-8B model, effectively supporting computations involving operands\nwith up to 100 digits, a level where GPT-4o falls short noticeably in some\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing and reasoning tasks. However, their\nperformance in the foundational domain of arithmetic remains unsatisfactory.\nWhen dealing with arithmetic tasks, LLMs often memorize specific examples\nrather than learning the underlying computational logic, limiting their ability\nto generalize to new problems. In this paper, we propose a Composable\nArithmetic Execution Framework (CAEF) that enables LLMs to learn to execute\nstep-by-step computations by emulating Turing Machines, thereby gaining a\ngenuine understanding of computational logic. Moreover, the proposed framework\nis highly scalable, allowing composing learned operators to significantly\nreduce the difficulty of learning complex operators. In our evaluation, CAEF\nachieves nearly 100% accuracy across seven common mathematical operations on\nthe LLaMA 3.1-8B model, effectively supporting computations involving operands\nwith up to 100 digits, a level where GPT-4o falls short noticeably in some\nsettings."
                },
                "authors": [
                    {
                        "name": "Junyu Lai"
                    },
                    {
                        "name": "Jiahe Xu"
                    },
                    {
                        "name": "Yao Yang"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13968v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13968v3",
                "updated": "2024-10-10T13:22:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    22,
                    58,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    16,
                    7,
                    0,
                    113,
                    0
                ],
                "title": "Protecting Your LLMs with Information Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Your LLMs with Information Bottleneck"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Zefan Wang"
                    },
                    {
                        "name": "Linjie Xu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Tianchun Wang"
                    },
                    {
                        "name": "Chunlin Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Accepted by Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13968v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13968v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06992v2",
                "updated": "2024-10-10T13:13:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    13,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T15:38:53Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    38,
                    53,
                    2,
                    283,
                    0
                ],
                "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
                },
                "summary": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in Software Engineering (SE) can offer\nassistance for coding. To facilitate a rigorous evaluation of LLMs in practical\ncoding contexts, Carlos et al. introduced the SWE-bench dataset, which\ncomprises 2,294 real-world GitHub issues and their corresponding pull requests,\ncollected from 12 widely used Python repositories. Several impressive LLM-based\ntoolkits recently are developed and evaluated on this dataset. However, a\nsystematic evaluation of the quality of SWE-bench remains missing. In this\npaper, we addressed this gap by presenting an empirical analysis of the\nSWE-bench dataset. We conducted a manual screening of instances where SWEAgent\n+ GPT-4 successfully resolved issues by comparing the model-generated patches\nwith the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench\nleaderboard during the time of our study. Our analysis reveals some critical\nissues with the SWE-bench dataset: 1) 32.67% of the successful patches involve\ncheating as the solutions were directly provided in the issue report or the\ncomments. We refer to as solution leakage problem. 2) 31.08% of the passed\npatches are suspicious patches due to weak test cases, i.e., the tests were not\nadequate to verify the correctness of a patch. When we filtered out these\nproblematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47%\nto 3.97%. We also observed that the same data quality issues also exist in the\ntwo variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In\naddition, over 94% of the issues were created before LLM's knowledge cutoff\ndates, posing potential data leakage issues."
                },
                "authors": [
                    {
                        "name": "Reem Aleithan"
                    },
                    {
                        "name": "Haoran Xue"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajer"
                    },
                    {
                        "name": "Elijah Nnorom"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Song Wang"
                    }
                ],
                "author_detail": {
                    "name": "Song Wang"
                },
                "author": "Song Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07881v1",
                "updated": "2024-10-10T13:02:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    2,
                    0,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T13:02:00Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    13,
                    2,
                    0,
                    3,
                    284,
                    0
                ],
                "title": "A Comprehensive Survey on Joint Resource Allocation Strategies in\n  Federated Edge Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Joint Resource Allocation Strategies in\n  Federated Edge Learning"
                },
                "summary": "Federated Edge Learning (FEL), an emerging distributed Machine Learning (ML)\nparadigm, enables model training in a distributed environment while ensuring\nuser privacy by using physical separation for each user data. However, with the\ndevelopment of complex application scenarios such as the Internet of Things\n(IoT) and Smart Earth, the conventional resource allocation schemes can no\nlonger effectively support these growing computational and communication\ndemands. Therefore, joint resource optimization may be the key solution to the\nscaling problem. This paper simultaneously addresses the multifaceted\nchallenges of computation and communication, with the growing multiple resource\ndemands. We systematically review the joint allocation strategies for different\nresources (computation, data, communication, and network topology) in FEL, and\nsummarize the advantages in improving system efficiency, reducing latency,\nenhancing resource utilization and enhancing robustness. In addition, we\npresent the potential ability of joint optimization to enhance privacy\npreservation by reducing communication requirements, indirectly. This work not\nonly provides theoretical support for resource management in federated learning\n(FL) systems, but also provides ideas for potential optimal deployment in\nmultiple real-world scenarios. By thoroughly discussing the current challenges\nand future research directions, it also provides some important insights into\nmulti-resource optimization in complex application environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL), an emerging distributed Machine Learning (ML)\nparadigm, enables model training in a distributed environment while ensuring\nuser privacy by using physical separation for each user data. However, with the\ndevelopment of complex application scenarios such as the Internet of Things\n(IoT) and Smart Earth, the conventional resource allocation schemes can no\nlonger effectively support these growing computational and communication\ndemands. Therefore, joint resource optimization may be the key solution to the\nscaling problem. This paper simultaneously addresses the multifaceted\nchallenges of computation and communication, with the growing multiple resource\ndemands. We systematically review the joint allocation strategies for different\nresources (computation, data, communication, and network topology) in FEL, and\nsummarize the advantages in improving system efficiency, reducing latency,\nenhancing resource utilization and enhancing robustness. In addition, we\npresent the potential ability of joint optimization to enhance privacy\npreservation by reducing communication requirements, indirectly. This work not\nonly provides theoretical support for resource management in federated learning\n(FL) systems, but also provides ideas for potential optimal deployment in\nmultiple real-world scenarios. By thoroughly discussing the current challenges\nand future research directions, it also provides some important insights into\nmulti-resource optimization in complex application environments."
                },
                "authors": [
                    {
                        "name": "Jingbo Zhang"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Qiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Fan"
                },
                "author": "Qiang Fan",
                "arxiv_comment": "This paper has been submitted to CMC-Computers Materials & Continua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05864v2",
                "updated": "2024-10-10T12:41:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    26,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-08T09:53:35Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    35,
                    1,
                    282,
                    0
                ],
                "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Words: On the Inner Lexicon of LLMs"
                },
                "summary": "Natural language is composed of words, but modern LLMs process sub-words as\ninput. A natural question raised by this discrepancy is whether LLMs encode\nwords internally, and if so how. We present evidence that LLMs engage in an\nintrinsic detokenization process, where sub-word sequences are combined into\ncoherent word representations. Our experiments show that this process takes\nplace primarily within the early and middle layers of the model. They also show\nthat it is robust to non-morphemic splits, typos and perhaps importantly-to\nout-of-vocabulary words: when feeding the inner representation of such words to\nthe model as input vectors, it can \"understand\" them despite never seeing them\nduring training. Our findings suggest that LLMs maintain a latent vocabulary\nbeyond the tokenizer's scope. These insights provide a practical,\nfinetuning-free application for expanding the vocabulary of pre-trained models.\nBy enabling the addition of new vocabulary words, we reduce input length and\ninference iterations, which reduces both space and model latency, with little\nto no loss in model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language is composed of words, but modern LLMs process sub-words as\ninput. A natural question raised by this discrepancy is whether LLMs encode\nwords internally, and if so how. We present evidence that LLMs engage in an\nintrinsic detokenization process, where sub-word sequences are combined into\ncoherent word representations. Our experiments show that this process takes\nplace primarily within the early and middle layers of the model. They also show\nthat it is robust to non-morphemic splits, typos and perhaps importantly-to\nout-of-vocabulary words: when feeding the inner representation of such words to\nthe model as input vectors, it can \"understand\" them despite never seeing them\nduring training. Our findings suggest that LLMs maintain a latent vocabulary\nbeyond the tokenizer's scope. These insights provide a practical,\nfinetuning-free application for expanding the vocabulary of pre-trained models.\nBy enabling the addition of new vocabulary words, we reduce input length and\ninference iterations, which reduces both space and model latency, with little\nto no loss in model accuracy."
                },
                "authors": [
                    {
                        "name": "Guy Kaplan"
                    },
                    {
                        "name": "Matanel Oren"
                    },
                    {
                        "name": "Yuval Reif"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07869v1",
                "updated": "2024-10-10T12:41:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "Benchmarking Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Agentic Workflow Generation"
                },
                "summary": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset will be available at\nhttps://github.com/zjunlp/WorFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset will be available at\nhttps://github.com/zjunlp/WorFBench."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07851v1",
                "updated": "2024-10-10T12:18:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    18,
                    42,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T12:18:42Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    18,
                    42,
                    3,
                    284,
                    0
                ],
                "title": "Scalable Representation Learning for Multimodal Tabular Transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Representation Learning for Multimodal Tabular Transactions"
                },
                "summary": "Large language models (LLMs) are primarily designed to understand\nunstructured text. When directly applied to structured formats such as tabular\ndata, they may struggle to discern inherent relationships and overlook critical\npatterns. While tabular representation learning methods can address some of\nthese limitations, existing efforts still face challenges with sparse\nhigh-cardinality fields, precise numerical reasoning, and column-heavy tables.\nFurthermore, leveraging these learned representations for downstream tasks\nthrough a language based interface is not apparent. In this paper, we present\nan innovative and scalable solution to these challenges. Concretely, our\napproach introduces a multi-tier partitioning mechanism that utilizes power-law\ndynamics to handle large vocabularies, an adaptive quantization mechanism to\nimpose priors on numerical continuity, and a distinct treatment of core-columns\nand meta-information columns. To facilitate instruction tuning on LLMs, we\npropose a parameter efficient decoder that interleaves transaction and text\nmodalities using a series of adapter layers, thereby exploiting rich cross-task\nknowledge. We validate the efficacy of our solution on a large-scale dataset of\nsynthetic payments transactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are primarily designed to understand\nunstructured text. When directly applied to structured formats such as tabular\ndata, they may struggle to discern inherent relationships and overlook critical\npatterns. While tabular representation learning methods can address some of\nthese limitations, existing efforts still face challenges with sparse\nhigh-cardinality fields, precise numerical reasoning, and column-heavy tables.\nFurthermore, leveraging these learned representations for downstream tasks\nthrough a language based interface is not apparent. In this paper, we present\nan innovative and scalable solution to these challenges. Concretely, our\napproach introduces a multi-tier partitioning mechanism that utilizes power-law\ndynamics to handle large vocabularies, an adaptive quantization mechanism to\nimpose priors on numerical continuity, and a distinct treatment of core-columns\nand meta-information columns. To facilitate instruction tuning on LLMs, we\npropose a parameter efficient decoder that interleaves transaction and text\nmodalities using a series of adapter layers, thereby exploiting rich cross-task\nknowledge. We validate the efficacy of our solution on a large-scale dataset of\nsynthetic payments transactions."
                },
                "authors": [
                    {
                        "name": "Natraj Raman"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07839v1",
                "updated": "2024-10-10T11:58:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:58:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    58,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Enhancing Language Model Reasoning via Weighted Reasoning in\n  Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Language Model Reasoning via Weighted Reasoning in\n  Self-Consistency"
                },
                "summary": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al's self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the detailed step-by-step reasoning paths applied by these\npaths. Our work enhances this approach by incorporating and analyzing both the\nreasoning paths of these rationales in addition to their final decisions before\ntaking a majority vote. These methods not only improve the reliability of\nreasoning paths but also cause more robust performance on complex reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al's self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the detailed step-by-step reasoning paths applied by these\npaths. Our work enhances this approach by incorporating and analyzing both the\nreasoning paths of these rationales in addition to their final decisions before\ntaking a majority vote. These methods not only improve the reliability of\nreasoning paths but also cause more robust performance on complex reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tim Knappe"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Ayush Chauhan"
                    },
                    {
                        "name": "Kaylee Chhua"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted to MATH-AI at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17515v2",
                "updated": "2024-10-10T11:53:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    53,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-26T03:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection"
                },
                "summary": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data."
                },
                "authors": [
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Maike Feng"
                    },
                    {
                        "name": "Jing Qiu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07836v1",
                "updated": "2024-10-10T11:52:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:52:07Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities"
                },
                "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Mircea Lica"
                    },
                    {
                        "name": "Zarif Ikram"
                    },
                    {
                        "name": "Akihiro Nakano"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Aniket Rajiv Didolkar"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13009v2",
                "updated": "2024-10-10T11:51:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    51,
                    24,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-13T10:51:43Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    51,
                    43,
                    0,
                    134,
                    0
                ],
                "title": "MetaReflection: Learning Instructions for Language Agents using Past\n  Reflections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaReflection: Learning Instructions for Language Agents using Past\n  Reflections"
                },
                "summary": "The popularity of Large Language Models (LLMs) have unleashed a new age\nofLanguage Agents for solving a diverse range of tasks. While contemporary\nfrontier LLMs are capable enough to power reasonably good Language agents, the\nclosed-API model makes it hard to improve in cases they perform sub-optimally.\nTo address this, recent works have explored ways to improve their performance\nusing techniques like self-reflection and prompt optimization. Unfortunately,\ntechniques like self-reflection can be used only in an online setup, while\ncontemporary prompt optimization techniques are designed and tested to work on\nsimple tasks. To this end, we introduce MetaReflection, a novel offline\nreinforcement learning technique that enhances the performance of Language\nAgents by augmenting a semantic memory based on experiential learnings from\npast trials. We demonstrate the efficacy of MetaReflection by evaluating across\nmultiple domains, including complex logical reasoning, biomedical semantic\nsimilarity, open world question answering, and vulnerability threat detection,\nin Infrastructure-as-Code, spanning different agent designs. MetaReflection\nboosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline\nand performs on par with existing state-of-the-art prompt optimization\ntechniques while requiring fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of Large Language Models (LLMs) have unleashed a new age\nofLanguage Agents for solving a diverse range of tasks. While contemporary\nfrontier LLMs are capable enough to power reasonably good Language agents, the\nclosed-API model makes it hard to improve in cases they perform sub-optimally.\nTo address this, recent works have explored ways to improve their performance\nusing techniques like self-reflection and prompt optimization. Unfortunately,\ntechniques like self-reflection can be used only in an online setup, while\ncontemporary prompt optimization techniques are designed and tested to work on\nsimple tasks. To this end, we introduce MetaReflection, a novel offline\nreinforcement learning technique that enhances the performance of Language\nAgents by augmenting a semantic memory based on experiential learnings from\npast trials. We demonstrate the efficacy of MetaReflection by evaluating across\nmultiple domains, including complex logical reasoning, biomedical semantic\nsimilarity, open world question answering, and vulnerability threat detection,\nin Infrastructure-as-Code, spanning different agent designs. MetaReflection\nboosts Language agents' performance by 4% to 16.82% over the raw GPT-4 baseline\nand performs on par with existing state-of-the-art prompt optimization\ntechniques while requiring fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Priyanshu Gupta"
                    },
                    {
                        "name": "Shashank Kirtania"
                    },
                    {
                        "name": "Ananya Singha"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sherry Shi"
                    },
                    {
                        "name": "Gustavo Soares"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Soares"
                },
                "author": "Gustavo Soares",
                "arxiv_comment": "We release our experimental code at:\n  https://aka.ms/metareflection-code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02952v3",
                "updated": "2024-10-10T11:41:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    41,
                    35,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-03T19:52:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    52,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications"
                },
                "summary": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation."
                },
                "authors": [
                    {
                        "name": "Oren Sultan"
                    },
                    {
                        "name": "Alex Khasin"
                    },
                    {
                        "name": "Guy Shiran"
                    },
                    {
                        "name": "Asnat Greenstein-Messica"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15934v2",
                "updated": "2024-10-10T11:37:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    37,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-24T09:57:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents"
                },
                "summary": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains."
                },
                "authors": [
                    {
                        "name": "Samuel Arcadinho"
                    },
                    {
                        "name": "David Aparicio"
                    },
                    {
                        "name": "Mariana Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Almeida"
                },
                "author": "Mariana Almeida",
                "arxiv_comment": "14 pages, 5 figures, Submitted to GenBench@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07830v1",
                "updated": "2024-10-10T11:33:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    33,
                    25,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:33:25Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    33,
                    25,
                    3,
                    284,
                    0
                ],
                "title": "NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional promise in\ntranslation tasks for high-resource languages. However, their performance in\nlow-resource languages is limited by the scarcity of both parallel and\nmonolingual corpora, as well as the presence of noise. Consequently, such LLMs\nsuffer with alignment and have lagged behind State-of-The-Art (SoTA) neural\nmachine translation (NMT) models in these settings. This paper introduces\nNusaMT-7B, an LLM-based machine translation model for low-resource Indonesian\nlanguages, starting with Balinese and Minangkabau. Leveraging the pretrained\nLLaMA2-7B, our approach integrates continued pre-training on monolingual data,\nSupervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to\nreduce noise in parallel sentences. In the FLORES-200 multilingual translation\nbenchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to\n+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms\nby up to -3.38 spBLEU in translations into higher-resource languages. Our\nresults show that fine-tuned LLMs can enhance translation quality for\nlow-resource languages, aiding in linguistic preservation and cross-cultural\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional promise in\ntranslation tasks for high-resource languages. However, their performance in\nlow-resource languages is limited by the scarcity of both parallel and\nmonolingual corpora, as well as the presence of noise. Consequently, such LLMs\nsuffer with alignment and have lagged behind State-of-The-Art (SoTA) neural\nmachine translation (NMT) models in these settings. This paper introduces\nNusaMT-7B, an LLM-based machine translation model for low-resource Indonesian\nlanguages, starting with Balinese and Minangkabau. Leveraging the pretrained\nLLaMA2-7B, our approach integrates continued pre-training on monolingual data,\nSupervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to\nreduce noise in parallel sentences. In the FLORES-200 multilingual translation\nbenchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to\n+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms\nby up to -3.38 spBLEU in translations into higher-resource languages. Our\nresults show that fine-tuned LLMs can enhance translation quality for\nlow-resource languages, aiding in linguistic preservation and cross-cultural\ncommunication."
                },
                "authors": [
                    {
                        "name": "William Tan"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Accepted to SoLaR @ NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07826v1",
                "updated": "2024-10-10T11:24:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    24,
                    4,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:24:04Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    24,
                    4,
                    3,
                    284,
                    0
                ],
                "title": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study\n  of Alignment with Human Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study\n  of Alignment with Human Responses"
                },
                "summary": "Language models often misinterpret human intentions due to their handling of\nambiguity, a limitation well-recognized in NLP research. While morally clear\nscenarios are more discernible to LLMs, greater difficulty is encountered in\nmorally ambiguous contexts. In this investigation, we explored LLM calibration\nto show that human and LLM judgments are poorly aligned in such scenarios. We\nused two curated datasets from the Scruples project for evaluation: DILEMMAS,\nwhich involves pairs of distinct moral scenarios to assess the model's ability\nto compare and contrast ethical situations, and ANECDOTES, which presents\nindividual narratives to evaluate the model's skill in drawing out details,\ninterpreting, and analyzing distinct moral scenarios. Model answer\nprobabilities were extracted for all possible choices and compared with human\nannotations to benchmark the alignment of three models: Llama-3.1-8b,\nZephyr-7b-beta, and Mistral-7b. Significant improvements were observed after\nfine-tuning, with notable enhancements in both cross-entropy and Dirichlet\nscores, particularly in the latter. Notably, after fine-tuning, the performance\nof Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental\nmodels that were examined were all still outperformed by the BERT and RoBERTa\nmodels in terms of cross-entropy scores. Our fine-tuning approach, which\nimproves the model's understanding of text distributions in a text-to-text\nformat, effectively enhances performance and alignment in complex\ndecision-making contexts, underscoring the need for further research to refine\nethical reasoning techniques and capture human judgment nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models often misinterpret human intentions due to their handling of\nambiguity, a limitation well-recognized in NLP research. While morally clear\nscenarios are more discernible to LLMs, greater difficulty is encountered in\nmorally ambiguous contexts. In this investigation, we explored LLM calibration\nto show that human and LLM judgments are poorly aligned in such scenarios. We\nused two curated datasets from the Scruples project for evaluation: DILEMMAS,\nwhich involves pairs of distinct moral scenarios to assess the model's ability\nto compare and contrast ethical situations, and ANECDOTES, which presents\nindividual narratives to evaluate the model's skill in drawing out details,\ninterpreting, and analyzing distinct moral scenarios. Model answer\nprobabilities were extracted for all possible choices and compared with human\nannotations to benchmark the alignment of three models: Llama-3.1-8b,\nZephyr-7b-beta, and Mistral-7b. Significant improvements were observed after\nfine-tuning, with notable enhancements in both cross-entropy and Dirichlet\nscores, particularly in the latter. Notably, after fine-tuning, the performance\nof Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental\nmodels that were examined were all still outperformed by the BERT and RoBERTa\nmodels in terms of cross-entropy scores. Our fine-tuning approach, which\nimproves the model's understanding of text distributions in a text-to-text\nformat, effectively enhances performance and alignment in complex\ndecision-making contexts, underscoring the need for further research to refine\nethical reasoning techniques and capture human judgment nuances."
                },
                "authors": [
                    {
                        "name": "Pranav Senthilkumar"
                    },
                    {
                        "name": "Visshwa Balasubramanian"
                    },
                    {
                        "name": "Prisha Jain"
                    },
                    {
                        "name": "Aneesa Maity"
                    },
                    {
                        "name": "Jonathan Lu"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Accepted to NeurIPS 2024, SoLaR workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07825v1",
                "updated": "2024-10-10T11:23:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:23:18Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    23,
                    18,
                    3,
                    284,
                    0
                ],
                "title": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models"
                },
                "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "18 Pages. Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10167v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10167v4",
                "updated": "2024-10-10T11:17:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    17,
                    10,
                    3,
                    284,
                    0
                ],
                "published": "2024-07-14T11:41:03Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    11,
                    41,
                    3,
                    6,
                    196,
                    0
                ],
                "title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs."
                },
                "authors": [
                    {
                        "name": "Xunyu Zhu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Can Ma"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Major Updates:1.fix faults in the error analysis, 2. improve our\n  method, 3. use ChatGPT as teacher LLMs to ensure fairness in performance\n  comparisons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10167v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10167v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07820v1",
                "updated": "2024-10-10T11:11:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    11,
                    32,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:11:32Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    11,
                    32,
                    3,
                    284,
                    0
                ],
                "title": "Mitigating Gender Bias in Code Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Gender Bias in Code Large Language Models via Model Editing"
                },
                "summary": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity."
                },
                "authors": [
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Zecheng Wang"
                    },
                    {
                        "name": "Deyuan Liu"
                    },
                    {
                        "name": "Cunhang Fan"
                    },
                    {
                        "name": "Zhao Lv"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    },
                    {
                        "name": "Dianbo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Sui"
                },
                "author": "Dianbo Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07819v1",
                "updated": "2024-10-10T11:09:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    9,
                    0,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T11:09:00Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    9,
                    0,
                    3,
                    284,
                    0
                ],
                "title": "Uncovering Overfitting in Large Language Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Overfitting in Large Language Model Editing"
                },
                "summary": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nof limited effectiveness in knowledge editing. To overcome this, inspired by\nLLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy\ncalled Learn to Inference (LTI), which introduce a Multi-stage Inference\nConstraint module to guide the edited models in recalling new knowledge\nsimilarly to how unedited LLMs leverage knowledge through in-context learning.\nExtensive experimental results across a wide range of tasks validate the\neffectiveness of LTI in mitigating Editing Overfit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nof limited effectiveness in knowledge editing. To overcome this, inspired by\nLLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy\ncalled Learn to Inference (LTI), which introduce a Multi-stage Inference\nConstraint module to guide the edited models in recalling new knowledge\nsimilarly to how unedited LLMs leverage knowledge through in-context learning.\nExtensive experimental results across a wide range of tasks validate the\neffectiveness of LTI in mitigating Editing Overfit."
                },
                "authors": [
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06930v2",
                "updated": "2024-10-10T11:03:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    3,
                    20,
                    3,
                    284,
                    0
                ],
                "published": "2024-01-12T23:33:01Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    23,
                    33,
                    1,
                    4,
                    12,
                    0
                ],
                "title": "PizzaCommonSense: Learning to Model Commonsense Reasoning about\n  Intermediate Steps in Cooking Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PizzaCommonSense: Learning to Model Commonsense Reasoning about\n  Intermediate Steps in Cooking Recipes"
                },
                "summary": "Understanding procedural texts, such as cooking recipes, is essential for\nenabling machines to follow instructions and reason about tasks, a key aspect\nof intelligent reasoning. In cooking, these instructions can be interpreted as\na series of modifications to a food preparation. For a model to effectively\nreason about cooking recipes, it must accurately discern and understand the\ninputs and outputs of intermediate steps within the recipe. We present a new\ncorpus of cooking recipes enriched with descriptions of intermediate steps that\ndescribe the input and output for each step. PizzaCommonsense serves as a\nbenchmark for the reasoning capabilities of LLMs because it demands rigorous\nexplicit input-output descriptions to demonstrate the acquisition of implicit\ncommonsense knowledge, which is unlikely to be easily memorized. GPT-4 achieves\nonly 26\\% human-evaluated preference for generations, leaving room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding procedural texts, such as cooking recipes, is essential for\nenabling machines to follow instructions and reason about tasks, a key aspect\nof intelligent reasoning. In cooking, these instructions can be interpreted as\na series of modifications to a food preparation. For a model to effectively\nreason about cooking recipes, it must accurately discern and understand the\ninputs and outputs of intermediate steps within the recipe. We present a new\ncorpus of cooking recipes enriched with descriptions of intermediate steps that\ndescribe the input and output for each step. PizzaCommonsense serves as a\nbenchmark for the reasoning capabilities of LLMs because it demands rigorous\nexplicit input-output descriptions to demonstrate the acquisition of implicit\ncommonsense knowledge, which is unlikely to be easily memorized. GPT-4 achieves\nonly 26\\% human-evaluated preference for generations, leaving room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "arxiv_comment": "Findings of EMNLP 2024. The data is available at:\n  https://github.com/adiallo07/PizzaCommonsense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06355v2",
                "updated": "2024-10-10T10:59:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    10,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-08T20:46:39Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    20,
                    46,
                    39,
                    1,
                    282,
                    0
                ],
                "title": "Context-Aware Command Understanding for Tabletop Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Command Understanding for Tabletop Scenarios"
                },
                "summary": "This paper presents a novel hybrid algorithm designed to interpret natural\nhuman commands in tabletop scenarios. By integrating multiple sources of\ninformation, including speech, gestures, and scene context, the system extracts\nactionable instructions for a robot, identifying relevant objects and actions.\nThe system operates in a zero-shot fashion, without reliance on predefined\nobject models, enabling flexible and adaptive use in various environments. We\nassess the integration of multiple deep learning models, evaluating their\nsuitability for deployment in real-world robotic setups. Our algorithm performs\nrobustly across different tasks, combining language processing with visual\ngrounding. In addition, we release a small dataset of video recordings used to\nevaluate the system. This dataset captures real-world interactions in which a\nhuman provides instructions in natural language to a robot, a contribution to\nfuture research on human-robot interaction. We discuss the strengths and\nlimitations of the system, with particular focus on how it handles multimodal\ncommand interpretation, and its ability to be integrated into symbolic robotic\nframeworks for safe and explainable decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel hybrid algorithm designed to interpret natural\nhuman commands in tabletop scenarios. By integrating multiple sources of\ninformation, including speech, gestures, and scene context, the system extracts\nactionable instructions for a robot, identifying relevant objects and actions.\nThe system operates in a zero-shot fashion, without reliance on predefined\nobject models, enabling flexible and adaptive use in various environments. We\nassess the integration of multiple deep learning models, evaluating their\nsuitability for deployment in real-world robotic setups. Our algorithm performs\nrobustly across different tasks, combining language processing with visual\ngrounding. In addition, we release a small dataset of video recordings used to\nevaluate the system. This dataset captures real-world interactions in which a\nhuman provides instructions in natural language to a robot, a contribution to\nfuture research on human-robot interaction. We discuss the strengths and\nlimitations of the system, with particular focus on how it handles multimodal\ncommand interpretation, and its ability to be integrated into symbolic robotic\nframeworks for safe and explainable decision-making."
                },
                "authors": [
                    {
                        "name": "Paul Gajewski"
                    },
                    {
                        "name": "Antonio Galiza Cerdeira Gonzalez"
                    },
                    {
                        "name": "Bipin Indurkhya"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Indurkhya"
                },
                "author": "Bipin Indurkhya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01379v3",
                "updated": "2024-10-10T10:51:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    10,
                    51,
                    51,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-02T15:20:01Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    15,
                    20,
                    1,
                    3,
                    123,
                    0
                ],
                "title": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and Refinement of Natural Language Explanations through\n  LLM-Symbolic Theorem Proving"
                },
                "summary": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations represent a proxy for evaluating\nexplanation-based and multi-step Natural Language Inference (NLI) models.\nHowever, assessing the validity of explanations for NLI is challenging as it\ntypically involves the crowd-sourcing of apposite datasets, a process that is\ntime-consuming and prone to logical errors. To address existing limitations,\nthis paper investigates the verification and refinement of natural language\nexplanations through the integration of Large Language Models (LLMs) and\nTheorem Provers (TPs). Specifically, we present a neuro-symbolic framework,\nnamed Explanation-Refiner, that integrates TPs with LLMs to generate and\nformalise explanatory sentences and suggest potential inference strategies for\nNLI. In turn, the TP is employed to provide formal guarantees on the logical\nvalidity of the explanations and to generate feedback for subsequent\nimprovements. We demonstrate how Explanation-Refiner can be jointly used to\nevaluate explanatory reasoning, autoformalisation, and error correction\nmechanisms of state-of-the-art LLMs as well as to automatically enhance the\nquality of explanations of variable complexity in different domains."
                },
                "authors": [
                    {
                        "name": "Xin Quan"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Louise A. Dennis"
                    },
                    {
                        "name": "Andr Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andr Freitas"
                },
                "author": "Andr Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13494v2",
                "updated": "2024-10-10T10:43:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    10,
                    43,
                    31,
                    3,
                    284,
                    0
                ],
                "published": "2024-09-20T13:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    13,
                    33,
                    31,
                    4,
                    264,
                    0
                ],
                "title": "Generalizing Deep Learning-Based CSI Feedback in Massive MIMO via\n  ID-Photo-Inspired Preprocessing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Deep Learning-Based CSI Feedback in Massive MIMO via\n  ID-Photo-Inspired Preprocessing"
                },
                "summary": "Deep learning (DL)-based channel state information (CSI) feedback has shown\ngreat potential in improving spectrum efficiency in massive MIMO systems.\nHowever, DL models optimized for specific environments often experience\nperformance degradation in others due to model mismatch. To overcome this\nbarrier in the practical deployment, we propose UniversalNet, an\nID-photo-inspired universal CSI feedback framework that enhances model\ngeneralizability by standardizing the input format across diverse data\ndistributions. Specifically, UniversalNet employs a standardized input format\nto mitigate the influence of environmental variability, coupled with a\nlightweight sparsity-aligning operation in the transformed sparse domain and\nmarginal control bits for original format recovery. This enables seamless\nintegration with existing CSI feedback models, requiring minimal modifications\nin preprocessing and postprocessing without updating neural network weights.\nFurthermore, we propose an efficient eigenvector joint optimization method to\nenhance the sparsity of the precoding matrix by projecting the channel\ncorrelation into the eigenspace, thus improving the implicit CSI compression\nefficiency. Test results demonstrate that UniversalNet effectively improves\ngeneralization performance and ensures precise CSI feedback, even in scenarios\nwith limited training diversity and previously unseen CSI environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL)-based channel state information (CSI) feedback has shown\ngreat potential in improving spectrum efficiency in massive MIMO systems.\nHowever, DL models optimized for specific environments often experience\nperformance degradation in others due to model mismatch. To overcome this\nbarrier in the practical deployment, we propose UniversalNet, an\nID-photo-inspired universal CSI feedback framework that enhances model\ngeneralizability by standardizing the input format across diverse data\ndistributions. Specifically, UniversalNet employs a standardized input format\nto mitigate the influence of environmental variability, coupled with a\nlightweight sparsity-aligning operation in the transformed sparse domain and\nmarginal control bits for original format recovery. This enables seamless\nintegration with existing CSI feedback models, requiring minimal modifications\nin preprocessing and postprocessing without updating neural network weights.\nFurthermore, we propose an efficient eigenvector joint optimization method to\nenhance the sparsity of the precoding matrix by projecting the channel\ncorrelation into the eigenspace, thus improving the implicit CSI compression\nefficiency. Test results demonstrate that UniversalNet effectively improves\ngeneralization performance and ensures precise CSI feedback, even in scenarios\nwith limited training diversity and previously unseen CSI environments."
                },
                "authors": [
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07797v1",
                "updated": "2024-10-10T10:30:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    10,
                    30,
                    28,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T10:30:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    10,
                    30,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Rewriting Conversational Utterances with Instructed Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewriting Conversational Utterances with Instructed Large Language\n  Models"
                },
                "summary": "Many recent studies have shown the ability of large language models (LLMs) to\nachieve state-of-the-art performance on many NLP tasks, such as question\nanswering, text summarization, coding, and translation. In some cases, the\nresults provided by LLMs are on par with those of human experts. These models'\nmost disruptive innovation is their ability to perform tasks via zero-shot or\nfew-shot prompting. This capability has been successfully exploited to train\ninstructed LLMs, where reinforcement learning with human feedback is used to\nguide the model to follow the user's requests directly. In this paper, we\ninvestigate the ability of instructed LLMs to improve conversational search\neffectiveness by rewriting user questions in a conversational setting. We study\nwhich prompts provide the most informative rewritten utterances that lead to\nthe best retrieval performance. Reproducible experiments are conducted on\npublicly-available TREC CAST datasets. The results show that rewriting\nconversational utterances with instructed LLMs achieves significant\nimprovements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and\n11.5% in Recall@500 over state-of-the-art techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent studies have shown the ability of large language models (LLMs) to\nachieve state-of-the-art performance on many NLP tasks, such as question\nanswering, text summarization, coding, and translation. In some cases, the\nresults provided by LLMs are on par with those of human experts. These models'\nmost disruptive innovation is their ability to perform tasks via zero-shot or\nfew-shot prompting. This capability has been successfully exploited to train\ninstructed LLMs, where reinforcement learning with human feedback is used to\nguide the model to follow the user's requests directly. In this paper, we\ninvestigate the ability of instructed LLMs to improve conversational search\neffectiveness by rewriting user questions in a conversational setting. We study\nwhich prompts provide the most informative rewritten utterances that lead to\nthe best retrieval performance. Reproducible experiments are conducted on\npublicly-available TREC CAST datasets. The results show that rewriting\nconversational utterances with instructed LLMs achieves significant\nimprovements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and\n11.5% in Recall@500 over state-of-the-art techniques."
                },
                "authors": [
                    {
                        "name": "Elnara Galimzhanova"
                    },
                    {
                        "name": "Cristina Ioana Muntean"
                    },
                    {
                        "name": "Franco Maria Nardini"
                    },
                    {
                        "name": "Raffaele Perego"
                    },
                    {
                        "name": "Guido Rocchietti"
                    }
                ],
                "author_detail": {
                    "name": "Guido Rocchietti"
                },
                "author": "Guido Rocchietti",
                "arxiv_doi": "10.1109/WI-IAT59888.2023.00014",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WI-IAT59888.2023.00014",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.07797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2023 IEEE/WIC International Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07768v1",
                "updated": "2024-10-10T09:58:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    58,
                    3,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:58:03Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    58,
                    3,
                    3,
                    284,
                    0
                ],
                "title": "Dialectical Behavior Therapy Approach to LLM Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectical Behavior Therapy Approach to LLM Prompting"
                },
                "summary": "Large language models demonstrated state-of-the-art results on various\nreasoning tasks when applying the chain-of-thought (CoT) prompting technique.\nCoT prompting guides the model into breaking tasks into a few intermediate\nsteps and provides step-by-step demonstrations. However, solving complex\nreasoning tasks remains a challenge. In this paper, we propose a novel\nprompting strategy inspired by Dialectical Behavioral Therapy (DBT). DBT, a\nform of cognitive-behavioral therapy, aims to help individuals cope with stress\nby developing a system of reasoning. We applied DBT's basic concepts of shaping\ndialog to construct prompts and conducted experiments on different datasets and\nLLMs with various numbers of parameters. Our results show that prompts crafted\nwith DBT techniques significantly improve results on smaller models, achieving\na 7% increase in accuracy on the StrategyQA, 4.8% on Aqua dataset using 8b\nparameters model, and a 16.2% increase on the StrategyQA, 5.3% on GSM8K dataset\nwith 14b parameters model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrated state-of-the-art results on various\nreasoning tasks when applying the chain-of-thought (CoT) prompting technique.\nCoT prompting guides the model into breaking tasks into a few intermediate\nsteps and provides step-by-step demonstrations. However, solving complex\nreasoning tasks remains a challenge. In this paper, we propose a novel\nprompting strategy inspired by Dialectical Behavioral Therapy (DBT). DBT, a\nform of cognitive-behavioral therapy, aims to help individuals cope with stress\nby developing a system of reasoning. We applied DBT's basic concepts of shaping\ndialog to construct prompts and conducted experiments on different datasets and\nLLMs with various numbers of parameters. Our results show that prompts crafted\nwith DBT techniques significantly improve results on smaller models, achieving\na 7% increase in accuracy on the StrategyQA, 4.8% on Aqua dataset using 8b\nparameters model, and a 16.2% increase on the StrategyQA, 5.3% on GSM8K dataset\nwith 14b parameters model."
                },
                "authors": [
                    {
                        "name": "Oxana Vitman"
                    },
                    {
                        "name": "Nika Amaglobeli"
                    },
                    {
                        "name": "Paul Plachinda"
                    }
                ],
                "author_detail": {
                    "name": "Paul Plachinda"
                },
                "author": "Paul Plachinda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07765v1",
                "updated": "2024-10-10T09:54:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    54,
                    28,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:54:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    54,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language\n  Models Through Traversing 2D Game Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language\n  Models Through Traversing 2D Game Maps"
                },
                "summary": "Large language models (LLMs) have recently demonstrated great success in\ngenerating and understanding natural language. While they have also shown\npotential beyond the domain of natural language, it remains an open question as\nto what extent and in which way these LLMs can plan. We investigate their\nplanning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark\nconsisting of diverse 2D grid-based game maps. An LLM succeeds if it can\ntraverse through given objectives, with a minimum number of steps and a minimum\nnumber of generation errors. We evaluate a number of LLMs on GTB and found that\nGPT-4-Turbo achieved the highest score of 44.97% on GTB\\_Score (GTBS), a\ncomposite score that combines the three above criteria. Furthermore, we\npreliminarily test large reasoning models, namely o1, which scores $67.84\\%$ on\nGTBS, indicating that the benchmark remains challenging for current models.\nCode, data, and documentation are available at\nhttps://github.com/umair-nasir14/Game-Traversal-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated great success in\ngenerating and understanding natural language. While they have also shown\npotential beyond the domain of natural language, it remains an open question as\nto what extent and in which way these LLMs can plan. We investigate their\nplanning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark\nconsisting of diverse 2D grid-based game maps. An LLM succeeds if it can\ntraverse through given objectives, with a minimum number of steps and a minimum\nnumber of generation errors. We evaluate a number of LLMs on GTB and found that\nGPT-4-Turbo achieved the highest score of 44.97% on GTB\\_Score (GTBS), a\ncomposite score that combines the three above criteria. Furthermore, we\npreliminarily test large reasoning models, namely o1, which scores $67.84\\%$ on\nGTBS, indicating that the benchmark remains challenging for current models.\nCode, data, and documentation are available at\nhttps://github.com/umair-nasir14/Game-Traversal-Benchmark."
                },
                "authors": [
                    {
                        "name": "Muhammad Umair Nasir"
                    },
                    {
                        "name": "Steven James"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "arxiv_comment": "Accepted at 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024) Track on Datasets and Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06703v2",
                "updated": "2024-10-10T09:38:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    38,
                    21,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-09T09:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    13,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness\n  in Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness\n  in Web Agents"
                },
                "summary": "Recent advancements in LLM-based web agents have introduced novel\narchitectures and benchmarks showcasing progress in autonomous web navigation\nand interaction. However, most existing benchmarks prioritize effectiveness and\naccuracy, overlooking crucial factors like safety and trustworthiness which are\nessential for deploying web agents in enterprise settings. The risks of unsafe\nweb agent behavior, such as accidentally deleting user accounts or performing\nunintended actions in critical business operations, pose significant barriers\nto widespread adoption. In this paper, we present ST-WebAgentBench, a new\nonline benchmark specifically designed to evaluate the safety and\ntrustworthiness of web agents in enterprise contexts. This benchmark is\ngrounded in a detailed framework that defines safe and trustworthy (ST) agent\nbehavior, outlines how ST policies should be structured and introduces the\nCompletion under Policies metric to assess agent performance. Our evaluation\nreveals that current SOTA agents struggle with policy adherence and cannot yet\nbe relied upon for critical business applications. Additionally, we propose\narchitectural principles aimed at improving policy awareness and compliance in\nweb agents. We open-source this benchmark and invite the community to\ncontribute, with the goal of fostering a new generation of safer, more\ntrustworthy AI agents. All code, data, environment reproduction resources, and\nvideo demonstrations are available at\nhttps://sites.google.com/view/st-webagentbench/home.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based web agents have introduced novel\narchitectures and benchmarks showcasing progress in autonomous web navigation\nand interaction. However, most existing benchmarks prioritize effectiveness and\naccuracy, overlooking crucial factors like safety and trustworthiness which are\nessential for deploying web agents in enterprise settings. The risks of unsafe\nweb agent behavior, such as accidentally deleting user accounts or performing\nunintended actions in critical business operations, pose significant barriers\nto widespread adoption. In this paper, we present ST-WebAgentBench, a new\nonline benchmark specifically designed to evaluate the safety and\ntrustworthiness of web agents in enterprise contexts. This benchmark is\ngrounded in a detailed framework that defines safe and trustworthy (ST) agent\nbehavior, outlines how ST policies should be structured and introduces the\nCompletion under Policies metric to assess agent performance. Our evaluation\nreveals that current SOTA agents struggle with policy adherence and cannot yet\nbe relied upon for critical business applications. Additionally, we propose\narchitectural principles aimed at improving policy awareness and compliance in\nweb agents. We open-source this benchmark and invite the community to\ncontribute, with the goal of fostering a new generation of safer, more\ntrustworthy AI agents. All code, data, environment reproduction resources, and\nvideo demonstrations are available at\nhttps://sites.google.com/view/st-webagentbench/home."
                },
                "authors": [
                    {
                        "name": "Ido Levy"
                    },
                    {
                        "name": "Ben Wiesel"
                    },
                    {
                        "name": "Sami Marreed"
                    },
                    {
                        "name": "Alon Oved"
                    },
                    {
                        "name": "Avi Yaeli"
                    },
                    {
                        "name": "Segev Shlomov"
                    }
                ],
                "author_detail": {
                    "name": "Segev Shlomov"
                },
                "author": "Segev Shlomov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07753v1",
                "updated": "2024-10-10T09:29:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    29,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:29:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    29,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Synthesizing Multi-Class Surgical Datasets with Anatomy-Aware Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Multi-Class Surgical Datasets with Anatomy-Aware Diffusion\n  Models"
                },
                "summary": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. Our codebase\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. Our codebase\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis"
                },
                "authors": [
                    {
                        "name": "Danush Kumar Venkatesh"
                    },
                    {
                        "name": "Dominik Rivoir"
                    },
                    {
                        "name": "Micha Pfeiffer"
                    },
                    {
                        "name": "Fiona Kolbinger"
                    },
                    {
                        "name": "Stefanie Speidel"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Speidel"
                },
                "author": "Stefanie Speidel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07752v1",
                "updated": "2024-10-10T09:28:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:28:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "TVBench: Redesigning Video-Language Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVBench: Redesigning Video-Language Evaluation"
                },
                "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline."
                },
                "authors": [
                    {
                        "name": "Daniel Cores"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Manuel Mucientes"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07745v1",
                "updated": "2024-10-10T09:23:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    23,
                    26,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:23:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    23,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs"
                },
                "summary": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool."
                },
                "authors": [
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Jingtao Zhan"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Zhiqiang Guo"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Ongoning Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07741v1",
                "updated": "2024-10-10T09:19:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    19,
                    43,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:19:43Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    19,
                    43,
                    3,
                    284,
                    0
                ],
                "title": "Membrane Space Telescope: Active Surface Control with Radiative Adaptive\n  Optics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membrane Space Telescope: Active Surface Control with Radiative Adaptive\n  Optics"
                },
                "summary": "Sensitivity and resolution of space telescopes are directly related to the\nsize of the primary mirror. Enabling such future extremely large space\ntelescopes or even arrays of those will require to drastically reduce the areal\nweight of the mirror system. Utilizing a thin parabolic polymeric membrane as\nprimary mirror offers the prospect of very low weight and the flexible nature\nof those membranes allows compactly store them upon launch. Upon deployment the\nstructure is unfolded and the mirror shape restored. Being an extremely thin\nstructure, an active shape correction is required. Utilizing a thermal control\nof the surface via radiative coupling, localized shape changes are imprinted\ninto the membrane telescope. In this paper we present the modelling and\nexperimental test of the radiative adaptive optics. A detailed modeling of the\ninfluence function of the radiative shaping onto the membrane mirror has been\ncarried out. Experimentally we have been radiatively actuated the shape of a\nprototype mirror in closed loop with a wavefront sensor and proven that we can\ncontrol the mirrors surface figure to a ~15nm RMS precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity and resolution of space telescopes are directly related to the\nsize of the primary mirror. Enabling such future extremely large space\ntelescopes or even arrays of those will require to drastically reduce the areal\nweight of the mirror system. Utilizing a thin parabolic polymeric membrane as\nprimary mirror offers the prospect of very low weight and the flexible nature\nof those membranes allows compactly store them upon launch. Upon deployment the\nstructure is unfolded and the mirror shape restored. Being an extremely thin\nstructure, an active shape correction is required. Utilizing a thermal control\nof the surface via radiative coupling, localized shape changes are imprinted\ninto the membrane telescope. In this paper we present the modelling and\nexperimental test of the radiative adaptive optics. A detailed modeling of the\ninfluence function of the radiative shaping onto the membrane mirror has been\ncarried out. Experimentally we have been radiatively actuated the shape of a\nprototype mirror in closed loop with a wavefront sensor and proven that we can\ncontrol the mirrors surface figure to a ~15nm RMS precision."
                },
                "authors": [
                    {
                        "name": "S. Rabien"
                    },
                    {
                        "name": "L. Busoni"
                    },
                    {
                        "name": "C. Del Vecchio"
                    },
                    {
                        "name": "J. Ziegleder"
                    },
                    {
                        "name": "S. Esposito"
                    }
                ],
                "author_detail": {
                    "name": "S. Esposito"
                },
                "author": "S. Esposito",
                "arxiv_doi": "10.1117/12.3019682",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3019682",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.07741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07739v1",
                "updated": "2024-10-10T09:16:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    16,
                    5,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:16:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    16,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture"
                },
                "summary": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Weibo Zheng"
                    },
                    {
                        "name": "Donghong Han"
                    }
                ],
                "author_detail": {
                    "name": "Donghong Han"
                },
                "author": "Donghong Han",
                "arxiv_comment": "11 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07737v1",
                "updated": "2024-10-10T09:15:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    15,
                    14,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T09:15:14Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    15,
                    14,
                    3,
                    284,
                    0
                ],
                "title": "Plug-and-Play Performance Estimation for LLM Services without Relying on\n  Labeled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Performance Estimation for LLM Services without Relying on\n  Labeled Data"
                },
                "summary": "Large Language Model (LLM) services exhibit impressive capability on\nunlearned tasks leveraging only a few examples by in-context learning (ICL).\nHowever, the success of ICL varies depending on the task and context, leading\nto heterogeneous service quality. Directly estimating the performance of LLM\nservices at each invocation can be laborious, especially requiring abundant\nlabeled data or internal information within the LLM. This paper introduces a\nnovel method to estimate the performance of LLM services across different tasks\nand contexts, which can be \"plug-and-play\" utilizing only a few unlabeled\nsamples like ICL. Our findings suggest that the negative log-likelihood and\nperplexity derived from LLM service invocation can function as effective and\nsignificant features. Based on these features, we utilize four distinct\nmeta-models to estimate the performance of LLM services. Our proposed method is\ncompared against unlabeled estimation baselines across multiple LLM services\nand tasks. And it is experimentally applied to two scenarios, demonstrating its\neffectiveness in the selection and further optimization of LLM services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) services exhibit impressive capability on\nunlearned tasks leveraging only a few examples by in-context learning (ICL).\nHowever, the success of ICL varies depending on the task and context, leading\nto heterogeneous service quality. Directly estimating the performance of LLM\nservices at each invocation can be laborious, especially requiring abundant\nlabeled data or internal information within the LLM. This paper introduces a\nnovel method to estimate the performance of LLM services across different tasks\nand contexts, which can be \"plug-and-play\" utilizing only a few unlabeled\nsamples like ICL. Our findings suggest that the negative log-likelihood and\nperplexity derived from LLM service invocation can function as effective and\nsignificant features. Based on these features, we utilize four distinct\nmeta-models to estimate the performance of LLM services. Our proposed method is\ncompared against unlabeled estimation baselines across multiple LLM services\nand tasks. And it is experimentally applied to two scenarios, demonstrating its\neffectiveness in the selection and further optimization of LLM services."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Hongliang Sun"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17532v2",
                "updated": "2024-10-10T09:03:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    3,
                    7,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-25T13:16:34Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    13,
                    16,
                    34,
                    1,
                    177,
                    0
                ],
                "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical\n  Study"
                },
                "summary": "Large language models (LLMs) have shown significant achievements in solving a\nwide range of tasks. Recently, LLMs' capability to store, retrieve and infer\nwith symbolic knowledge has drawn a great deal of attention, showing their\npotential to understand structured information. However, it is not yet known\nwhether LLMs can understand Description Logic (DL) ontologies. In this work, we\nempirically analyze the LLMs' capability of understanding DL-Lite ontologies\ncovering 6 representative tasks from syntactic and semantic aspects. With\nextensive experiments, we demonstrate both the effectiveness and limitations of\nLLMs in understanding DL-Lite ontologies. We find that LLMs can understand\nformal syntax and model-theoretic semantics of concepts and roles. However,\nLLMs struggle with understanding TBox NI transitivity and handling ontologies\nwith large ABoxes. We hope that our experiments and analyses provide more\ninsights into LLMs and inspire to build more faithful knowledge engineering\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant achievements in solving a\nwide range of tasks. Recently, LLMs' capability to store, retrieve and infer\nwith symbolic knowledge has drawn a great deal of attention, showing their\npotential to understand structured information. However, it is not yet known\nwhether LLMs can understand Description Logic (DL) ontologies. In this work, we\nempirically analyze the LLMs' capability of understanding DL-Lite ontologies\ncovering 6 representative tasks from syntactic and semantic aspects. With\nextensive experiments, we demonstrate both the effectiveness and limitations of\nLLMs in understanding DL-Lite ontologies. We find that LLMs can understand\nformal syntax and model-theoretic semantics of concepts and roles. However,\nLLMs struggle with understanding TBox NI transitivity and handling ontologies\nwith large ABoxes. We hope that our experiments and analyses provide more\ninsights into LLMs and inspire to build more faithful knowledge engineering\nsolutions."
                },
                "authors": [
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Songlin Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Zhai"
                },
                "author": "Songlin Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09669v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09669v7",
                "updated": "2024-10-10T08:57:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    57,
                    23,
                    3,
                    284,
                    0
                ],
                "published": "2023-12-15T10:30:36Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    10,
                    30,
                    36,
                    4,
                    349,
                    0
                ],
                "title": "Silent Guardian: Protecting Text from Malicious Exploitation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Guardian: Protecting Text from Malicious Exploitation by Large\n  Language Models"
                },
                "summary": "The rapid development of large language models (LLMs) has yielded impressive\nsuccess in various downstream tasks. However, the vast potential and remarkable\ncapabilities of LLMs also raise new security and privacy concerns if they are\nexploited for nefarious purposes due to their open-endedness. For example, LLMs\nmay be used to plagiarize or imitate writing, thereby infringing the copyright\nof the original content, or to create indiscriminate fake information based on\na certain source text. In some cases, LLMs can even analyze text from the\nInternet to infer personal privacy. Unfortunately, previous text protection\nresearch could not foresee the emergence of powerful LLMs, rendering it no\nlonger effective in this new context. To bridge this gap, we introduce Silent\nGuardian (SG), a text protection mechanism against LLMs, which allows LLMs to\nrefuse to generate response when receiving protected text, preventing the\nmalicious use of text from the source. Specifically, we first propose the\nconcept of Truncation Protection Examples (TPE). By carefully modifying the\ntext to be protected, TPE can induce LLMs to first sample the end token, thus\ndirectly terminating the interaction. In addition, to efficiently construct TPE\nin the discrete space of text data, we propose a novel optimization algorithm\ncalled Super Tailored Protection (STP), which is not only highly efficient but\nalso maintains the semantic consistency of the text during the optimization\nprocess. The comprehensive experimental evaluation demonstrates that SG can\neffectively protect the target text under various configurations and achieve\nalmost 100% protection success rate in some cases. Notably, SG also exhibits\nrelatively good transferability and robustness, making its application in\npractical scenarios possible. Our code is available at\nhttps://github.com/weiyezhimeng/Silent-Guardian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has yielded impressive\nsuccess in various downstream tasks. However, the vast potential and remarkable\ncapabilities of LLMs also raise new security and privacy concerns if they are\nexploited for nefarious purposes due to their open-endedness. For example, LLMs\nmay be used to plagiarize or imitate writing, thereby infringing the copyright\nof the original content, or to create indiscriminate fake information based on\na certain source text. In some cases, LLMs can even analyze text from the\nInternet to infer personal privacy. Unfortunately, previous text protection\nresearch could not foresee the emergence of powerful LLMs, rendering it no\nlonger effective in this new context. To bridge this gap, we introduce Silent\nGuardian (SG), a text protection mechanism against LLMs, which allows LLMs to\nrefuse to generate response when receiving protected text, preventing the\nmalicious use of text from the source. Specifically, we first propose the\nconcept of Truncation Protection Examples (TPE). By carefully modifying the\ntext to be protected, TPE can induce LLMs to first sample the end token, thus\ndirectly terminating the interaction. In addition, to efficiently construct TPE\nin the discrete space of text data, we propose a novel optimization algorithm\ncalled Super Tailored Protection (STP), which is not only highly efficient but\nalso maintains the semantic consistency of the text during the optimization\nprocess. The comprehensive experimental evaluation demonstrates that SG can\neffectively protect the target text under various configurations and achieve\nalmost 100% protection success rate in some cases. Notably, SG also exhibits\nrelatively good transferability and robustness, making its application in\npractical scenarios possible. Our code is available at\nhttps://github.com/weiyezhimeng/Silent-Guardian."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Yuang Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_doi": "10.1109/TIFS.2024.3455775",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIFS.2024.3455775",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.09669v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09669v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper was accepted by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10719v3",
                "updated": "2024-10-10T08:30:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    30,
                    17,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-16T16:51:53Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    16,
                    51,
                    53,
                    1,
                    107,
                    0
                ],
                "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions. Our code is publicly available at\nhttps://github.com/openpsi-project/ReaLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions. Our code is publicly available at\nhttps://github.com/openpsi-project/ReaLHF."
                },
                "authors": [
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Wenjie Ye"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "arxiv_comment": "16 pages, 2 figures, 14 tables",
                "arxiv_journal_ref": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00896v3",
                "updated": "2024-10-10T08:27:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    27,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-03-01T15:38:55Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    15,
                    38,
                    55,
                    4,
                    61,
                    0
                ],
                "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large\n  Language Models"
                },
                "summary": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research."
                },
                "authors": [
                    {
                        "name": "Kedi Chen"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yishen He"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13703v3",
                "updated": "2024-10-10T08:25:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    25,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-02-21T11:07:07Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    11,
                    7,
                    7,
                    2,
                    52,
                    0
                ],
                "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand\n  for Multilingual Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand\n  for Multilingual Instructions?"
                },
                "summary": "The adaption of multilingual pre-trained LLMs into eloquent and helpful\nassistants is essential to facilitate their use across different language\nregions. In that spirit, we are the first to conduct an extensive study of the\nperformance of multilingual models instruction-tuned on different language\ncompositions on parallel instruction-tuning benchmarks across a selection of\nthe most spoken Indo-European languages. We systematically examine the effects\nof language and instruction dataset size on a mid-sized and a large,\nmultilingual LLMs by instruction-tuning them on parallel instruction-tuning\ndatasets. Our results demonstrate that instruction-tuning on parallel instead\nof monolingual corpora benefits cross-lingual instruction following\ncapabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment\nHypothesis does not hold in general, as the investigated multilingual 7B\nparameter model presents a counter-example requiring large-scale\ninstruction-tuning datasets. Finally, we conduct a human annotation study to\nunderstand the alignment between human-based and GPT-4-based evaluation within\nmultilingual chat scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adaption of multilingual pre-trained LLMs into eloquent and helpful\nassistants is essential to facilitate their use across different language\nregions. In that spirit, we are the first to conduct an extensive study of the\nperformance of multilingual models instruction-tuned on different language\ncompositions on parallel instruction-tuning benchmarks across a selection of\nthe most spoken Indo-European languages. We systematically examine the effects\nof language and instruction dataset size on a mid-sized and a large,\nmultilingual LLMs by instruction-tuning them on parallel instruction-tuning\ndatasets. Our results demonstrate that instruction-tuning on parallel instead\nof monolingual corpora benefits cross-lingual instruction following\ncapabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment\nHypothesis does not hold in general, as the investigated multilingual 7B\nparameter model presents a counter-example requiring large-scale\ninstruction-tuning datasets. Finally, we conduct a human annotation study to\nunderstand the alignment between human-based and GPT-4-based evaluation within\nmultilingual chat scenarios."
                },
                "authors": [
                    {
                        "name": "Alexander Arno Weber"
                    },
                    {
                        "name": "Klaudia Thellmann"
                    },
                    {
                        "name": "Jan Ebert"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Jens Lehmann"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Mehdi Ali"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Ali"
                },
                "author": "Mehdi Ali",
                "arxiv_comment": "Accepted for EMNLP 2024 (Main), 27 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07706v1",
                "updated": "2024-10-10T08:19:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    19,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T08:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    19,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+\n  Interaction Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+\n  Interaction Trajectories"
                },
                "summary": "Fine-tuning on agent-environment interaction trajectory data holds\nsignificant promise for surfacing generalized agent capabilities in open-source\nlarge language models (LLMs). In this work, we introduce AgentBank, by far the\nlargest trajectory tuning data collection featuring more than 50k diverse\nhigh-quality interaction trajectories which comprises 16 tasks covering five\ndistinct agent skill dimensions. Leveraging a novel annotation pipeline, we are\nable to scale the annotated trajectories and generate a trajectory dataset with\nminimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a\nseries of agent models, Samoyed. Our comparative experiments demonstrate the\neffectiveness of scaling the interaction trajectory data to acquire generalized\nagent capabilities. Additional studies also reveal some key observations\nregarding trajectory tuning and agent skill generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on agent-environment interaction trajectory data holds\nsignificant promise for surfacing generalized agent capabilities in open-source\nlarge language models (LLMs). In this work, we introduce AgentBank, by far the\nlargest trajectory tuning data collection featuring more than 50k diverse\nhigh-quality interaction trajectories which comprises 16 tasks covering five\ndistinct agent skill dimensions. Leveraging a novel annotation pipeline, we are\nable to scale the annotated trajectories and generate a trajectory dataset with\nminimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a\nseries of agent models, Samoyed. Our comparative experiments demonstrate the\neffectiveness of scaling the interaction trajectory data to acquire generalized\nagent capabilities. Additional studies also reveal some key observations\nregarding trajectory tuning and agent skill generalization."
                },
                "authors": [
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Xiutian Zhao"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07698v1",
                "updated": "2024-10-10T08:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    10,
                    53,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T08:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    10,
                    53,
                    3,
                    284,
                    0
                ],
                "title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank\n  Structures"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs\nwhen adapting large language models (LLMs) for downstream applications.\nHowever, traditional first-order (FO) fine-tuning algorithms incur substantial\nmemory overhead due to the need to store activation values for back-propagation\nduring gradient computation, particularly in long-context fine-tuning tasks.\nZeroth-order (ZO) algorithms offer a promising alternative by approximating\ngradients using finite differences of function values, thus eliminating the\nneed for activation storage. Nevertheless, existing ZO methods struggle to\ncapture the low-rank gradient structure common in LLM fine-tuning, leading to\nsuboptimal performance. This paper proposes a low-rank ZO gradient estimator\nand introduces a novel low-rank ZO algorithm (LOZO) that effectively captures\nthis structure in LLMs. We provide convergence guarantees for LOZO by framing\nit as a subspace optimization method. Additionally, its low-rank nature enables\nLOZO to integrate with momentum techniques while incurring negligible extra\nmemory costs. Extensive experiments across various model sizes and downstream\ntasks demonstrate that LOZO and its momentum-based variant outperform existing\nZO methods and closely approach the performance of FO algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs\nwhen adapting large language models (LLMs) for downstream applications.\nHowever, traditional first-order (FO) fine-tuning algorithms incur substantial\nmemory overhead due to the need to store activation values for back-propagation\nduring gradient computation, particularly in long-context fine-tuning tasks.\nZeroth-order (ZO) algorithms offer a promising alternative by approximating\ngradients using finite differences of function values, thus eliminating the\nneed for activation storage. Nevertheless, existing ZO methods struggle to\ncapture the low-rank gradient structure common in LLM fine-tuning, leading to\nsuboptimal performance. This paper proposes a low-rank ZO gradient estimator\nand introduces a novel low-rank ZO algorithm (LOZO) that effectively captures\nthis structure in LLMs. We provide convergence guarantees for LOZO by framing\nit as a subspace optimization method. Additionally, its low-rank nature enables\nLOZO to integrate with momentum techniques while incurring negligible extra\nmemory costs. Extensive experiments across various model sizes and downstream\ntasks demonstrate that LOZO and its momentum-based variant outperform existing\nZO methods and closely approach the performance of FO algorithms."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Liyuan Cao"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07691v1",
                "updated": "2024-10-10T08:01:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    1,
                    42,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T08:01:42Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    8,
                    1,
                    42,
                    3,
                    284,
                    0
                ],
                "title": "Growing Efficient Accurate and Robust Neural Networks on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing Efficient Accurate and Robust Neural Networks on the Edge"
                },
                "summary": "The ubiquitous deployment of deep learning systems on resource-constrained\nEdge devices is hindered by their high computational complexity coupled with\ntheir fragility to out-of-distribution (OOD) data, especially to naturally\noccurring common corruptions. Current solutions rely on the Cloud to train and\ncompress models before deploying to the Edge. This incurs high energy and\nlatency costs in transmitting locally acquired field data to the Cloud while\nalso raising privacy concerns. We propose GEARnn (Growing Efficient, Accurate,\nand Robust neural networks) to grow and train robust networks in-situ, i.e.,\ncompletely on the Edge device. Starting with a low-complexity initial backbone\nnetwork, GEARnn employs One-Shot Growth (OSG) to grow a network satisfying the\nmemory constraints of the Edge device using clean data, and robustifies the\nnetwork using Efficient Robust Augmentation (ERA) to obtain the final network.\nWe demonstrate results on a NVIDIA Jetson Xavier NX, and analyze the trade-offs\nbetween accuracy, robustness, model size, energy consumption, and training\ntime. Our results demonstrate the construction of efficient, accurate, and\nrobust networks entirely on an Edge device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquitous deployment of deep learning systems on resource-constrained\nEdge devices is hindered by their high computational complexity coupled with\ntheir fragility to out-of-distribution (OOD) data, especially to naturally\noccurring common corruptions. Current solutions rely on the Cloud to train and\ncompress models before deploying to the Edge. This incurs high energy and\nlatency costs in transmitting locally acquired field data to the Cloud while\nalso raising privacy concerns. We propose GEARnn (Growing Efficient, Accurate,\nand Robust neural networks) to grow and train robust networks in-situ, i.e.,\ncompletely on the Edge device. Starting with a low-complexity initial backbone\nnetwork, GEARnn employs One-Shot Growth (OSG) to grow a network satisfying the\nmemory constraints of the Edge device using clean data, and robustifies the\nnetwork using Efficient Robust Augmentation (ERA) to obtain the final network.\nWe demonstrate results on a NVIDIA Jetson Xavier NX, and analyze the trade-offs\nbetween accuracy, robustness, model size, energy consumption, and training\ntime. Our results demonstrate the construction of efficient, accurate, and\nrobust networks entirely on an Edge device."
                },
                "authors": [
                    {
                        "name": "Vignesh Sundaresha"
                    },
                    {
                        "name": "Naresh Shanbhag"
                    }
                ],
                "author_detail": {
                    "name": "Naresh Shanbhag"
                },
                "author": "Naresh Shanbhag",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00942v3",
                "updated": "2024-10-10T07:59:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    7,
                    59,
                    9,
                    3,
                    284,
                    0
                ],
                "published": "2024-05-02T02:04:01Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    2,
                    4,
                    1,
                    3,
                    123,
                    0
                ],
                "title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs"
                },
                "summary": "Communication is defined as \"Who says what to whom with what effect\". A\nmessage from a communicator generates downstream receiver effects, also known\nas behavior. Receiver behavior, being a downstream effect of the message,\ncarries rich signals about it. Even after carrying signals about the message,\nthe behavior data is often ignored while training large language models. We\nshow that training LLMs on receiver behavior can actually help improve their\ncontent-understanding abilities. Specifically, we show that training LLMs to\npredict the receiver behavior of likes and comments improves the LLM's\nperformance on a wide variety of downstream content understanding tasks. We\nshow this performance increase over 46 video and image understanding tasks over\n26 benchmark datasets across both 0-shot and fine-tuning settings,\noutperforming many supervised baselines. Moreover, since receiver behavior,\nsuch as likes and comments, is collected by default on the internet and does\nnot need any human annotations to be useful, the performance improvement we get\nafter training on this data is essentially free-lunch. We release the receiver\nbehavior cleaned comments and likes of 750k images and videos collected from\nmultiple platforms along with our instruction-tuning data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication is defined as \"Who says what to whom with what effect\". A\nmessage from a communicator generates downstream receiver effects, also known\nas behavior. Receiver behavior, being a downstream effect of the message,\ncarries rich signals about it. Even after carrying signals about the message,\nthe behavior data is often ignored while training large language models. We\nshow that training LLMs on receiver behavior can actually help improve their\ncontent-understanding abilities. Specifically, we show that training LLMs to\npredict the receiver behavior of likes and comments improves the LLM's\nperformance on a wide variety of downstream content understanding tasks. We\nshow this performance increase over 46 video and image understanding tasks over\n26 benchmark datasets across both 0-shot and fine-tuning settings,\noutperforming many supervised baselines. Moreover, since receiver behavior,\nsuch as likes and comments, is collected by default on the internet and does\nnot need any human annotations to be useful, the performance improvement we get\nafter training on this data is essentially free-lunch. We release the receiver\nbehavior cleaned comments and likes of 750k images and videos collected from\nmultiple platforms along with our instruction-tuning data."
                },
                "authors": [
                    {
                        "name": "Somesh Singh"
                    },
                    {
                        "name": "Harini S I"
                    },
                    {
                        "name": "Yaman K Singla"
                    },
                    {
                        "name": "Veeky Baths"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Changyou Chen"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07688v1",
                "updated": "2024-10-10T07:54:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    7,
                    54,
                    17,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T07:54:17Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    7,
                    54,
                    17,
                    3,
                    284,
                    0
                ],
                "title": "PokeFlex: A Real-World Dataset of Deformable Objects for Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokeFlex: A Real-World Dataset of Deformable Objects for Robotics"
                },
                "summary": "Data-driven methods have shown great potential in solving challenging\nmanipulation tasks, however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this, we\npropose PokeFlex, a dataset featuring real-world paired and annotated\nmultimodal data that includes 3D textured meshes, point clouds, RGB images, and\ndepth maps. Such data can be leveraged for several downstream tasks such as\nonline 3D mesh reconstruction, and it can potentially enable underexplored\napplications such as the real-world deployment of traditional control methods\nbased on mesh simulations. To deal with the challenges posed by real-world 3D\nmesh reconstruction, we leverage a professional volumetric capture system that\nallows complete 360{\\deg} reconstruction. PokeFlex consists of 18 deformable\nobjects with varying stiffness and shapes. Deformations are generated by\ndropping objects onto a flat surface or by poking the objects with a robot arm.\nInteraction forces and torques are also reported for the latter case. Using\ndifferent data modalities, we demonstrated a use case for the PokeFlex dataset\nin online 3D mesh reconstruction. We refer the reader to our website (\nhttps://pokeflex-dataset.github.io/ ) for demos and examples of our dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven methods have shown great potential in solving challenging\nmanipulation tasks, however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this, we\npropose PokeFlex, a dataset featuring real-world paired and annotated\nmultimodal data that includes 3D textured meshes, point clouds, RGB images, and\ndepth maps. Such data can be leveraged for several downstream tasks such as\nonline 3D mesh reconstruction, and it can potentially enable underexplored\napplications such as the real-world deployment of traditional control methods\nbased on mesh simulations. To deal with the challenges posed by real-world 3D\nmesh reconstruction, we leverage a professional volumetric capture system that\nallows complete 360{\\deg} reconstruction. PokeFlex consists of 18 deformable\nobjects with varying stiffness and shapes. Deformations are generated by\ndropping objects onto a flat surface or by poking the objects with a robot arm.\nInteraction forces and torques are also reported for the latter case. Using\ndifferent data modalities, we demonstrated a use case for the PokeFlex dataset\nin online 3D mesh reconstruction. We refer the reader to our website (\nhttps://pokeflex-dataset.github.io/ ) for demos and examples of our dataset."
                },
                "authors": [
                    {
                        "name": "Jan Obrist"
                    },
                    {
                        "name": "Miguel Zamora"
                    },
                    {
                        "name": "Hehui Zheng"
                    },
                    {
                        "name": "Ronan Hinchet"
                    },
                    {
                        "name": "Firat Ozdemir"
                    },
                    {
                        "name": "Juan Zarate"
                    },
                    {
                        "name": "Robert K. Katzschmann"
                    },
                    {
                        "name": "Stelian Coros"
                    }
                ],
                "author_detail": {
                    "name": "Stelian Coros"
                },
                "author": "Stelian Coros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]