[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v2",
                "updated": "2025-07-24T17:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    20,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pál András Papp"
                    },
                    {
                        "name": "Toni Böhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristóbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "José María Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muñoz"
                    },
                    {
                        "name": "Manuel Gil Pérez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anaïs Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18631v2",
                "updated": "2025-07-25T07:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    20,
                    24,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T17:59:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    59,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment"
                },
                "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Zhenghao Lu"
                    },
                    {
                        "name": "Xianyi Wei"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18618v1",
                "updated": "2025-07-24T17:54:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    54,
                    44,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:54:44Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    54,
                    44,
                    3,
                    205,
                    0
                ],
                "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards"
                },
                "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH."
                },
                "authors": [
                    {
                        "name": "Andreea Nica"
                    },
                    {
                        "name": "Ivan Zakazov"
                    },
                    {
                        "name": "Nicolas Mario Baldwin"
                    },
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18607v1",
                "updated": "2025-07-24T17:43:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    43,
                    40,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:43:40Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    43,
                    40,
                    3,
                    205,
                    0
                ],
                "title": "Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents"
                },
                "summary": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods."
                },
                "authors": [
                    {
                        "name": "Xinyuan Yan"
                    },
                    {
                        "name": "Rita Sevastjanova"
                    },
                    {
                        "name": "Sinie van der Ben"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Bei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bei Wang"
                },
                "author": "Bei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18606v1",
                "updated": "2025-07-24T17:42:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    42,
                    30,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:42:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    42,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs"
                },
                "summary": "Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings."
                },
                "authors": [
                    {
                        "name": "Gilberto Cunha"
                    },
                    {
                        "name": "Alexandra Ramôa"
                    },
                    {
                        "name": "André Sequeira"
                    },
                    {
                        "name": "Michael de Oliveira"
                    },
                    {
                        "name": "Luís Barbosa"
                    }
                ],
                "author_detail": {
                    "name": "Luís Barbosa"
                },
                "author": "Luís Barbosa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09150v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09150v4",
                "updated": "2025-07-24T17:36:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    36,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2024-11-14T03:00:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    0,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Information upper bounds in composite quantum systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information upper bounds in composite quantum systems"
                },
                "summary": "The Pusey-Barrett-Rudolph (PBR) no go theorem provides arguments for the\nreality of quantum states, indicating that quantum states ought to be ontic.\nFor $\\psi$-ontology, a $n$-qubits system is specified by $2^n$ complex\nparameters. However, subject to the Holevo bound, an $n$-qubits system can only\nencode at most $n$ bits of classical information. The two form an inexplicable\ncontradiction. Therefore, based on a posterior statistical inference framework\ncompatible with the $\\psi$-ontology perspective, we generally proved the\ninformation upper bound of the 2-qubits system by analyzing the fundamental\ncorrelation structure among the parameters of quantum systems. And we extended\nit to the $n$-qubits system based on the convex optimization process. Our core\nconclusion is: the information-carrying capacity (information upper bound) of\nan $n$-qubits system is $n$ classical bits. The reason of the scale contrast is\nthat the high degree of correlation among the parameters of the quantum system\ncauses the amount of information that the system can carry to only reach the\norder of $O(n)$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pusey-Barrett-Rudolph (PBR) no go theorem provides arguments for the\nreality of quantum states, indicating that quantum states ought to be ontic.\nFor $\\psi$-ontology, a $n$-qubits system is specified by $2^n$ complex\nparameters. However, subject to the Holevo bound, an $n$-qubits system can only\nencode at most $n$ bits of classical information. The two form an inexplicable\ncontradiction. Therefore, based on a posterior statistical inference framework\ncompatible with the $\\psi$-ontology perspective, we generally proved the\ninformation upper bound of the 2-qubits system by analyzing the fundamental\ncorrelation structure among the parameters of quantum systems. And we extended\nit to the $n$-qubits system based on the convex optimization process. Our core\nconclusion is: the information-carrying capacity (information upper bound) of\nan $n$-qubits system is $n$ classical bits. The reason of the scale contrast is\nthat the high degree of correlation among the parameters of the quantum system\ncauses the amount of information that the system can carry to only reach the\norder of $O(n)$."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Dong"
                    },
                    {
                        "name": "Yuexian Hou"
                    },
                    {
                        "name": "Chenguang Zhang"
                    },
                    {
                        "name": "Yingjie Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yingjie Gao"
                },
                "author": "Yingjie Gao",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09150v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09150v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18595v1",
                "updated": "2025-07-24T17:26:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    26,
                    30,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:26:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    26,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "Investigating Mobility in Spatial Biodiversity Models through Recurrence\n  Quantification Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Mobility in Spatial Biodiversity Models through Recurrence\n  Quantification Analysis"
                },
                "summary": "Recurrence plots and their associated quantifiers provide a robust framework\nfor detecting and characterising complex patterns in non-linear time-series. In\nthis paper, we employ recurrence quantification analysis to investigate the\ndynamics of the cyclic, non-hierarchical May-Leonard model, also referred to as\nrock--paper--scissors systems, that describes competitive interactions among\nthree species. A crucial control parameter in these systems is the species'\nmobility $m$, which governs the spatial displacement of individuals and\nprofoundly influences the resulting dynamics. By systematically varying $m$ and\nconstructing suitable recurrence plots from numerical simulations, we explore\nhow recurrence quantifiers reflect distinct dynamical features associated with\ndifferent ecological states. We then introduce an ensemble-based approach that\nleverages statistical distributions of recurrence quantifiers, computed from\nnumerous independent realisations, allowing us to identify dynamical outliers\nas significant deviations from typical system behaviour. Through detailed\nnumerical analyses, we demonstrate that these outliers correspond to divergent\necological regimes associated with specific mobility values, providing also a\nrobust manner to infer the mobility parameter from observed numerical data. Our\nresults highlight the potential of recurrence-based methods as diagnostic tools\nfor analysing spatial ecological systems and extracting ecologically relevant\ninformation from their non-linear dynamical patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrence plots and their associated quantifiers provide a robust framework\nfor detecting and characterising complex patterns in non-linear time-series. In\nthis paper, we employ recurrence quantification analysis to investigate the\ndynamics of the cyclic, non-hierarchical May-Leonard model, also referred to as\nrock--paper--scissors systems, that describes competitive interactions among\nthree species. A crucial control parameter in these systems is the species'\nmobility $m$, which governs the spatial displacement of individuals and\nprofoundly influences the resulting dynamics. By systematically varying $m$ and\nconstructing suitable recurrence plots from numerical simulations, we explore\nhow recurrence quantifiers reflect distinct dynamical features associated with\ndifferent ecological states. We then introduce an ensemble-based approach that\nleverages statistical distributions of recurrence quantifiers, computed from\nnumerous independent realisations, allowing us to identify dynamical outliers\nas significant deviations from typical system behaviour. Through detailed\nnumerical analyses, we demonstrate that these outliers correspond to divergent\necological regimes associated with specific mobility values, providing also a\nrobust manner to infer the mobility parameter from observed numerical data. Our\nresults highlight the potential of recurrence-based methods as diagnostic tools\nfor analysing spatial ecological systems and extracting ecologically relevant\ninformation from their non-linear dynamical patterns."
                },
                "authors": [
                    {
                        "name": "Matheus Palmero"
                    },
                    {
                        "name": "Matheus Bongestab"
                    }
                ],
                "author_detail": {
                    "name": "Matheus Bongestab"
                },
                "author": "Matheus Bongestab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14314v2",
                "updated": "2025-07-24T17:10:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    10,
                    17,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-18T18:39:07Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    39,
                    7,
                    4,
                    199,
                    0
                ],
                "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes You CLIC: Detection of Croatian Clickbait Headlines"
                },
                "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs."
                },
                "authors": [
                    {
                        "name": "Marija Anđelić"
                    },
                    {
                        "name": "Dominik Šipek"
                    },
                    {
                        "name": "Laura Majer"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "arxiv_comment": "Accepted at Slavic NLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18584v1",
                "updated": "2025-07-24T17:03:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    3,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:03:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    3,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs"
                },
                "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Ke"
                    },
                    {
                        "name": "Hexuan Deng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Jun Rao"
                    },
                    {
                        "name": "Zhenxi Song"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "32 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18580v1",
                "updated": "2025-07-24T16:56:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    56,
                    38,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:56:38Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    56,
                    38,
                    3,
                    205,
                    0
                ],
                "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese\n  Hate Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese\n  Hate Speech Recognition"
                },
                "summary": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Ramen Liu"
                    },
                    {
                        "name": "Longhui Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "8 pages, 3 figures, accepted as oral presentation at CCL25-Eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11636v2",
                "updated": "2025-07-24T16:55:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    55,
                    17,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-14T17:54:39Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    54,
                    39,
                    4,
                    73,
                    0
                ],
                "title": "Towards Markov-State Holography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Markov-State Holography"
                },
                "summary": "Experiments, in particular on biological systems, typically probe\nlower-dimensional observables which are projections of high-dimensional\ndynamics. In order to infer consistent models capturing the relevant dynamics\nof the system, it is important to detect and account for the memory in the\ndynamics. We develop a method to infer the presence of hidden states and\ntransition pathways based on observable transition probabilities conditioned on\nhistory sequences for projected (i.e. observed) dynamics of Markov processes.\nHistograms conditioned on histories reveal information on the transition\nprobabilities of hidden paths locally between any specific pair of observed\nstates. The convergence rate of these histograms towards a stationary\ndistribution provides a local quantification of the duration of memory, which\nreflects how distinct microscopic paths projecting onto the same observed\ntransition decorrelate in path space. This motivates the notion of \"weak Markov\norder\" and provides insight about the hidden topology of microscopic paths in a\nholography-like fashion. The method can be used to test for the local Markov\nproperty of observables. The information extracted is also helpful in inferring\nrelevant hidden transitions which are not captured by a Markov-state model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments, in particular on biological systems, typically probe\nlower-dimensional observables which are projections of high-dimensional\ndynamics. In order to infer consistent models capturing the relevant dynamics\nof the system, it is important to detect and account for the memory in the\ndynamics. We develop a method to infer the presence of hidden states and\ntransition pathways based on observable transition probabilities conditioned on\nhistory sequences for projected (i.e. observed) dynamics of Markov processes.\nHistograms conditioned on histories reveal information on the transition\nprobabilities of hidden paths locally between any specific pair of observed\nstates. The convergence rate of these histograms towards a stationary\ndistribution provides a local quantification of the duration of memory, which\nreflects how distinct microscopic paths projecting onto the same observed\ntransition decorrelate in path space. This motivates the notion of \"weak Markov\norder\" and provides insight about the hidden topology of microscopic paths in a\nholography-like fashion. The method can be used to test for the local Markov\nproperty of observables. The information extracted is also helpful in inferring\nrelevant hidden transitions which are not captured by a Markov-state model."
                },
                "authors": [
                    {
                        "name": "Xizhu Zhao"
                    },
                    {
                        "name": "Dmitrii E. Makarov"
                    },
                    {
                        "name": "Aljaž Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaž Godec"
                },
                "author": "Aljaž Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12548v3",
                "updated": "2025-07-24T16:54:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    54,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2024-06-18T12:25:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    25,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via\n  Mixture of Specialized LoRA Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via\n  Mixture of Specialized LoRA Experts"
                },
                "summary": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality."
                },
                "authors": [
                    {
                        "name": "Yuhao Dan"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Junfeng Tian"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18578v1",
                "updated": "2025-07-24T16:51:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    51,
                    33,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:51:33Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    51,
                    33,
                    3,
                    205,
                    0
                ],
                "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective\n  DLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective\n  DLLMs"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO."
                },
                "authors": [
                    {
                        "name": "Feng Hong"
                    },
                    {
                        "name": "Geng Yu"
                    },
                    {
                        "name": "Yushi Ye"
                    },
                    {
                        "name": "Haicheng Huang"
                    },
                    {
                        "name": "Huangjie Zheng"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Jiangchao Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchao Yao"
                },
                "author": "Jiangchao Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16809v2",
                "updated": "2025-07-24T16:51:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    51,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T17:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    57,
                    44,
                    1,
                    203,
                    0
                ],
                "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework\n  for Multi-Step and Cross-Cultural Inference with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework\n  for Multi-Step and Cross-Cultural Inference with LLMs"
                },
                "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Da-Chen Lian"
                    },
                    {
                        "name": "Ri-Sheng Huang"
                    },
                    {
                        "name": "Pin-Er Chen"
                    },
                    {
                        "name": "Chunki Lim"
                    },
                    {
                        "name": "You-Kuan Lin"
                    },
                    {
                        "name": "Guan-Yu Tseng"
                    },
                    {
                        "name": "Zi-Cheng Yang"
                    },
                    {
                        "name": "Zhen-Yu Lin"
                    },
                    {
                        "name": "Pin-Cheng Chen"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Kai Hsieh"
                },
                "author": "Shu-Kai Hsieh",
                "arxiv_comment": "42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize\n  contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent,\n  OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each\n  experiment once; statistical tests are needed for strong claims. Revised Sec\n  6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17289v2",
                "updated": "2025-07-24T16:50:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    50,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-23T07:51:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    7,
                    51,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting\n  Compliance Tasks in Enterprise Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compliance Brain Assistant: Conversational Agentic AI for Assisting\n  Compliance Tasks in Enterprise Environments"
                },
                "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds."
                },
                "authors": [
                    {
                        "name": "Shitong Zhu"
                    },
                    {
                        "name": "Chenhao Fang"
                    },
                    {
                        "name": "Derek Larson"
                    },
                    {
                        "name": "Neel Reddy Pochareddy"
                    },
                    {
                        "name": "Rajeev Rao"
                    },
                    {
                        "name": "Sophie Zeng"
                    },
                    {
                        "name": "Yanqing Peng"
                    },
                    {
                        "name": "Wendy Summer"
                    },
                    {
                        "name": "Alex Goncalves"
                    },
                    {
                        "name": "Arya Pudota"
                    },
                    {
                        "name": "Hervé Robert"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Robert"
                },
                "author": "Hervé Robert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09228v2",
                "updated": "2025-07-24T16:50:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    50,
                    1,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-12T09:56:02Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    9,
                    56,
                    2,
                    5,
                    193,
                    0
                ],
                "title": "Alleviating the Hubble tension with Torsion Condensation (TorC)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating the Hubble tension with Torsion Condensation (TorC)"
                },
                "summary": "Constraints on the cosmological parameters of Torsion Condensation (TorC) are\ninvestigated using Planck 2018 Cosmic Microwave Background data. TorC is a case\nof Poincar\\'e gauge theory -- a formulation of gravity motivated by the gauge\nfield theories underlying fundamental forces in the standard model of particle\nphysics. Unlike general relativity, TorC incorporates intrinsic torsion degrees\nof freedom while maintaining second-order field equations. At specific\nparameter values, it reduces to the $\\Lambda$CDM model, providing a natural\nextension to standard cosmology. The base model of TorC introduces two\nparameters beyond those in $\\Lambda$CDM: the initial value of the torsion\nscalar field and its time derivative -- one can absorb the latter by allowing\nthe dark energy density to float. To constrain these parameters, `PolyChord`\nnested sampling algorithm is employed, interfaced via `Cobaya` with a modified\nversion of `CAMB`. Our results indicate that TorC allows for a larger inferred\nHubble constant, offering a potential resolution to the Hubble tension. Tension\nanalysis using the $R$-statistic shows that TorC alleviates the statistical\ntension between the Planck 2018 and SH0Es 2020 datasets, though this\nimprovement is not sufficient to decisively favour TorC over $\\Lambda$CDM in a\nBayesian model comparison. This study highlights TorC as a compelling theory of\ngravity, demonstrating its potential to address cosmological tensions and\nmotivating further investigations of extended theories of gravity within a\ncosmological context. As current and upcoming surveys -- including Euclid,\nRoman Space Telescope, Vera C. Rubin Observatory, LISA, and Simons Observatory\n-- deliver data on gravity across all scales, they will offer critical tests of\ngravity models like TorC, making the present a pivotal moment for exploring\nextended theories of gravity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the cosmological parameters of Torsion Condensation (TorC) are\ninvestigated using Planck 2018 Cosmic Microwave Background data. TorC is a case\nof Poincar\\'e gauge theory -- a formulation of gravity motivated by the gauge\nfield theories underlying fundamental forces in the standard model of particle\nphysics. Unlike general relativity, TorC incorporates intrinsic torsion degrees\nof freedom while maintaining second-order field equations. At specific\nparameter values, it reduces to the $\\Lambda$CDM model, providing a natural\nextension to standard cosmology. The base model of TorC introduces two\nparameters beyond those in $\\Lambda$CDM: the initial value of the torsion\nscalar field and its time derivative -- one can absorb the latter by allowing\nthe dark energy density to float. To constrain these parameters, `PolyChord`\nnested sampling algorithm is employed, interfaced via `Cobaya` with a modified\nversion of `CAMB`. Our results indicate that TorC allows for a larger inferred\nHubble constant, offering a potential resolution to the Hubble tension. Tension\nanalysis using the $R$-statistic shows that TorC alleviates the statistical\ntension between the Planck 2018 and SH0Es 2020 datasets, though this\nimprovement is not sufficient to decisively favour TorC over $\\Lambda$CDM in a\nBayesian model comparison. This study highlights TorC as a compelling theory of\ngravity, demonstrating its potential to address cosmological tensions and\nmotivating further investigations of extended theories of gravity within a\ncosmological context. As current and upcoming surveys -- including Euclid,\nRoman Space Telescope, Vera C. Rubin Observatory, LISA, and Simons Observatory\n-- deliver data on gravity across all scales, they will offer critical tests of\ngravity models like TorC, making the present a pivotal moment for exploring\nextended theories of gravity."
                },
                "authors": [
                    {
                        "name": "Sinah Legner"
                    },
                    {
                        "name": "Will Handley"
                    },
                    {
                        "name": "Will Barker"
                    }
                ],
                "author_detail": {
                    "name": "Will Barker"
                },
                "author": "Will Barker",
                "arxiv_comment": "21 pages (main text: 12 pages), 9 figures, 7 tables, comments\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18576v1",
                "updated": "2025-07-24T16:49:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    49,
                    19,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:49:19Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    49,
                    19,
                    3,
                    205,
                    0
                ],
                "title": "SafeWork-R1: Coevolving Safety and Intelligence under the\n  AI-45$^{\\circ}$ Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeWork-R1: Coevolving Safety and Intelligence under the\n  AI-45$^{\\circ}$ Law"
                },
                "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI."
                },
                "authors": [
                    {
                        "name": "Shanghai AI Lab"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Yicheng Bao"
                    },
                    {
                        "name": "Guanxu Chen"
                    },
                    {
                        "name": "Mingkang Chen"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Lingjie Chen"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Xinquan Chen"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Dengke Deng"
                    },
                    {
                        "name": "Yizhuo Ding"
                    },
                    {
                        "name": "Dan Ding"
                    },
                    {
                        "name": "Xiaoshan Ding"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Zhichen Dong"
                    },
                    {
                        "name": "Lingxiao Du"
                    },
                    {
                        "name": "Yuyu Fan"
                    },
                    {
                        "name": "Xinshun Feng"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Yuxuan Gao"
                    },
                    {
                        "name": "Ruijun Ge"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Lujun Gui"
                    },
                    {
                        "name": "Jiaxuan Guo"
                    },
                    {
                        "name": "Qianxi He"
                    },
                    {
                        "name": "Yuenan Hou"
                    },
                    {
                        "name": "Xuhao Hu"
                    },
                    {
                        "name": "Hong Huang"
                    },
                    {
                        "name": "Kaichen Huang"
                    },
                    {
                        "name": "Shiyang Huang"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Shanzhe Lei"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xiangtian Li"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Haotian Liang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihua Liu"
                    },
                    {
                        "name": "Zhixuan Liu"
                    },
                    {
                        "name": "Bangwei Liu"
                    },
                    {
                        "name": "Huacan Liu"
                    },
                    {
                        "name": "Yuexiao Liu"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Yudong Lu"
                    },
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Zhenghao Lu"
                    },
                    {
                        "name": "Qitan Lv"
                    },
                    {
                        "name": "Caoyuan Ma"
                    },
                    {
                        "name": "Jiachen Ma"
                    },
                    {
                        "name": "Xiaoya Ma"
                    },
                    {
                        "name": "Zhongtian Ma"
                    },
                    {
                        "name": "Lingyu Meng"
                    },
                    {
                        "name": "Ziqi Miao"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Yuezhang Peng"
                    },
                    {
                        "name": "Yuan Pu"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Xingge Qiao"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Jiashu Qu"
                    },
                    {
                        "name": "Wanying Qu"
                    },
                    {
                        "name": "Wenwen Qu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Dongxing Shi"
                    },
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xinhao Song"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Xuan Tong"
                    },
                    {
                        "name": "Yingchun Wang"
                    },
                    {
                        "name": "Xuhong Wang"
                    },
                    {
                        "name": "Shujie Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yige Wang"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Yuanfu Wang"
                    },
                    {
                        "name": "Futing Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yajie Wang"
                    },
                    {
                        "name": "Muhao Wei"
                    },
                    {
                        "name": "Xiaoyu Wen"
                    },
                    {
                        "name": "Fenghua Weng"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Xingcheng Xu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yulei Ye"
                    },
                    {
                        "name": "Zhenyun Yin"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Jinxuan Zhang"
                    },
                    {
                        "name": "Yexin Zhang"
                    },
                    {
                        "name": "Yinqiang Zheng"
                    },
                    {
                        "name": "Hefeng Zhou"
                    },
                    {
                        "name": "Zhanhui Zhou"
                    },
                    {
                        "name": "Pengyu Zhu"
                    },
                    {
                        "name": "Qingzi Zhu"
                    },
                    {
                        "name": "Yubo Zhu"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16802v3",
                "updated": "2025-07-24T16:46:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    46,
                    58,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T17:52:16Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    52,
                    16,
                    1,
                    203,
                    0
                ],
                "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova."
                },
                "authors": [
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Jingze Song"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18562v1",
                "updated": "2025-07-24T16:36:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    36,
                    47,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:36:47Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    36,
                    47,
                    3,
                    205,
                    0
                ],
                "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation"
                },
                "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference."
                },
                "authors": [
                    {
                        "name": "Jiafeng Xiong"
                    },
                    {
                        "name": "Yuting Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuting Zhao"
                },
                "author": "Yuting Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18560v1",
                "updated": "2025-07-24T16:35:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    35,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:35:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    35,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization"
                },
                "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility."
                },
                "authors": [
                    {
                        "name": "Benjamin Coriat"
                    },
                    {
                        "name": "Eric Benhamou"
                    }
                ],
                "author_detail": {
                    "name": "Eric Benhamou"
                },
                "author": "Eric Benhamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18557v1",
                "updated": "2025-07-24T16:30:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    30,
                    46,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:30:46Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    30,
                    46,
                    3,
                    205,
                    0
                ],
                "title": "Deep Learning for Blood-Brain Barrier Permeability Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning for Blood-Brain Barrier Permeability Prediction"
                },
                "summary": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development."
                },
                "authors": [
                    {
                        "name": "Zihan Yang"
                    },
                    {
                        "name": "Haipeng Gong"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Gong"
                },
                "author": "Haipeng Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14783v2",
                "updated": "2025-07-24T16:25:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    54,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-20T01:50:16Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    1,
                    50,
                    16,
                    6,
                    201,
                    0
                ],
                "title": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards"
                },
                "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs."
                },
                "authors": [
                    {
                        "name": "Derek Li"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Amirreza Kazemi"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Abbas Ghaddar"
                    },
                    {
                        "name": "Mohammad Ali Alomrani"
                    },
                    {
                        "name": "Liheng Ma"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Feng Wen"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mark Coates"
                    },
                    {
                        "name": "Yingxue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yingxue Zhang"
                },
                "author": "Yingxue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18554v1",
                "updated": "2025-07-24T16:23:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    23,
                    11,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:23:11Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    23,
                    11,
                    3,
                    205,
                    0
                ],
                "title": "How weak are weak factors? Uniform inference for signal strength in\n  signal plus noise models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How weak are weak factors? Uniform inference for signal strength in\n  signal plus noise models"
                },
                "summary": "The paper analyzes four classical signal-plus-noise models: the factor model,\nspiked sample covariance matrices, the sum of a Wigner matrix and a low-rank\nperturbation, and canonical correlation analysis with low-rank dependencies.\nThe objective is to construct confidence intervals for the signal strength that\nare uniformly valid across all regimes - strong, weak, and critical signals. We\ndemonstrate that traditional Gaussian approximations fail in the critical\nregime. Instead, we introduce a universal transitional distribution that\nenables valid inference across the entire spectrum of signal strengths. The\napproach is illustrated through applications in macroeconomics and finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper analyzes four classical signal-plus-noise models: the factor model,\nspiked sample covariance matrices, the sum of a Wigner matrix and a low-rank\nperturbation, and canonical correlation analysis with low-rank dependencies.\nThe objective is to construct confidence intervals for the signal strength that\nare uniformly valid across all regimes - strong, weak, and critical signals. We\ndemonstrate that traditional Gaussian approximations fail in the critical\nregime. Instead, we introduce a universal transitional distribution that\nenables valid inference across the entire spectrum of signal strengths. The\napproach is illustrated through applications in macroeconomics and finance."
                },
                "authors": [
                    {
                        "name": "Anna Bykhovskaya"
                    },
                    {
                        "name": "Vadim Gorin"
                    },
                    {
                        "name": "Sasha Sodin"
                    }
                ],
                "author_detail": {
                    "name": "Sasha Sodin"
                },
                "author": "Sasha Sodin",
                "arxiv_comment": "75 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18553v1",
                "updated": "2025-07-24T16:22:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm"
                },
                "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models."
                },
                "authors": [
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08989v3",
                "updated": "2025-07-24T16:21:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    21,
                    10,
                    3,
                    205,
                    0
                ],
                "published": "2024-10-11T17:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "title": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero."
                },
                "authors": [
                    {
                        "name": "Ziming Yu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Sike Wang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "arxiv_comment": "ICCV 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15551v2",
                "updated": "2025-07-24T16:19:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    19,
                    32,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-21T12:28:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders"
                },
                "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Zhifang Fan"
                    },
                    {
                        "name": "Xiaoxie Zhu"
                    },
                    {
                        "name": "Yuchen Jiang"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Haoran Ding"
                    },
                    {
                        "name": "Xinmin Wang"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Zhen Gong"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Qiwei Chen"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Zuotao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuotao Liu"
                },
                "author": "Zuotao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18551v1",
                "updated": "2025-07-24T16:19:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    19,
                    8,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:19:08Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    19,
                    8,
                    3,
                    205,
                    0
                ],
                "title": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration"
                },
                "summary": "Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY."
                },
                "authors": [
                    {
                        "name": "Daniil Morozov"
                    },
                    {
                        "name": "Reuben Dorent"
                    },
                    {
                        "name": "Nazim Haouchine"
                    }
                ],
                "author_detail": {
                    "name": "Nazim Haouchine"
                },
                "author": "Nazim Haouchine",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18546v1",
                "updated": "2025-07-24T16:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    11,
                    14,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:11:14Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    11,
                    14,
                    3,
                    205,
                    0
                ],
                "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface"
                },
                "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2."
                },
                "authors": [
                    {
                        "name": "Urchade Zaratiana"
                    },
                    {
                        "name": "Gil Pasternak"
                    },
                    {
                        "name": "Oliver Boyd"
                    },
                    {
                        "name": "George Hurn-Maloney"
                    },
                    {
                        "name": "Ash Lewis"
                    }
                ],
                "author_detail": {
                    "name": "Ash Lewis"
                },
                "author": "Ash Lewis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18544v1",
                "updated": "2025-07-24T16:10:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    10,
                    20,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:10:20Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    10,
                    20,
                    3,
                    205,
                    0
                ],
                "title": "EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova\n  Originating from a Close Helium-star Binary via Isolated Binary Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova\n  Originating from a Close Helium-star Binary via Isolated Binary Evolution"
                },
                "summary": "SN\\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic\nsupernova (SN Ic-BL) known, showing unique helium absorptions, a late-time\nbroad H$\\alpha$, and an early bump. In this {\\em{Letter}}, we propose a\njet-cocoon origin to explain EP250108a as off-axis cooling emission from a\nmildly relativistic inner cocoon viewed at $\\sim45^\\circ$ and the early bump of\nSN\\,2025kg as the outer cocoon cooling emission, both constraining an energy of\n$\\sim(1-2)\\times10^{52}{\\rm{erg}}$ and a progenitor radius of $\\sim5\\,R_\\odot$.\nTo explain SN\\,2025kg's exceptionally luminous peak, potential energy injection\ninto the $\\sim2.5\\,M_\\odot$ ejecta from a magnetar with initial period\n$\\sim1.7\\,{\\rm{ms}}$ and magnetic field $\\sim2\\times10^{15}{\\rm{G}}$ may be\nrequired, implying a rapidly rotating $\\sim4\\,M_\\odot$ progenitor. Thus, the\nprogenitor may be a low-mass helium star with an extended helium envelope,\nsupported by helium absorption lines and an inferred weak pre-SN wind.\nHydrogen-rich material may reside in the inner ejecta layers, as suggested by\nthe late-time broad H$\\alpha$, possibly originating from main-sequence\ncompanion material evaporated by the magnetar wind. Since the observed\nnear-solar metallicity challenges the popular quasi-chemically homogeneous\nevolution channel, the rapidly rotating helium-star progenitor of\nEP250108a/SN\\,2025kg might attain angular momentum by being tidally spun up by\na main-sequence companion in a close binary formed through isolated binary\nevolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN\\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic\nsupernova (SN Ic-BL) known, showing unique helium absorptions, a late-time\nbroad H$\\alpha$, and an early bump. In this {\\em{Letter}}, we propose a\njet-cocoon origin to explain EP250108a as off-axis cooling emission from a\nmildly relativistic inner cocoon viewed at $\\sim45^\\circ$ and the early bump of\nSN\\,2025kg as the outer cocoon cooling emission, both constraining an energy of\n$\\sim(1-2)\\times10^{52}{\\rm{erg}}$ and a progenitor radius of $\\sim5\\,R_\\odot$.\nTo explain SN\\,2025kg's exceptionally luminous peak, potential energy injection\ninto the $\\sim2.5\\,M_\\odot$ ejecta from a magnetar with initial period\n$\\sim1.7\\,{\\rm{ms}}$ and magnetic field $\\sim2\\times10^{15}{\\rm{G}}$ may be\nrequired, implying a rapidly rotating $\\sim4\\,M_\\odot$ progenitor. Thus, the\nprogenitor may be a low-mass helium star with an extended helium envelope,\nsupported by helium absorption lines and an inferred weak pre-SN wind.\nHydrogen-rich material may reside in the inner ejecta layers, as suggested by\nthe late-time broad H$\\alpha$, possibly originating from main-sequence\ncompanion material evaporated by the magnetar wind. Since the observed\nnear-solar metallicity challenges the popular quasi-chemically homogeneous\nevolution channel, the rapidly rotating helium-star progenitor of\nEP250108a/SN\\,2025kg might attain angular momentum by being tidally spun up by\na main-sequence companion in a close binary formed through isolated binary\nevolution."
                },
                "authors": [
                    {
                        "name": "Jin-Ping Zhu"
                    },
                    {
                        "name": "Jian-He Zheng"
                    },
                    {
                        "name": "Bing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Zhang"
                },
                "author": "Bing Zhang",
                "arxiv_comment": "7 pages, 2 figures, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08510v2",
                "updated": "2025-07-24T16:04:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    4,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-11T15:00:22Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    0,
                    22,
                    1,
                    70,
                    0
                ],
                "title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Knowledge Injection for CLIP-Based Class-Incremental Learning"
                },
                "summary": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE"
                },
                "authors": [
                    {
                        "name": "Da-Wei Zhou"
                    },
                    {
                        "name": "Kai-Wen Li"
                    },
                    {
                        "name": "Jingyi Ning"
                    },
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "Lijun Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "De-Chuan Zhan"
                },
                "author": "De-Chuan Zhan",
                "arxiv_comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV25-ENGINE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18536v1",
                "updated": "2025-07-24T16:03:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    3,
                    29,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:03:29Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    3,
                    29,
                    3,
                    205,
                    0
                ],
                "title": "Is the $3S$-$2D$ mixing strong for the charmonia $ψ(4040)$ and\n  $ψ(4160)$?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the $3S$-$2D$ mixing strong for the charmonia $ψ(4040)$ and\n  $ψ(4160)$?"
                },
                "summary": "In this work, we revisit the $3S$-$2D$ mixing scheme for the charmonia\n$\\psi(4040)$ and $\\psi(4160)$. We introduce a coupled-channel\nmechanism-distinct from the tensor-force contribution in potential models,\nwhich alone is insufficient to induce significant mixing-to describe the mixing\nbetween these states. Our analysis yields mixing angles of $\\theta_1=7^\\circ$\nand $\\theta_2=10^\\circ$, inconsistent with the larger angle inferred from\nexperimental data, such as the di-lectronic widths of the $\\psi(4040)$ and\n$\\psi(4160)$. We discuss possible origins of this discrepancy and emphasize the\nneed for future experiments to resolve it. Precise measurements of the\nresonance parameters and di-lectronic decay widths, via both inclusive and\nexclusive processes, will be crucial in clarifying this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we revisit the $3S$-$2D$ mixing scheme for the charmonia\n$\\psi(4040)$ and $\\psi(4160)$. We introduce a coupled-channel\nmechanism-distinct from the tensor-force contribution in potential models,\nwhich alone is insufficient to induce significant mixing-to describe the mixing\nbetween these states. Our analysis yields mixing angles of $\\theta_1=7^\\circ$\nand $\\theta_2=10^\\circ$, inconsistent with the larger angle inferred from\nexperimental data, such as the di-lectronic widths of the $\\psi(4040)$ and\n$\\psi(4160)$. We discuss possible origins of this discrepancy and emphasize the\nneed for future experiments to resolve it. Precise measurements of the\nresonance parameters and di-lectronic decay widths, via both inclusive and\nexclusive processes, will be crucial in clarifying this issue."
                },
                "authors": [
                    {
                        "name": "Zi-Long Man"
                    },
                    {
                        "name": "Si-Qiang Luo"
                    },
                    {
                        "name": "Xiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Liu"
                },
                "author": "Xiang Liu",
                "arxiv_comment": "6 pages, 4 table and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18531v1",
                "updated": "2025-07-24T15:58:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    58,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:58:36Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    58,
                    36,
                    3,
                    205,
                    0
                ],
                "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning"
                },
                "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet."
                },
                "authors": [
                    {
                        "name": "Tianheng Qiu"
                    },
                    {
                        "name": "Jingchun Gao"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Huiyi Leong"
                    },
                    {
                        "name": "Xuan Huang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Xiaocheng Zhang"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Lan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Zhang"
                },
                "author": "Lan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17041v2",
                "updated": "2025-07-24T15:51:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    51,
                    28,
                    3,
                    205,
                    0
                ],
                "published": "2024-06-24T18:02:06Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    18,
                    2,
                    6,
                    0,
                    176,
                    0
                ],
                "title": "A New Gravitational Wave Probe to the Nature of Dark Energy from the\n  Aging of the Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Gravitational Wave Probe to the Nature of Dark Energy from the\n  Aging of the Universe"
                },
                "summary": "One of the most dominant energy budgets in the Universe is Dark Energy, which\nremains enigmatic since its existence was first claimed based on observations\nof late-time cosmic acceleration. We propose a new way of inferring the dark\nenergy equation of state (EoS) by measuring the aging of the Universe using\nonly gravitational wave (GW) signals from coalescing binary compact objects of\nany masses. We show that the behavior of dark energy as the Universe ages will\nlead to a change in the observed chirp mass of GW sources inferred from\nobservations of different stages of their coalescence. This change can be\nstudied by monitoring a coherent source over a few years, with two\nwell-separated GW frequencies. With a coordinated network of GW detectors that\ncan reach a sensitivity of Big Bang Observer, we can reach a $5\\sigma$\ndetection of the dark energy EoS parameter $w_0=-1$ and its variation with\ncosmic time by using stellar origin binary black holes and binary neutron stars\nup to high redshift over 10 years of observation time without using any\nexternal calibrator. If the next generation of GW detectors can achieve this\nprecision, then it can open a new window to discover the fundamental nature of\ndark energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most dominant energy budgets in the Universe is Dark Energy, which\nremains enigmatic since its existence was first claimed based on observations\nof late-time cosmic acceleration. We propose a new way of inferring the dark\nenergy equation of state (EoS) by measuring the aging of the Universe using\nonly gravitational wave (GW) signals from coalescing binary compact objects of\nany masses. We show that the behavior of dark energy as the Universe ages will\nlead to a change in the observed chirp mass of GW sources inferred from\nobservations of different stages of their coalescence. This change can be\nstudied by monitoring a coherent source over a few years, with two\nwell-separated GW frequencies. With a coordinated network of GW detectors that\ncan reach a sensitivity of Big Bang Observer, we can reach a $5\\sigma$\ndetection of the dark energy EoS parameter $w_0=-1$ and its variation with\ncosmic time by using stellar origin binary black holes and binary neutron stars\nup to high redshift over 10 years of observation time without using any\nexternal calibrator. If the next generation of GW detectors can achieve this\nprecision, then it can open a new window to discover the fundamental nature of\ndark energy."
                },
                "authors": [
                    {
                        "name": "Suvodip Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Suvodip Mukherjee"
                },
                "author": "Suvodip Mukherjee",
                "arxiv_comment": "6 pages, 4 figures. Accepted for publication in Monthly Notices of\n  the Royal Astronomical Society",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12901v2",
                "updated": "2025-07-24T15:50:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    50,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-17T08:40:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a\nlarge-scale financial reasoning dataset characterized by its systematic CoT\nsynthesis optimization. We first introduce a comprehensive CoT synthesis\npipeline featuring Multi-perspective Knowledge Extraction (MKE) and\nSelf-Corrective Rewriting (SCR) to generate exhaustive and deep financial\nreasoning trajectories. Furthermore, a systematic investigation, termed CoT\nCube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-100K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-100K , hoping to advance the research in financial\nreasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a\nlarge-scale financial reasoning dataset characterized by its systematic CoT\nsynthesis optimization. We first introduce a comprehensive CoT synthesis\npipeline featuring Multi-perspective Knowledge Extraction (MKE) and\nSelf-Corrective Rewriting (SCR) to generate exhaustive and deep financial\nreasoning trajectories. Furthermore, a systematic investigation, termed CoT\nCube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-100K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-100K , hoping to advance the research in financial\nreasoning models."
                },
                "authors": [
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Lihong Wang"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaiyuan Zheng"
                    },
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Li"
                },
                "author": "Zhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02976v2",
                "updated": "2025-07-24T15:50:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    50,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-30T21:10:19Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    10,
                    19,
                    0,
                    181,
                    0
                ],
                "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench"
                },
                "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools."
                },
                "authors": [
                    {
                        "name": "Amirali Sajadi"
                    },
                    {
                        "name": "Kostadin Damevski"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18523v1",
                "updated": "2025-07-24T15:49:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:49:06Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "title": "The Moral Gap of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moral Gap of Large Language Models"
                },
                "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "arxiv_doi": "10.13140/RG.2.2.26221.70880",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.26221.70880",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18522v1",
                "updated": "2025-07-24T15:46:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    46,
                    38,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:46:38Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    46,
                    38,
                    3,
                    205,
                    0
                ],
                "title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy\n  Prediction Using 3D Gaussians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy\n  Prediction Using 3D Gaussians"
                },
                "summary": "3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Tomislav Pavković"
                    },
                    {
                        "name": "Mohammad-Ali Nikouei Mahani"
                    },
                    {
                        "name": "Johannes Niedermayer"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18515v1",
                "updated": "2025-07-24T15:36:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    36,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:36:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    36,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion:\n  Experience on WeChat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Dive into Retrieval-Augmented Generation for Code Completion:\n  Experience on WeChat"
                },
                "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments."
                },
                "authors": [
                    {
                        "name": "Zezhou Yang"
                    },
                    {
                        "name": "Ting Peng"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Hailiang Huang"
                    },
                    {
                        "name": "Yuetang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yuetang Deng"
                },
                "author": "Yuetang Deng",
                "arxiv_comment": "Accepted in ICSME 25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19780v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19780v5",
                "updated": "2025-07-24T15:23:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    23,
                    54,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-24T16:47:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    47,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model\n  Alignment"
                },
                "summary": "Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Yuhui Sun"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Zixi Li"
                    },
                    {
                        "name": "Zhenlong Yuan"
                    },
                    {
                        "name": "Jinman Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Zhao"
                },
                "arxiv_affiliation": "University of Toronto",
                "author": "Jinman Zhao",
                "arxiv_comment": "12 pages, 12 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19780v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19780v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18504v1",
                "updated": "2025-07-24T15:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:22:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18494v1",
                "updated": "2025-07-24T15:08:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    8,
                    26,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:08:26Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    8,
                    26,
                    3,
                    205,
                    0
                ],
                "title": "Partitioned Wild Bootstrap for Panel Data Quantile Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Wild Bootstrap for Panel Data Quantile Regression"
                },
                "summary": "Practical inference procedures for quantile regression models of panel data\nhave been a pervasive concern in empirical work, and can be especially\nchallenging when the panel is observed over many time periods and temporal\ndependence needs to be taken into account. In this paper, we propose a new\nbootstrap method that applies random weighting to a partition of the data --\npartition-invariant weights are used in the bootstrap data generating process\n-- to conduct statistical inference for conditional quantiles in panel data\nthat have significant time-series dependence. We demonstrate that the procedure\nis asymptotically valid for approximating the distribution of the fixed effects\nquantile regression estimator. The bootstrap procedure offers a viable\nalternative to existing resampling methods. Simulation studies show numerical\nevidence that the novel approach has accurate small sample behavior, and an\nempirical application illustrates its use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical inference procedures for quantile regression models of panel data\nhave been a pervasive concern in empirical work, and can be especially\nchallenging when the panel is observed over many time periods and temporal\ndependence needs to be taken into account. In this paper, we propose a new\nbootstrap method that applies random weighting to a partition of the data --\npartition-invariant weights are used in the bootstrap data generating process\n-- to conduct statistical inference for conditional quantiles in panel data\nthat have significant time-series dependence. We demonstrate that the procedure\nis asymptotically valid for approximating the distribution of the fixed effects\nquantile regression estimator. The bootstrap procedure offers a viable\nalternative to existing resampling methods. Simulation studies show numerical\nevidence that the novel approach has accurate small sample behavior, and an\nempirical application illustrates its use."
                },
                "authors": [
                    {
                        "name": "Antonio F. Galvao"
                    },
                    {
                        "name": "Carlos Lamarche"
                    },
                    {
                        "name": "Thomas Parker"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Parker"
                },
                "author": "Thomas Parker",
                "arxiv_comment": "63 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18479v1",
                "updated": "2025-07-24T14:54:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:54:45Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts"
                },
                "summary": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Ngoc Luyen Le"
                    },
                    {
                        "name": "Marie-Hélène Abel"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Hélène Abel"
                },
                "author": "Marie-Hélène Abel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18476v1",
                "updated": "2025-07-24T14:50:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    50,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:50:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    50,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning"
                },
                "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review."
                },
                "authors": [
                    {
                        "name": "Busra Icoz"
                    },
                    {
                        "name": "Goksel Biricik"
                    }
                ],
                "author_detail": {
                    "name": "Goksel Biricik"
                },
                "author": "Goksel Biricik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18455v1",
                "updated": "2025-07-24T14:36:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    36,
                    10,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:36:10Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    36,
                    10,
                    3,
                    205,
                    0
                ],
                "title": "LLM-based Embedders for Prior Case Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Embedders for Prior Case Retrieval"
                },
                "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models."
                },
                "authors": [
                    {
                        "name": "Damith Premasiri"
                    },
                    {
                        "name": "Tharindu Ranasinghe"
                    },
                    {
                        "name": "Ruslan Mitkov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Mitkov"
                },
                "author": "Ruslan Mitkov",
                "arxiv_comment": "Accepted in Recent Advancements in Natural Language Processing (RANLP\n  2025) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18452v1",
                "updated": "2025-07-24T14:35:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    35,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:35:52Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    35,
                    52,
                    3,
                    205,
                    0
                ],
                "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIFFA: Large Language Diffusion Models Can Listen and Understand"
                },
                "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git."
                },
                "authors": [
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Enzhi Wang"
                    },
                    {
                        "name": "Yujie Guo"
                    },
                    {
                        "name": "Haoqin Sun"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20230v2",
                "updated": "2025-07-24T14:28:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    28,
                    28,
                    3,
                    205,
                    0
                ],
                "published": "2025-05-26T17:08:03Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    8,
                    3,
                    0,
                    146,
                    0
                ],
                "title": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from\n  Application Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from\n  Application Code"
                },
                "summary": "In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the U-Schema unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the U-Schema logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the U-Schema unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the U-Schema logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process."
                },
                "authors": [
                    {
                        "name": "Carlos J. Fernandez-Candel"
                    },
                    {
                        "name": "Anthony Cleve"
                    },
                    {
                        "name": "Jesus J. García-Molina"
                    }
                ],
                "author_detail": {
                    "name": "Jesus J. García-Molina"
                },
                "author": "Jesus J. García-Molina",
                "arxiv_comment": "Submitted to Journal Systems and Software, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18442v1",
                "updated": "2025-07-24T14:26:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    26,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:26:41Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    26,
                    41,
                    3,
                    205,
                    0
                ],
                "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data"
                },
                "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data."
                },
                "authors": [
                    {
                        "name": "Rana Alshaikh"
                    },
                    {
                        "name": "Israa Alghanmi"
                    },
                    {
                        "name": "Shelan Jeawak"
                    }
                ],
                "author_detail": {
                    "name": "Shelan Jeawak"
                },
                "author": "Shelan Jeawak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06874v3",
                "updated": "2025-07-24T14:00:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    0,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-07T17:42:21Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    17,
                    42,
                    21,
                    5,
                    158,
                    0
                ],
                "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational\n  Dependencies on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational\n  Dependencies on Large Language Models"
                },
                "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts."
                },
                "authors": [
                    {
                        "name": "Ala Yankouskaya"
                    },
                    {
                        "name": "Areej B. Babiker"
                    },
                    {
                        "name": "Syeda W. F. Rizvi"
                    },
                    {
                        "name": "Sameha Alshakhsi"
                    },
                    {
                        "name": "Magnus Liebherr"
                    },
                    {
                        "name": "Raian Ali"
                    }
                ],
                "author_detail": {
                    "name": "Raian Ali"
                },
                "author": "Raian Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18423v1",
                "updated": "2025-07-24T14:00:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    0,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:00:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    0,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "Multi-Model Ensemble and Reservoir Computing for River Discharge\n  Prediction in Ungauged Basins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Model Ensemble and Reservoir Computing for River Discharge\n  Prediction in Ungauged Basins"
                },
                "summary": "Despite the critical need for accurate flood prediction and water management,\nmany regions lack sufficient river discharge observations, limiting the skill\nof rainfall-runoff analyses. Although numerous physically based and machine\nlearning models exist, achieving high accuracy, interpretability, and\ncomputational efficiency under data-scarce conditions remains a major\nchallenge. We address this challenge with a novel method, HYdrological\nPrediction with multi-model Ensemble and Reservoir computing (HYPER) that\nleverages multi-model ensemble and reservoir computing (RC). Our approach first\napplies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based\nconceptual hydrological models. An RC model is then trained via linear\nregression to correct errors in the BMA output, a non-iterative process that\nensures high computational efficiency. For ungauged basins, we infer the\nrequired BMA and RC weights by linking them to catchment attributes from gauged\nbasins, creating a generalizable framework. We evaluated HYPER using data from\n87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta\nEfficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)\nbut required only 5% of its computational time. In a data-scarce scenario (23%\nof basins gauged), HYPER maintained robust performance (KGE 0.55) and lower\nuncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).\nThese results reveal that individual conceptual hydrological models do not\nnecessarily need to be calibrated when an effectively large ensemble is\nassembled and combined with machine-learning-based bias correction. HYPER\nprovides a robust, efficient, and generalizable solution for discharge\nprediction, particularly in ungauged basins, making it applicable to a wide\nrange of regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the critical need for accurate flood prediction and water management,\nmany regions lack sufficient river discharge observations, limiting the skill\nof rainfall-runoff analyses. Although numerous physically based and machine\nlearning models exist, achieving high accuracy, interpretability, and\ncomputational efficiency under data-scarce conditions remains a major\nchallenge. We address this challenge with a novel method, HYdrological\nPrediction with multi-model Ensemble and Reservoir computing (HYPER) that\nleverages multi-model ensemble and reservoir computing (RC). Our approach first\napplies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based\nconceptual hydrological models. An RC model is then trained via linear\nregression to correct errors in the BMA output, a non-iterative process that\nensures high computational efficiency. For ungauged basins, we infer the\nrequired BMA and RC weights by linking them to catchment attributes from gauged\nbasins, creating a generalizable framework. We evaluated HYPER using data from\n87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta\nEfficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)\nbut required only 5% of its computational time. In a data-scarce scenario (23%\nof basins gauged), HYPER maintained robust performance (KGE 0.55) and lower\nuncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).\nThese results reveal that individual conceptual hydrological models do not\nnecessarily need to be calibrated when an effectively large ensemble is\nassembled and combined with machine-learning-based bias correction. HYPER\nprovides a robust, efficient, and generalizable solution for discharge\nprediction, particularly in ungauged basins, making it applicable to a wide\nrange of regions."
                },
                "authors": [
                    {
                        "name": "Mizuki Funato"
                    },
                    {
                        "name": "Yohei Sawada"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Sawada"
                },
                "author": "Yohei Sawada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18418v1",
                "updated": "2025-07-24T13:57:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:57:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "Distributing Retractions, Weak Distributive Laws and Applications to\n  Monads of Hyperspaces, Continuous Valuations and Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing Retractions, Weak Distributive Laws and Applications to\n  Monads of Hyperspaces, Continuous Valuations and Measures"
                },
                "summary": "Given two monads $S$, $T$ on a category where idempotents split, and a weak\ndistributive law between them, one can build a combined monad $U$. Making\nexplicit what this monad $U$ is requires some effort. When we already have an\nidea what $U$ should be, we show how to recognize that $U$ is indeed the\ncombined monad obtained from $S$ and $T$: it suffices to exhibit what we call a\ndistributing retraction of $ST$ onto $U$. We show that distributing retractions\nand weak distributive laws are in one-to-one correspondence, in a 2-categorical\nsetting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin\nhyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad\nof previsions or of forks, depending on the case. As a byproduct, this allows\nus to describe the algebras of monads of superlinear, resp. sublinear\nprevisions. In the category of compact Hausdorff spaces, the Plotkin hyperspace\nmonad is sometimes known as the Vietoris monad, the monad of probability\nvaluations coincides with the Radon monad, and we infer that the associated\ncombined monad is the monad of normalized forks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given two monads $S$, $T$ on a category where idempotents split, and a weak\ndistributive law between them, one can build a combined monad $U$. Making\nexplicit what this monad $U$ is requires some effort. When we already have an\nidea what $U$ should be, we show how to recognize that $U$ is indeed the\ncombined monad obtained from $S$ and $T$: it suffices to exhibit what we call a\ndistributing retraction of $ST$ onto $U$. We show that distributing retractions\nand weak distributive laws are in one-to-one correspondence, in a 2-categorical\nsetting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin\nhyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad\nof previsions or of forks, depending on the case. As a byproduct, this allows\nus to describe the algebras of monads of superlinear, resp. sublinear\nprevisions. In the category of compact Hausdorff spaces, the Plotkin hyperspace\nmonad is sometimes known as the Vietoris monad, the monad of probability\nvaluations coincides with the Radon monad, and we infer that the associated\ncombined monad is the monad of normalized forks."
                },
                "authors": [
                    {
                        "name": "Jean Goubault-Larrecq"
                    }
                ],
                "author_detail": {
                    "name": "Jean Goubault-Larrecq"
                },
                "author": "Jean Goubault-Larrecq",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18N15, 18C15 (Primary) 54B20, 28A33, 46E27 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18417v1",
                "updated": "2025-07-24T13:57:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    5,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:57:05Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    5,
                    3,
                    205,
                    0
                ],
                "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs"
                },
                "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps)."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13991v2",
                "updated": "2025-07-24T13:56:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    56,
                    45,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-18T14:58:03Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    58,
                    3,
                    4,
                    199,
                    0
                ],
                "title": "Inflated hot Jupiters: Inferring average atmospheric velocity via Ohmic\n  models coupled with internal dynamo evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inflated hot Jupiters: Inferring average atmospheric velocity via Ohmic\n  models coupled with internal dynamo evolution"
                },
                "summary": "The inflated radii observed in hundreds of hot Jupiters (HJ) represent a\nlong-standing open issue. In this study, we quantitatively investigate this\nphenomenon within the framework of Ohmic dissipation arising from magnetic\ninduction in the atmosphere, one of the most promising mechanisms for\nexplaining the radius anomaly. We simulate the evolution of irradiated giant\nplanets with MESA, spanning the observed range of masses and equilibrium\ntemperatures, incorporating an internal source of Ohmic dissipation that\nextends to deep layers of the envelope. We infer average atmospheric wind\nintensities, averaged in the region $p < 10$ bar, in the range 0.01-1 km/s in\norder to reproduce the range of observed radii, decreasing roughly linearly\nwith planetary mass, and much more steeply with equilibrium temperature. This\nis consistent with the expected effects of magnetic drag from the induced\nfield, which is higher for more intense irradiation, via conductivity, and for\nlarger masses, which have higher dynamo fields. Due to the evolution of the\ndynamo field and the proportionality of the induced currents on it, the Ohmic\nefficiency typically decreases by at least one order of magnitude from 0.1 to\n10 Gyr, at contrast with the common assumption of a constant-in-time value.\nNotably, the extent of the main convective region, and the associated heat flux\nsupporting the dynamo, is reduced in the presence of strong Ohmic dissipation,\nwhich in turn depends on the dynamo field strength, generating a non-trivial\ncoupling of the latter with the atmospheric induction, potentially leading to\nan oscillatory behaviour of the field strength. These findings remain generally\nvalid even when accounting for a long-term increase in the main-sequence host\nstar luminosity, although this case can more readily lead to HJ re-inflation,\nconsistent with previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inflated radii observed in hundreds of hot Jupiters (HJ) represent a\nlong-standing open issue. In this study, we quantitatively investigate this\nphenomenon within the framework of Ohmic dissipation arising from magnetic\ninduction in the atmosphere, one of the most promising mechanisms for\nexplaining the radius anomaly. We simulate the evolution of irradiated giant\nplanets with MESA, spanning the observed range of masses and equilibrium\ntemperatures, incorporating an internal source of Ohmic dissipation that\nextends to deep layers of the envelope. We infer average atmospheric wind\nintensities, averaged in the region $p < 10$ bar, in the range 0.01-1 km/s in\norder to reproduce the range of observed radii, decreasing roughly linearly\nwith planetary mass, and much more steeply with equilibrium temperature. This\nis consistent with the expected effects of magnetic drag from the induced\nfield, which is higher for more intense irradiation, via conductivity, and for\nlarger masses, which have higher dynamo fields. Due to the evolution of the\ndynamo field and the proportionality of the induced currents on it, the Ohmic\nefficiency typically decreases by at least one order of magnitude from 0.1 to\n10 Gyr, at contrast with the common assumption of a constant-in-time value.\nNotably, the extent of the main convective region, and the associated heat flux\nsupporting the dynamo, is reduced in the presence of strong Ohmic dissipation,\nwhich in turn depends on the dynamo field strength, generating a non-trivial\ncoupling of the latter with the atmospheric induction, potentially leading to\nan oscillatory behaviour of the field strength. These findings remain generally\nvalid even when accounting for a long-term increase in the main-sequence host\nstar luminosity, although this case can more readily lead to HJ re-inflation,\nconsistent with previous studies."
                },
                "authors": [
                    {
                        "name": "Daniele Viganò"
                    },
                    {
                        "name": "Soumya Sengupta"
                    },
                    {
                        "name": "Clàudia Soriano-Guerrero"
                    },
                    {
                        "name": "Rosalba Perna"
                    },
                    {
                        "name": "Albert Elias-López"
                    },
                    {
                        "name": "Sandeep Kumar"
                    },
                    {
                        "name": "Taner Akgün"
                    }
                ],
                "author_detail": {
                    "name": "Taner Akgün"
                },
                "author": "Taner Akgün",
                "arxiv_doi": "10.1051/0004-6361/202555219",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202555219",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.13991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in A&A main journal. 22 pages, 14 figures, 1\n  table. Comments welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07893v3",
                "updated": "2025-07-24T13:52:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    52,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T16:22:41Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    22,
                    41,
                    3,
                    191,
                    0
                ],
                "title": "An Integrated Framework of Prompt Engineering and Multidimensional\n  Knowledge Graphs for Legal Dispute Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Integrated Framework of Prompt Engineering and Multidimensional\n  Knowledge Graphs for Legal Dispute Analysis"
                },
                "summary": "Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems."
                },
                "authors": [
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jianglong Qing"
                    },
                    {
                        "name": "Qing xu"
                    },
                    {
                        "name": "Kaiwen Pan"
                    },
                    {
                        "name": "Ting luo"
                    }
                ],
                "author_detail": {
                    "name": "Ting luo"
                },
                "author": "Ting luo",
                "arxiv_comment": "19 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30, 91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; K.5.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15487v3",
                "updated": "2025-07-24T13:47:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    47,
                    56,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-21T14:23:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size."
                },
                "authors": [
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Serena Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Lucia Passaro"
                    },
                    {
                        "name": "Irene Sucameli"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "Accepted for publication in Findings of ACL 2025",
                "arxiv_journal_ref": "In Findings of the Association for Computational Linguistics: ACL\n  2025, pages 17335-17355, Vienna, Austria. Association for Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18397v1",
                "updated": "2025-07-24T13:28:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    28,
                    44,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:28:44Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    28,
                    44,
                    3,
                    205,
                    0
                ],
                "title": "Correlation and Redundancy of Time-Delay Interferometry Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlation and Redundancy of Time-Delay Interferometry Configurations"
                },
                "summary": "Time-Delay Interferometry (TDI) is essential for space-based gravitational\nwave (GW) missions, as it suppresses laser frequency noise and achieve the\nrequired sensitivity. Beyond the standard Michelson configuration, a variety of\nsecond-generation TDI schemes have been proposed, each utilizing different\ncombinations of inter-spacecraft laser links. In this work, we conduct a\ncomparative study of several representative TDI configurations with varying\ntime spans and demonstrate that their (quasi-)orthogonal channels are highly\ncorrelated, indicating substantial redundancy among these schemes. In the\nlow-frequency regime, the performance of different TDI configurations are\nnearly identical. Their distinctions emerge primarily at high frequencies,\nwhere the GW wavelength becomes comparable to the arm length. In this regime,\nshorter TDI time spans with minimal null frequencies facilitate more accurate\nwaveform modeling and parameter recovery in frequency domain. In contrast,\nconfigurations with longer time spans and more null frequencies, such as the\nMichelson, are more susceptible to frequency aliasing and waveform modulation\neffects, which degrade inference accuracy. However, if signal modeling and\nanalysis are performed in the time domain, all TDI configurations become\neffectively equivalent. Considering the usability in both frequency and time\ndomain, the short-span PD4L scheme, which exhibits minimal nulls and superior\nperformance in high frequencies, emerges as a promising candidate for future\nspace-based GW mission designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Delay Interferometry (TDI) is essential for space-based gravitational\nwave (GW) missions, as it suppresses laser frequency noise and achieve the\nrequired sensitivity. Beyond the standard Michelson configuration, a variety of\nsecond-generation TDI schemes have been proposed, each utilizing different\ncombinations of inter-spacecraft laser links. In this work, we conduct a\ncomparative study of several representative TDI configurations with varying\ntime spans and demonstrate that their (quasi-)orthogonal channels are highly\ncorrelated, indicating substantial redundancy among these schemes. In the\nlow-frequency regime, the performance of different TDI configurations are\nnearly identical. Their distinctions emerge primarily at high frequencies,\nwhere the GW wavelength becomes comparable to the arm length. In this regime,\nshorter TDI time spans with minimal null frequencies facilitate more accurate\nwaveform modeling and parameter recovery in frequency domain. In contrast,\nconfigurations with longer time spans and more null frequencies, such as the\nMichelson, are more susceptible to frequency aliasing and waveform modulation\neffects, which degrade inference accuracy. However, if signal modeling and\nanalysis are performed in the time domain, all TDI configurations become\neffectively equivalent. Considering the usability in both frequency and time\ndomain, the short-span PD4L scheme, which exhibits minimal nulls and superior\nperformance in high frequencies, emerges as a promising candidate for future\nspace-based GW mission designs."
                },
                "authors": [
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18392v1",
                "updated": "2025-07-24T13:15:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    15,
                    21,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:15:21Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    15,
                    21,
                    3,
                    205,
                    0
                ],
                "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy"
                },
                "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study."
                },
                "authors": [
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    }
                ],
                "author_detail": {
                    "name": "Michal Shmueli-Scheuer"
                },
                "author": "Michal Shmueli-Scheuer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18391v1",
                "updated": "2025-07-24T13:14:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    14,
                    25,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:14:25Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    14,
                    25,
                    3,
                    205,
                    0
                ],
                "title": "Revisiting LLM Reasoning via Information Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting LLM Reasoning via Information Bottleneck"
                },
                "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance."
                },
                "authors": [
                    {
                        "name": "Shiye Lei"
                    },
                    {
                        "name": "Zhihao Cheng"
                    },
                    {
                        "name": "Kai Jia"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23276v2",
                "updated": "2025-07-24T13:13:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    13,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-29T15:02:47Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    2,
                    47,
                    6,
                    180,
                    0
                ],
                "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games"
                },
                "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim"
                },
                "authors": [
                    {
                        "name": "David Guzman Piedrahita"
                    },
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Giorgia Ramponi"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "arxiv_comment": "Published at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18382v1",
                "updated": "2025-07-24T12:57:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    57,
                    22,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:57:22Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    57,
                    22,
                    3,
                    205,
                    0
                ],
                "title": "Towards Consistent Long-Term Pose Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Consistent Long-Term Pose Generation"
                },
                "summary": "Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios."
                },
                "authors": [
                    {
                        "name": "Yayuan Li"
                    },
                    {
                        "name": "Filippos Bellos"
                    },
                    {
                        "name": "Jason Corso"
                    }
                ],
                "author_detail": {
                    "name": "Jason Corso"
                },
                "author": "Jason Corso",
                "arxiv_comment": "10 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18380v1",
                "updated": "2025-07-24T12:56:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    56,
                    16,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:56:16Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    56,
                    16,
                    3,
                    205,
                    0
                ],
                "title": "ARTreeFormer: A Faster Attention-based Autoregressive Model for\n  Phylogenetic Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARTreeFormer: A Faster Attention-based Autoregressive Model for\n  Phylogenetic Inference"
                },
                "summary": "Probabilistic modeling over the combinatorially large space of tree\ntopologies remains a central challenge in phylogenetic inference. Previous\napproaches often necessitate pre-sampled tree topologies, limiting their\nmodeling capability to a subset of the entire tree space. A recent advancement\nis ARTree, a deep autoregressive model that offers unrestricted distributions\nfor tree topologies. However, its reliance on repetitive tree traversals and\ninefficient local message passing for computing topological node\nrepresentations may hamper the scalability to large datasets. This paper\nproposes ARTreeFormer, a novel approach that harnesses fixed-point iteration\nand attention mechanisms to accelerate ARTree. By introducing a fixed-point\niteration algorithm for computing the topological node embeddings, ARTreeFormer\nallows fast vectorized computation, especially on CUDA devices. This, together\nwith an attention-based global message passing scheme, significantly improves\nthe computation speed of ARTree while maintaining great approximation\nperformance. We demonstrate the effectiveness and efficiency of our method on a\nbenchmark of challenging real data phylogenetic inference problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic modeling over the combinatorially large space of tree\ntopologies remains a central challenge in phylogenetic inference. Previous\napproaches often necessitate pre-sampled tree topologies, limiting their\nmodeling capability to a subset of the entire tree space. A recent advancement\nis ARTree, a deep autoregressive model that offers unrestricted distributions\nfor tree topologies. However, its reliance on repetitive tree traversals and\ninefficient local message passing for computing topological node\nrepresentations may hamper the scalability to large datasets. This paper\nproposes ARTreeFormer, a novel approach that harnesses fixed-point iteration\nand attention mechanisms to accelerate ARTree. By introducing a fixed-point\niteration algorithm for computing the topological node embeddings, ARTreeFormer\nallows fast vectorized computation, especially on CUDA devices. This, together\nwith an attention-based global message passing scheme, significantly improves\nthe computation speed of ARTree while maintaining great approximation\nperformance. We demonstrate the effectiveness and efficiency of our method on a\nbenchmark of challenging real data phylogenetic inference problems."
                },
                "authors": [
                    {
                        "name": "Tianyu Xie"
                    },
                    {
                        "name": "Yicong Mao"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18378v1",
                "updated": "2025-07-24T12:54:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    54,
                    8,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:54:08Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    54,
                    8,
                    3,
                    205,
                    0
                ],
                "title": "A comparison of stretched-grid and limited-area modelling for\n  data-driven regional weather forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of stretched-grid and limited-area modelling for\n  data-driven regional weather forecasting"
                },
                "summary": "Regional machine learning weather prediction (MLWP) models based on graph\nneural networks have recently demonstrated remarkable predictive accuracy,\noutperforming numerical weather prediction models at lower computational costs.\nIn particular, limited-area model (LAM) and stretched-grid model (SGM)\napproaches have emerged for generating high-resolution regional forecasts,\nbased on initial conditions from a regional (re)analysis. While LAM uses\nlateral boundaries from an external global model, SGM incorporates a global\ndomain at lower resolution. This study aims to understand how the differences\nin model design impact relative performance and potential applications.\nSpecifically, the strengths and weaknesses of these two approaches are\nidentified for generating deterministic regional forecasts over Europe. Using\nthe Anemoi framework, models of both types are built by minimally adapting a\nshared architecture and trained using global and regional reanalyses in a\nnear-identical setup. Several inference experiments have been conducted to\nexplore their relative performance and highlight key differences. Results show\nthat both LAM and SGM are competitive deterministic MLWP models with generally\naccurate and comparable forecasting performance over the regional domain.\nVarious differences were identified in the performance of the models across\napplications. LAM is able to successfully exploit high-quality boundary\nforcings to make predictions within the regional domain and is suitable in\ncontexts where global data is difficult to acquire. SGM is fully self-contained\nfor easier operationalisation, can take advantage of more training data and\nsignificantly surpasses LAM in terms of (temporal) generalisability. Our paper\ncan serve as a starting point for meteorological institutes to guide their\nchoice between LAM and SGM in developing an operational data-driven forecasting\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regional machine learning weather prediction (MLWP) models based on graph\nneural networks have recently demonstrated remarkable predictive accuracy,\noutperforming numerical weather prediction models at lower computational costs.\nIn particular, limited-area model (LAM) and stretched-grid model (SGM)\napproaches have emerged for generating high-resolution regional forecasts,\nbased on initial conditions from a regional (re)analysis. While LAM uses\nlateral boundaries from an external global model, SGM incorporates a global\ndomain at lower resolution. This study aims to understand how the differences\nin model design impact relative performance and potential applications.\nSpecifically, the strengths and weaknesses of these two approaches are\nidentified for generating deterministic regional forecasts over Europe. Using\nthe Anemoi framework, models of both types are built by minimally adapting a\nshared architecture and trained using global and regional reanalyses in a\nnear-identical setup. Several inference experiments have been conducted to\nexplore their relative performance and highlight key differences. Results show\nthat both LAM and SGM are competitive deterministic MLWP models with generally\naccurate and comparable forecasting performance over the regional domain.\nVarious differences were identified in the performance of the models across\napplications. LAM is able to successfully exploit high-quality boundary\nforcings to make predictions within the regional domain and is suitable in\ncontexts where global data is difficult to acquire. SGM is fully self-contained\nfor easier operationalisation, can take advantage of more training data and\nsignificantly surpasses LAM in terms of (temporal) generalisability. Our paper\ncan serve as a starting point for meteorological institutes to guide their\nchoice between LAM and SGM in developing an operational data-driven forecasting\nsystem."
                },
                "authors": [
                    {
                        "name": "Jasper S. Wijnands"
                    },
                    {
                        "name": "Michiel Van Ginderachter"
                    },
                    {
                        "name": "Bastien François"
                    },
                    {
                        "name": "Sophie Buurman"
                    },
                    {
                        "name": "Piet Termonia"
                    },
                    {
                        "name": "Dieter Van den Bleeken"
                    }
                ],
                "author_detail": {
                    "name": "Dieter Van den Bleeken"
                },
                "author": "Dieter Van den Bleeken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18368v1",
                "updated": "2025-07-24T12:47:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    47,
                    29,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:47:29Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    47,
                    29,
                    3,
                    205,
                    0
                ],
                "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios"
                },
                "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance."
                },
                "authors": [
                    {
                        "name": "Zhuang Qiang Bok"
                    },
                    {
                        "name": "Watson Wei Khong Chua"
                    }
                ],
                "author_detail": {
                    "name": "Watson Wei Khong Chua"
                },
                "author": "Watson Wei Khong Chua",
                "arxiv_comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18366v1",
                "updated": "2025-07-24T12:46:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    40,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:46:40Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    40,
                    3,
                    205,
                    0
                ],
                "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation"
                },
                "summary": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation."
                },
                "authors": [
                    {
                        "name": "Lakshmana Sri Harsha Nemani"
                    },
                    {
                        "name": "P. K. Srijith"
                    },
                    {
                        "name": "Tomasz Kuśmierczyk"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kuśmierczyk"
                },
                "author": "Tomasz Kuśmierczyk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18365v1",
                "updated": "2025-07-24T12:46:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecPS: Privacy Risk Scoring for Recommender Systems"
                },
                "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md."
                },
                "authors": [
                    {
                        "name": "Jiajie He"
                    },
                    {
                        "name": "Yuechun Gu"
                    },
                    {
                        "name": "Keke Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keke Chen"
                },
                "author": "Keke Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12988v3",
                "updated": "2025-07-24T12:42:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    42,
                    50,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-18T16:11:54Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs"
                },
                "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards."
                },
                "authors": [
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ishita Agrawal"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18362v1",
                "updated": "2025-07-24T12:33:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    33,
                    10,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:33:10Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    33,
                    10,
                    3,
                    205,
                    0
                ],
                "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion\n  Model"
                },
                "summary": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable\nperformance across a variety of generative tasks. The inherent randomness in\ndiffusion models helps address issues such as blurring at the edges of medical\nimages and labels, positioning Diffusion Probabilistic Models (DPMs) as a\npromising approach for lesion segmentation. However, we find that the current\ntraining and inference strategies of diffusion models result in an uneven\ndistribution of attention across different timesteps, leading to longer\ntraining times and suboptimal solutions. To this end, we propose UniSegDiff, a\nnovel diffusion model framework designed to address lesion segmentation in a\nunified manner across multiple modalities and organs. This framework introduces\na staged training and inference approach, dynamically adjusting the prediction\ntargets at different stages, forcing the model to maintain high attention\nacross all timesteps, and achieves unified lesion segmentation through\npre-training the feature extraction network for segmentation. We evaluate\nperformance on six different organs across various imaging modalities.\nComprehensive experimental results demonstrate that UniSegDiff significantly\noutperforms previous state-of-the-art (SOTA) approaches. The code is available\nat https://github.com/HUYILONG-Z/UniSegDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable\nperformance across a variety of generative tasks. The inherent randomness in\ndiffusion models helps address issues such as blurring at the edges of medical\nimages and labels, positioning Diffusion Probabilistic Models (DPMs) as a\npromising approach for lesion segmentation. However, we find that the current\ntraining and inference strategies of diffusion models result in an uneven\ndistribution of attention across different timesteps, leading to longer\ntraining times and suboptimal solutions. To this end, we propose UniSegDiff, a\nnovel diffusion model framework designed to address lesion segmentation in a\nunified manner across multiple modalities and organs. This framework introduces\na staged training and inference approach, dynamically adjusting the prediction\ntargets at different stages, forcing the model to maintain high attention\nacross all timesteps, and achieves unified lesion segmentation through\npre-training the feature extraction network for segmentation. We evaluate\nperformance on six different organs across various imaging modalities.\nComprehensive experimental results demonstrate that UniSegDiff significantly\noutperforms previous state-of-the-art (SOTA) approaches. The code is available\nat https://github.com/HUYILONG-Z/UniSegDiff."
                },
                "authors": [
                    {
                        "name": "Yilong Hu"
                    },
                    {
                        "name": "Shijie Chang"
                    },
                    {
                        "name": "Lihe Zhang"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Weibing Sun"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_comment": "MICCAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18352v1",
                "updated": "2025-07-24T12:25:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    25,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:25:12Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    25,
                    12,
                    3,
                    205,
                    0
                ],
                "title": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation"
                },
                "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters."
                },
                "authors": [
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Mattias Teye"
                    },
                    {
                        "name": "Derek Yadgaroff"
                    },
                    {
                        "name": "Judith Bütepage"
                    }
                ],
                "author_detail": {
                    "name": "Judith Bütepage"
                },
                "author": "Judith Bütepage",
                "arxiv_comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08017v3",
                "updated": "2025-07-24T12:23:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    23,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18343v1",
                "updated": "2025-07-24T12:16:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    16,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:16:52Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    16,
                    52,
                    3,
                    205,
                    0
                ],
                "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM\n  Pre-Annotations with Human Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Annotation for Propaganda Detection: Integrating LLM\n  Pre-Annotations with Human Intelligence"
                },
                "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository."
                },
                "authors": [
                    {
                        "name": "Ariana Sahitaj"
                    },
                    {
                        "name": "Premtim Sahitaj"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Jiaao Li"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "NLP4PI at ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18342v1",
                "updated": "2025-07-24T12:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    14,
                    49,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:14:49Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    14,
                    49,
                    3,
                    205,
                    0
                ],
                "title": "EgoExoBench: A Benchmark for First- and Third-person View Video\n  Understanding in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoExoBench: A Benchmark for First- and Third-person View Video\n  Understanding in MLLMs"
                },
                "summary": "Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence."
                },
                "authors": [
                    {
                        "name": "Yuping He"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Jilan Xu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18340v1",
                "updated": "2025-07-24T12:12:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    12,
                    4,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:12:04Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    12,
                    4,
                    3,
                    205,
                    0
                ],
                "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for\n  In-Context Learning"
                },
                "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR."
                },
                "authors": [
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Bingchen Huang"
                    },
                    {
                        "name": "Zhiling Wang"
                    },
                    {
                        "name": "Yuanchao Du"
                    },
                    {
                        "name": "Junfeng Luo"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Zhineng chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhineng chen"
                },
                "author": "Zhineng chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18338v1",
                "updated": "2025-07-24T12:10:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    10,
                    21,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:10:21Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    10,
                    21,
                    3,
                    205,
                    0
                ],
                "title": "Uncertainty Quantification for Evaluating Machine Translation Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification for Evaluating Machine Translation Bias"
                },
                "summary": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances."
                },
                "authors": [
                    {
                        "name": "Ieva Raminta Staliūnaitė"
                    },
                    {
                        "name": "Julius Cheng"
                    },
                    {
                        "name": "Andreas Vlachos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vlachos"
                },
                "author": "Andreas Vlachos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10062v2",
                "updated": "2025-07-24T11:58:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    58,
                    1,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-14T08:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "LLMShot: Reducing snapshot testing maintenance via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMShot: Reducing snapshot testing maintenance via LLMs"
                },
                "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages Vision-Language\nModels (VLMs) to automatically analyze snapshot test failures through semantic\nclassification of UI changes. To evaluate LLMShot's effectiveness, we developed\na comprehensive dataset using a feature-rich iOS application with configurable\nfeature flags, creating realistic scenarios that produce authentic snapshot\ndifferences representative of real development workflows. Our evaluation using\nGemma3 models demonstrates strong classification performance, with the 12B\nvariant achieving over 84% recall in identifying failure root causes while the\n4B model offers practical deployment advantages with acceptable performance for\ncontinuous integration environments. However, our exploration of selective\nignore mechanisms revealed significant limitations in current prompting-based\napproaches for controllable visual reasoning. LLMShot represents the first\nautomated approach to semantic snapshot test analysis, offering developers\nstructured insights that can substantially reduce manual triage effort and\nadvance toward more intelligent UI testing paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages Vision-Language\nModels (VLMs) to automatically analyze snapshot test failures through semantic\nclassification of UI changes. To evaluate LLMShot's effectiveness, we developed\na comprehensive dataset using a feature-rich iOS application with configurable\nfeature flags, creating realistic scenarios that produce authentic snapshot\ndifferences representative of real development workflows. Our evaluation using\nGemma3 models demonstrates strong classification performance, with the 12B\nvariant achieving over 84% recall in identifying failure root causes while the\n4B model offers practical deployment advantages with acceptable performance for\ncontinuous integration environments. However, our exploration of selective\nignore mechanisms revealed significant limitations in current prompting-based\napproaches for controllable visual reasoning. LLMShot represents the first\nautomated approach to semantic snapshot test analysis, offering developers\nstructured insights that can substantially reduce manual triage effort and\nadvance toward more intelligent UI testing paradigms."
                },
                "authors": [
                    {
                        "name": "Ergün Batuhan Kaynak"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18328v1",
                "updated": "2025-07-24T11:54:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    54,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:54:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    54,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of\n  Information Optimization in Vehicular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of\n  Information Optimization in Vehicular Networks"
                },
                "summary": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI."
                },
                "authors": [
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE TMC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08621v2",
                "updated": "2025-07-24T11:49:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-11T14:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1"
                },
                "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings."
                },
                "authors": [
                    {
                        "name": "Marcin Pietroń"
                    },
                    {
                        "name": "Rafał Olszowski"
                    },
                    {
                        "name": "Jakub Gomułka"
                    },
                    {
                        "name": "Filip Gampel"
                    },
                    {
                        "name": "Andrzej Tomski"
                    }
                ],
                "author_detail": {
                    "name": "Andrzej Tomski"
                },
                "author": "Andrzej Tomski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14019v3",
                "updated": "2025-07-24T11:38:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    38,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2024-12-18T16:37:51Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "title": "Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases"
                },
                "summary": "Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order."
                },
                "authors": [
                    {
                        "name": "Federico Baldo"
                    },
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18316v1",
                "updated": "2025-07-24T11:32:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    32,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:32:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    32,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation"
                },
                "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs)."
                },
                "authors": [
                    {
                        "name": "Michael Konstantinou"
                    },
                    {
                        "name": "Renzo Degiovanni"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Mike Papadakis"
                    }
                ],
                "author_detail": {
                    "name": "Mike Papadakis"
                },
                "author": "Mike Papadakis",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v4",
                "updated": "2025-07-24T11:31:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    31,
                    3,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Yugui Lin"
                    },
                    {
                        "name": "Pietro Lio"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Code and data will be made available on request to the corresponding\n  author. V4 Update: Add Future Work; Improve Typesetting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18305v1",
                "updated": "2025-07-24T11:24:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    24,
                    35,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:24:35Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    24,
                    35,
                    3,
                    205,
                    0
                ],
                "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large\n  Reasoning Models for Fun or Profit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadReasoner: Planting Tunable Overthinking Backdoors into Large\n  Reasoning Models for Fun or Profit"
                },
                "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zekun Fei"
                    },
                    {
                        "name": "Jianing Geng"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Lihai Nie"
                    },
                    {
                        "name": "Zheli Liu"
                    },
                    {
                        "name": "Yiming Li"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Li"
                },
                "author": "Yiming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22074v2",
                "updated": "2025-07-24T11:24:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    24,
                    17,
                    3,
                    205,
                    0
                ],
                "published": "2024-10-29T14:33:52Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    33,
                    52,
                    1,
                    303,
                    0
                ],
                "title": "Variational inference for pile-up removal at hadron colliders with\n  diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference for pile-up removal at hadron colliders with\n  diffusion models"
                },
                "summary": "In this paper, we present a novel method for pile-up removal of $pp$\ninteractions using variational inference with diffusion models, called vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval, yielding a clear advantage over existing methods especially in the\npresence of imperfect detector efficiency. We evaluate the performance of vipr\nin a sample of jets from simulated $t\\bar{t}$ events overlain with pile-up\ncontamination. vipr outperforms softdrop and has comparable performance to\npuppiml in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel method for pile-up removal of $pp$\ninteractions using variational inference with diffusion models, called vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval, yielding a clear advantage over existing methods especially in the\npresence of imperfect detector efficiency. We evaluate the performance of vipr\nin a sample of jets from simulated $t\\bar{t}$ events overlain with pile-up\ncontamination. vipr outperforms softdrop and has comparable performance to\npuppiml in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios."
                },
                "authors": [
                    {
                        "name": "Malte Algren"
                    },
                    {
                        "name": "Tobias Golling"
                    },
                    {
                        "name": "Christopher Pollard"
                    },
                    {
                        "name": "John Andrew Raine"
                    }
                ],
                "author_detail": {
                    "name": "John Andrew Raine"
                },
                "author": "John Andrew Raine",
                "arxiv_doi": "10.1103/PhysRevD.111.116010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.116010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.22074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 13 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 116010 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19969v3",
                "updated": "2025-07-24T11:23:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    23,
                    43,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-25T18:00:03Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    0,
                    3,
                    1,
                    84,
                    0
                ],
                "title": "Inferring CSM Properties of Type II SNe Using a Magnitude-Limited ZTF\n  Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring CSM Properties of Type II SNe Using a Magnitude-Limited ZTF\n  Sample"
                },
                "summary": "Although all Type II supernovae (SNe) originate from massive stars possessing\na hydrogen-rich envelope, their light curve morphology is diverse, reflecting\npoorly characterised heterogeneity in the physical properties of their\nprogenitor systems. Here, we present a detailed light curve analysis of a\nmagnitude-limited sample of 639 Type II SNe from the Zwicky Transient Facility\nBright Transient Survey. Using Gaussian processes, we systematically measure\nempirical light curve features (e.g. rise times, peak colours and luminosities)\nin a robust sampling-independent manner. We focus on rise times as they are\nhighly sensitive to pre-explosion progenitor properties, especially the\npresence of a dense circumstellar medium (CSM) shed by the progenitor in the\nyears immediately pre-explosion. By correlating our feature measurements with\nphysical parameters from an extensive grid of STELLA hydrodynamical models with\nvarying progenitor properties (CSM structure, $\\dot M$, $R_{CSM}$ and\n$M_{ZAMS}$), we quantify the proportion of events with sufficient pre-explosion\nmass-loss to significantly alter the initial light curve (roughly $M_{CSM} \\geq\n10^{-2.5} M_{\\odot}$) in a highly complete sample of 377 spectroscopically\nclassified Type II SNe. We find that 67 $\\pm$ 6\\% of observed SNe in our\nmagnitude-limited sample show evidence for substantial CSM ($M_{CSM} \\geq\n10^{-2.5} M_{\\odot}$) close to the progenitor ($R_{CSM} <10^{15}$ cm) at the\ntime of explosion. After applying a volumetric-correction, we find\n36$^{+5}_{-7}$\\% of all Type II SN progenitors possess substantial CSM within\n$10^{15}$ cm at the time of explosion. This high fraction of progenitors with\ndense CSM, supported by photometric and spectroscopic evidence of previous SNe,\nreveals mass-loss rates significantly exceeding those measured in local group\nred supergiants or predicted by current theoretical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although all Type II supernovae (SNe) originate from massive stars possessing\na hydrogen-rich envelope, their light curve morphology is diverse, reflecting\npoorly characterised heterogeneity in the physical properties of their\nprogenitor systems. Here, we present a detailed light curve analysis of a\nmagnitude-limited sample of 639 Type II SNe from the Zwicky Transient Facility\nBright Transient Survey. Using Gaussian processes, we systematically measure\nempirical light curve features (e.g. rise times, peak colours and luminosities)\nin a robust sampling-independent manner. We focus on rise times as they are\nhighly sensitive to pre-explosion progenitor properties, especially the\npresence of a dense circumstellar medium (CSM) shed by the progenitor in the\nyears immediately pre-explosion. By correlating our feature measurements with\nphysical parameters from an extensive grid of STELLA hydrodynamical models with\nvarying progenitor properties (CSM structure, $\\dot M$, $R_{CSM}$ and\n$M_{ZAMS}$), we quantify the proportion of events with sufficient pre-explosion\nmass-loss to significantly alter the initial light curve (roughly $M_{CSM} \\geq\n10^{-2.5} M_{\\odot}$) in a highly complete sample of 377 spectroscopically\nclassified Type II SNe. We find that 67 $\\pm$ 6\\% of observed SNe in our\nmagnitude-limited sample show evidence for substantial CSM ($M_{CSM} \\geq\n10^{-2.5} M_{\\odot}$) close to the progenitor ($R_{CSM} <10^{15}$ cm) at the\ntime of explosion. After applying a volumetric-correction, we find\n36$^{+5}_{-7}$\\% of all Type II SN progenitors possess substantial CSM within\n$10^{15}$ cm at the time of explosion. This high fraction of progenitors with\ndense CSM, supported by photometric and spectroscopic evidence of previous SNe,\nreveals mass-loss rates significantly exceeding those measured in local group\nred supergiants or predicted by current theoretical models."
                },
                "authors": [
                    {
                        "name": "K-Ryan Hinds"
                    },
                    {
                        "name": "Daniel Perley"
                    },
                    {
                        "name": "Jesper Sollerman"
                    },
                    {
                        "name": "Adam Miller"
                    },
                    {
                        "name": "Christoffer Fremling"
                    },
                    {
                        "name": "Takashi Moriya"
                    },
                    {
                        "name": "Kaustav Das"
                    },
                    {
                        "name": "Yu-Jing Qin"
                    },
                    {
                        "name": "Eric Bellm"
                    },
                    {
                        "name": "Xi Tracy Chen"
                    },
                    {
                        "name": "Michael Coughlin"
                    },
                    {
                        "name": "Wynn Jacobson-Galán"
                    },
                    {
                        "name": "Mansi Kasliwal"
                    },
                    {
                        "name": "Shrinivas Kulkarni"
                    },
                    {
                        "name": "Ashish Mahabal"
                    },
                    {
                        "name": "F. Masci"
                    },
                    {
                        "name": "Priscila"
                    },
                    {
                        "name": "J. Pessi"
                    },
                    {
                        "name": "J. N. Purdum"
                    },
                    {
                        "name": "Reed Riddle"
                    },
                    {
                        "name": "Avinash Singh"
                    },
                    {
                        "name": "Roger Smith"
                    },
                    {
                        "name": "Niharika Sravan"
                    }
                ],
                "author_detail": {
                    "name": "Niharika Sravan"
                },
                "author": "Niharika Sravan",
                "arxiv_doi": "10.1093/mnras/staf888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.19969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 541,\n  Issue 1, July 2025, Pages 135 - 165",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18302v1",
                "updated": "2025-07-24T11:18:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    18,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:18:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    18,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language\n  Models"
                },
                "summary": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers."
                },
                "authors": [
                    {
                        "name": "Delong Ran"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Anyu Wang"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xiaoyun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyun Wang"
                },
                "author": "Xiaoyun Wang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12972v2",
                "updated": "2025-07-24T11:14:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    14,
                    43,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-17T09:31:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    31,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning"
                },
                "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."
                },
                "authors": [
                    {
                        "name": "Junming Liu"
                    },
                    {
                        "name": "Siyuan Meng"
                    },
                    {
                        "name": "Yanting Gao"
                    },
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Yirong Chen"
                    },
                    {
                        "name": "Zilin Bian"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "14 pages, 7 figures, 6 tables; Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10009v2",
                "updated": "2025-07-24T11:09:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    9,
                    58,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-13T03:40:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    40,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM"
                },
                "summary": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Pengcheng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Luo"
                },
                "author": "Pengcheng Luo",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19795v2",
                "updated": "2025-07-24T11:08:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    8,
                    59,
                    3,
                    205,
                    0
                ],
                "published": "2024-07-29T08:38:46Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    8,
                    38,
                    46,
                    0,
                    211,
                    0
                ],
                "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks"
                },
                "summary": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "Junehyoung Kwon"
                    },
                    {
                        "name": "JungMin Yun"
                    },
                    {
                        "name": "Seunguk Yu"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "arxiv_comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18300v1",
                "updated": "2025-07-24T11:05:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    5,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:05:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    5,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMM-Det: Make Large Multimodal Models Excel in Object Detection"
                },
                "summary": "Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det."
                },
                "authors": [
                    {
                        "name": "Jincheng Li"
                    },
                    {
                        "name": "Chunyu Xie"
                    },
                    {
                        "name": "Ji Ao"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17596v2",
                "updated": "2025-07-24T11:04:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    4,
                    42,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-23T15:28:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    28,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
                },
                "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix."
                },
                "authors": [
                    {
                        "name": "Maciej K. Wozniak"
                    },
                    {
                        "name": "Lianhang Liu"
                    },
                    {
                        "name": "Yixi Cai"
                    },
                    {
                        "name": "Patric Jensfelt"
                    }
                ],
                "author_detail": {
                    "name": "Patric Jensfelt"
                },
                "author": "Patric Jensfelt",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18294v1",
                "updated": "2025-07-24T10:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    57,
                    32,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:57:32Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    57,
                    32,
                    3,
                    205,
                    0
                ],
                "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient\n  Stylistic Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient\n  Stylistic Transfer"
                },
                "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs."
                },
                "authors": [
                    {
                        "name": "Pritika Ramu"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Meghanath M Y"
                    },
                    {
                        "name": "Varsha Sankar"
                    },
                    {
                        "name": "Debraj Basu"
                    }
                ],
                "author_detail": {
                    "name": "Debraj Basu"
                },
                "author": "Debraj Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18289v1",
                "updated": "2025-07-24T10:51:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    51,
                    11,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:51:11Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    51,
                    11,
                    3,
                    205,
                    0
                ],
                "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling"
                },
                "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Wenzhang Yang"
                    },
                    {
                        "name": "Yuekun Wang"
                    },
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Shaohua Wang"
                    },
                    {
                        "name": "Yinxing Xue"
                    },
                    {
                        "name": "Lijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Zhang"
                },
                "author": "Lijun Zhang",
                "arxiv_comment": "15 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18276v1",
                "updated": "2025-07-24T10:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    25,
                    58,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:25:58Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    25,
                    58,
                    3,
                    205,
                    0
                ],
                "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding"
                },
                "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories."
                },
                "authors": [
                    {
                        "name": "Xiaojie Zhang"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Ruihai Wu"
                    },
                    {
                        "name": "Kunqi Xu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Liuyu Xiang"
                    },
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18274v1",
                "updated": "2025-07-24T10:19:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    19,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:19:06Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    19,
                    6,
                    3,
                    205,
                    0
                ],
                "title": "Phantom crossing or dark interaction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantom crossing or dark interaction?"
                },
                "summary": "Recent results from DESI BAO measurements, together with Planck CMB and\nPantheon+ data, suggest that there may be a `phantom' phase ($w_{\\rm de}<-1$)\nin the expansion of the Universe. This inference follows when the $w_0, w_a$\nparametrization for the dark energy equation of state $w_{\\rm de}$ is used to\nfit the data. Since phantom dark energy in general relativity is unphysical, we\ninvestigate the possibility that the phantom behaviour is not intrinsic, but\neffective -- due to a non-gravitational interaction between dark matter and\nnon-phantom dark energy. To this end, we assume a physically motivated thawing\nquintessence-like form of the intrinsic dark energy equation of state $w_{\\rm\nde}$. Then we use a $w_0, w_a$ model for the \\emph{effective} equation of state\nof dark energy. We find that the data favours a phantom crossing for the\neffective dark energy, but only at low significance. The intrinsic equation of\nstate of dark energy is non-phantom, without imposing any non-phantom priors. A\nnonzero interaction is favoured at more than $3\\sigma$ at $z\\sim0.3$. The\nenergy flows from dark matter to dark energy at early times and reverses at\nlater times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results from DESI BAO measurements, together with Planck CMB and\nPantheon+ data, suggest that there may be a `phantom' phase ($w_{\\rm de}<-1$)\nin the expansion of the Universe. This inference follows when the $w_0, w_a$\nparametrization for the dark energy equation of state $w_{\\rm de}$ is used to\nfit the data. Since phantom dark energy in general relativity is unphysical, we\ninvestigate the possibility that the phantom behaviour is not intrinsic, but\neffective -- due to a non-gravitational interaction between dark matter and\nnon-phantom dark energy. To this end, we assume a physically motivated thawing\nquintessence-like form of the intrinsic dark energy equation of state $w_{\\rm\nde}$. Then we use a $w_0, w_a$ model for the \\emph{effective} equation of state\nof dark energy. We find that the data favours a phantom crossing for the\neffective dark energy, but only at low significance. The intrinsic equation of\nstate of dark energy is non-phantom, without imposing any non-phantom priors. A\nnonzero interaction is favoured at more than $3\\sigma$ at $z\\sim0.3$. The\nenergy flows from dark matter to dark energy at early times and reverses at\nlater times."
                },
                "authors": [
                    {
                        "name": "Sêcloka L. Guedezounme"
                    },
                    {
                        "name": "Bikash R. Dinda"
                    },
                    {
                        "name": "Roy Maartens"
                    }
                ],
                "author_detail": {
                    "name": "Roy Maartens"
                },
                "author": "Roy Maartens",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07685v2",
                "updated": "2025-07-24T10:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    9,
                    5,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-09T12:14:44Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    14,
                    44,
                    0,
                    160,
                    0
                ],
                "title": "Theoretical Analysis for the CommSense Measurement System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Analysis for the CommSense Measurement System"
                },
                "summary": "Future 6G networks envisions to blur the line between communication and\nsensing, leveraging ubiquitous OFDM waveforms for both high throughput data and\nenvironmental awareness. In this work, we do a thorough analysis of\nCommunication based Sensing (CommSense) framework that embeds lightweight, PCA\nbased detectors into standard OFDM receivers; enabling real-time, device free\ndetection of passive scatterers (e.g. drones, vehicles etc.) without any extra\ntransmitters. Starting from a realistic three link Rician channel model (direct\nTx to Rx, cascaded Tx to Scatterer and Scatterer to Rx), we compare four\ndetectors: the full dimensional Likelihood Ratio Test (Full LRT), PCA based\nLRT, PCA-SVM with linear and RBF kernels. By projecting N-dimensional CSI onto\na P (very less than N) principal component subspace, inference time gets\nreduced by an order of magnitude compared to the full LRT, while achieving\noptimal error rates i.e. empirical errors align tightly with the Bhattacharyya\nerror bound and Area Under ROC Curve (AUC) approx. equal to 1 for P approx.\nequal to 10. From the simulated result we have shown LRT based techniques are\nsusceptible to the parameter estimation error, where as SVM is resilient to\nthat. Our results demonstrate that PCA driven detection when paired with\nlightweight SVMs can deliver fast, accurate, and robust scatterer sensing,\npaving the way for integrated sensing and communication (ISAC) in 6G and\nbeyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future 6G networks envisions to blur the line between communication and\nsensing, leveraging ubiquitous OFDM waveforms for both high throughput data and\nenvironmental awareness. In this work, we do a thorough analysis of\nCommunication based Sensing (CommSense) framework that embeds lightweight, PCA\nbased detectors into standard OFDM receivers; enabling real-time, device free\ndetection of passive scatterers (e.g. drones, vehicles etc.) without any extra\ntransmitters. Starting from a realistic three link Rician channel model (direct\nTx to Rx, cascaded Tx to Scatterer and Scatterer to Rx), we compare four\ndetectors: the full dimensional Likelihood Ratio Test (Full LRT), PCA based\nLRT, PCA-SVM with linear and RBF kernels. By projecting N-dimensional CSI onto\na P (very less than N) principal component subspace, inference time gets\nreduced by an order of magnitude compared to the full LRT, while achieving\noptimal error rates i.e. empirical errors align tightly with the Bhattacharyya\nerror bound and Area Under ROC Curve (AUC) approx. equal to 1 for P approx.\nequal to 10. From the simulated result we have shown LRT based techniques are\nsusceptible to the parameter estimation error, where as SVM is resilient to\nthat. Our results demonstrate that PCA driven detection when paired with\nlightweight SVMs can deliver fast, accurate, and robust scatterer sensing,\npaving the way for integrated sensing and communication (ISAC) in 6G and\nbeyond."
                },
                "authors": [
                    {
                        "name": "Sandip Jana"
                    },
                    {
                        "name": "Amit Kumar Mishra"
                    },
                    {
                        "name": "Mohammed Zafar Ali Khan"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Zafar Ali Khan"
                },
                "author": "Mohammed Zafar Ali Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14400v4",
                "updated": "2025-07-24T10:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-20T09:37:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    37,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPS: Hard Preference Sampling for Human Preference Alignment"
                },
                "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Wanyu Lin"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Pan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pan Zhou"
                },
                "author": "Pan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18255v1",
                "updated": "2025-07-24T09:55:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    55,
                    20,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:55:20Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    55,
                    20,
                    3,
                    205,
                    0
                ],
                "title": "LONG3R: Long Sequence Streaming 3D Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONG3R: Long Sequence Streaming 3D Reconstruction"
                },
                "summary": "Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/."
                },
                "authors": [
                    {
                        "name": "Zhuoguang Chen"
                    },
                    {
                        "name": "Minghui Qin"
                    },
                    {
                        "name": "Tianyuan Yuan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "arxiv_comment": "Accepted by ICCV 2025. Project page:\n  https://zgchen33.github.io/LONG3R/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18253v1",
                "updated": "2025-07-24T09:52:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    52,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:52:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    52,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "Countering Privacy Nihilism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countering Privacy Nihilism"
                },
                "summary": "Of growing concern in privacy scholarship is artificial intelligence (AI), as\na powerful producer of inferences. Taken to its limits, AI may be presumed\ncapable of inferring \"everything from everything,\" thereby making untenable any\nnormative scheme, including privacy theory and privacy regulation, which rests\non protecting privacy based on categories of data - sensitive versus\nnon-sensitive, private versus public. Discarding data categories as a normative\nanchoring in privacy and data protection as a result of an unconditional\nacceptance of AI's inferential capacities is what we call privacy nihilism. An\nethically reasoned response to AI inferences requires a sober consideration of\nAI capabilities rather than issuing an epistemic carte blanche. We introduce\nthe notion of conceptual overfitting to expose how privacy nihilism turns a\nblind eye toward flawed epistemic practices in AI development. Conceptual\noverfitting refers to the adoption of norms of convenience that simplify the\ndevelopment of AI models by forcing complex constructs to fit data that are\nconceptually under-representative or even irrelevant. While conceptual\noverfitting serves as a helpful device to counter normative suggestions\ngrounded in hyperbolic AI capability claims, AI inferences shake any privacy\nregulation that hinges protections based on restrictions around data\ncategories. We propose moving away from privacy frameworks that focus solely on\ndata type, neglecting all other factors. Theories like contextual integrity\nevaluate the normative value of privacy across several parameters, including\nthe type of data, the actors involved in sharing it, and the purposes for which\nthe information is used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Of growing concern in privacy scholarship is artificial intelligence (AI), as\na powerful producer of inferences. Taken to its limits, AI may be presumed\ncapable of inferring \"everything from everything,\" thereby making untenable any\nnormative scheme, including privacy theory and privacy regulation, which rests\non protecting privacy based on categories of data - sensitive versus\nnon-sensitive, private versus public. Discarding data categories as a normative\nanchoring in privacy and data protection as a result of an unconditional\nacceptance of AI's inferential capacities is what we call privacy nihilism. An\nethically reasoned response to AI inferences requires a sober consideration of\nAI capabilities rather than issuing an epistemic carte blanche. We introduce\nthe notion of conceptual overfitting to expose how privacy nihilism turns a\nblind eye toward flawed epistemic practices in AI development. Conceptual\noverfitting refers to the adoption of norms of convenience that simplify the\ndevelopment of AI models by forcing complex constructs to fit data that are\nconceptually under-representative or even irrelevant. While conceptual\noverfitting serves as a helpful device to counter normative suggestions\ngrounded in hyperbolic AI capability claims, AI inferences shake any privacy\nregulation that hinges protections based on restrictions around data\ncategories. We propose moving away from privacy frameworks that focus solely on\ndata type, neglecting all other factors. Theories like contextual integrity\nevaluate the normative value of privacy across several parameters, including\nthe type of data, the actors involved in sharing it, and the purposes for which\nthe information is used."
                },
                "authors": [
                    {
                        "name": "Severin Engelmann"
                    },
                    {
                        "name": "Helen Nissenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Helen Nissenbaum"
                },
                "author": "Helen Nissenbaum",
                "arxiv_journal_ref": "Conceptions of Data Protection and Privacy. Legal and\n  Philosophical Perspective Hart Publishing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18252v1",
                "updated": "2025-07-24T09:49:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    49,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:49:53Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    49,
                    53,
                    3,
                    205,
                    0
                ],
                "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning"
                },
                "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics."
                },
                "authors": [
                    {
                        "name": "Dongyang Guo"
                    },
                    {
                        "name": "Yasmeen Abdrabou"
                    },
                    {
                        "name": "Enkeleda Thaqi"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16068v2",
                "updated": "2025-07-24T09:25:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    25,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-21T21:09:15Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    21,
                    9,
                    15,
                    0,
                    202,
                    0
                ],
                "title": "Compositional Coordination for Multi-Robot Teams with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Coordination for Multi-Robot Teams with Large Language\n  Models"
                },
                "summary": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb"
                },
                "authors": [
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Guangyao Shi"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "Gaurav S. Sukhatme"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav S. Sukhatme"
                },
                "author": "Gaurav S. Sukhatme",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11482v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11482v8",
                "updated": "2025-07-24T09:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    19,
                    38,
                    3,
                    205,
                    0
                ],
                "published": "2023-11-20T01:51:13Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    1,
                    51,
                    13,
                    0,
                    324,
                    0
                ],
                "title": "Meta Prompting for AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta Prompting for AI Systems"
                },
                "summary": "We introduce Meta Prompting (MP), a framework that elevates the reasoning\ncapabilities of large language models (LLMs) by focusing on the formal\nstructure of a task rather than content-specific examples. We establish a\ntheoretical foundation for this paradigm, formalizing MP as a functor that maps\na category of tasks to a category of structured prompts, thereby guaranteeing\nthat compositional problem-solving strategies can be systematically decomposed\ninto modular prompt structures. We extend this concept to Recursive Meta\nPrompting (RMP), an automated process where an LLM can generate and refine its\nown prompts. We model this self-improvement loop formally as a monad, providing\na principled framework for automated prompt engineering. Our claims are\nvalidated through extensive experiments demonstrating that a Qwen-72B base\nmodel, guided by a single, example-agnostic meta-prompt, achieves\nstate-of-the-art results on MATH, GSM8K, and Game of 24. These results are\nachieved with substantial token efficiency gains over traditional few-shot\nmethods. Project Page: https://github.com/meta-prompting/meta-prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Meta Prompting (MP), a framework that elevates the reasoning\ncapabilities of large language models (LLMs) by focusing on the formal\nstructure of a task rather than content-specific examples. We establish a\ntheoretical foundation for this paradigm, formalizing MP as a functor that maps\na category of tasks to a category of structured prompts, thereby guaranteeing\nthat compositional problem-solving strategies can be systematically decomposed\ninto modular prompt structures. We extend this concept to Recursive Meta\nPrompting (RMP), an automated process where an LLM can generate and refine its\nown prompts. We model this self-improvement loop formally as a monad, providing\na principled framework for automated prompt engineering. Our claims are\nvalidated through extensive experiments demonstrating that a Qwen-72B base\nmodel, guided by a single, example-agnostic meta-prompt, achieves\nstate-of-the-art results on MATH, GSM8K, and Game of 24. These results are\nachieved with substantial token efficiency gains over traditional few-shot\nmethods. Project Page: https://github.com/meta-prompting/meta-prompting."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Project Page: https://github.com/meta-prompting/meta-prompting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11482v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11482v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18225v1",
                "updated": "2025-07-24T09:18:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    18,
                    39,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:18:39Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    18,
                    39,
                    3,
                    205,
                    0
                ],
                "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Test-time Adaptation via Graph Spectral Driven Point Shift"
                },
                "summary": "While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification."
                },
                "authors": [
                    {
                        "name": "Xin Wei"
                    },
                    {
                        "name": "Qin Yang"
                    },
                    {
                        "name": "Yijie Fang"
                    },
                    {
                        "name": "Mingrui Zhu"
                    },
                    {
                        "name": "Nannan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nannan Wang"
                },
                "author": "Nannan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18631v2",
                "updated": "2025-07-25T07:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    20,
                    24,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T17:59:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    59,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment"
                },
                "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Zhenghao Lu"
                    },
                    {
                        "name": "Xianyi Wei"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18618v1",
                "updated": "2025-07-24T17:54:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    54,
                    44,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:54:44Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    54,
                    44,
                    3,
                    205,
                    0
                ],
                "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards"
                },
                "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH."
                },
                "authors": [
                    {
                        "name": "Andreea Nica"
                    },
                    {
                        "name": "Ivan Zakazov"
                    },
                    {
                        "name": "Nicolas Mario Baldwin"
                    },
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18607v1",
                "updated": "2025-07-24T17:43:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    43,
                    40,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:43:40Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    43,
                    40,
                    3,
                    205,
                    0
                ],
                "title": "Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents"
                },
                "summary": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods."
                },
                "authors": [
                    {
                        "name": "Xinyuan Yan"
                    },
                    {
                        "name": "Rita Sevastjanova"
                    },
                    {
                        "name": "Sinie van der Ben"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Bei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bei Wang"
                },
                "author": "Bei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18606v1",
                "updated": "2025-07-24T17:42:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    42,
                    30,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:42:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    42,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs"
                },
                "summary": "Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings."
                },
                "authors": [
                    {
                        "name": "Gilberto Cunha"
                    },
                    {
                        "name": "Alexandra Ramôa"
                    },
                    {
                        "name": "André Sequeira"
                    },
                    {
                        "name": "Michael de Oliveira"
                    },
                    {
                        "name": "Luís Barbosa"
                    }
                ],
                "author_detail": {
                    "name": "Luís Barbosa"
                },
                "author": "Luís Barbosa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03654v3",
                "updated": "2025-07-24T17:28:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    28,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-04T07:46:24Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    46,
                    24,
                    2,
                    155,
                    0
                ],
                "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object\n  Detection"
                },
                "summary": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX."
                },
                "authors": [
                    {
                        "name": "Xiaochun Lei"
                    },
                    {
                        "name": "Siqi Wu"
                    },
                    {
                        "name": "Weilin Wu"
                    },
                    {
                        "name": "Zetao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zetao Jiang"
                },
                "author": "Zetao Jiang",
                "arxiv_comment": "This paper is under consideration at Image and Vision Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14314v2",
                "updated": "2025-07-24T17:10:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    10,
                    17,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-18T18:39:07Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    39,
                    7,
                    4,
                    199,
                    0
                ],
                "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes You CLIC: Detection of Croatian Clickbait Headlines"
                },
                "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs."
                },
                "authors": [
                    {
                        "name": "Marija Anđelić"
                    },
                    {
                        "name": "Dominik Šipek"
                    },
                    {
                        "name": "Laura Majer"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "arxiv_comment": "Accepted at Slavic NLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18587v1",
                "updated": "2025-07-24T17:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    10,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:10:06Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    10,
                    6,
                    3,
                    205,
                    0
                ],
                "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User\n  Rate-Power Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User\n  Rate-Power Tradeoff"
                },
                "summary": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness."
                },
                "authors": [
                    {
                        "name": "Jérôme Emery"
                    },
                    {
                        "name": "Ali Hasanzadeh Karkan"
                    },
                    {
                        "name": "Jean-François Frigon"
                    },
                    {
                        "name": "François Leduc-Primeau"
                    }
                ],
                "author_detail": {
                    "name": "François Leduc-Primeau"
                },
                "author": "François Leduc-Primeau",
                "arxiv_comment": "6 pages, 3 figures. Accepted to the IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18584v1",
                "updated": "2025-07-24T17:03:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    3,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T17:03:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    3,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs"
                },
                "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Ke"
                    },
                    {
                        "name": "Hexuan Deng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Jun Rao"
                    },
                    {
                        "name": "Zhenxi Song"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "32 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12548v3",
                "updated": "2025-07-24T16:54:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    54,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2024-06-18T12:25:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    25,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via\n  Mixture of Specialized LoRA Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via\n  Mixture of Specialized LoRA Experts"
                },
                "summary": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality."
                },
                "authors": [
                    {
                        "name": "Yuhao Dan"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Junfeng Tian"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16809v2",
                "updated": "2025-07-24T16:51:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    51,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T17:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    57,
                    44,
                    1,
                    203,
                    0
                ],
                "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework\n  for Multi-Step and Cross-Cultural Inference with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework\n  for Multi-Step and Cross-Cultural Inference with LLMs"
                },
                "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Da-Chen Lian"
                    },
                    {
                        "name": "Ri-Sheng Huang"
                    },
                    {
                        "name": "Pin-Er Chen"
                    },
                    {
                        "name": "Chunki Lim"
                    },
                    {
                        "name": "You-Kuan Lin"
                    },
                    {
                        "name": "Guan-Yu Tseng"
                    },
                    {
                        "name": "Zi-Cheng Yang"
                    },
                    {
                        "name": "Zhen-Yu Lin"
                    },
                    {
                        "name": "Pin-Cheng Chen"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Kai Hsieh"
                },
                "author": "Shu-Kai Hsieh",
                "arxiv_comment": "42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize\n  contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent,\n  OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each\n  experiment once; statistical tests are needed for strong claims. Revised Sec\n  6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17289v2",
                "updated": "2025-07-24T16:50:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    50,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-23T07:51:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    7,
                    51,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting\n  Compliance Tasks in Enterprise Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compliance Brain Assistant: Conversational Agentic AI for Assisting\n  Compliance Tasks in Enterprise Environments"
                },
                "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds."
                },
                "authors": [
                    {
                        "name": "Shitong Zhu"
                    },
                    {
                        "name": "Chenhao Fang"
                    },
                    {
                        "name": "Derek Larson"
                    },
                    {
                        "name": "Neel Reddy Pochareddy"
                    },
                    {
                        "name": "Rajeev Rao"
                    },
                    {
                        "name": "Sophie Zeng"
                    },
                    {
                        "name": "Yanqing Peng"
                    },
                    {
                        "name": "Wendy Summer"
                    },
                    {
                        "name": "Alex Goncalves"
                    },
                    {
                        "name": "Arya Pudota"
                    },
                    {
                        "name": "Hervé Robert"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Robert"
                },
                "author": "Hervé Robert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16802v3",
                "updated": "2025-07-24T16:46:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    46,
                    58,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T17:52:16Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    52,
                    16,
                    1,
                    203,
                    0
                ],
                "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova."
                },
                "authors": [
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Jingze Song"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18560v1",
                "updated": "2025-07-24T16:35:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    35,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:35:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    35,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization"
                },
                "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility."
                },
                "authors": [
                    {
                        "name": "Benjamin Coriat"
                    },
                    {
                        "name": "Eric Benhamou"
                    }
                ],
                "author_detail": {
                    "name": "Eric Benhamou"
                },
                "author": "Eric Benhamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14783v2",
                "updated": "2025-07-24T16:25:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    54,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-20T01:50:16Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    1,
                    50,
                    16,
                    6,
                    201,
                    0
                ],
                "title": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards"
                },
                "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs."
                },
                "authors": [
                    {
                        "name": "Derek Li"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Amirreza Kazemi"
                    },
                    {
                        "name": "Qianyi Sun"
                    },
                    {
                        "name": "Abbas Ghaddar"
                    },
                    {
                        "name": "Mohammad Ali Alomrani"
                    },
                    {
                        "name": "Liheng Ma"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Feng Wen"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mark Coates"
                    },
                    {
                        "name": "Yingxue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yingxue Zhang"
                },
                "author": "Yingxue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18553v1",
                "updated": "2025-07-24T16:22:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm"
                },
                "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models."
                },
                "authors": [
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08989v3",
                "updated": "2025-07-24T16:21:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    21,
                    10,
                    3,
                    205,
                    0
                ],
                "published": "2024-10-11T17:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    1,
                    43,
                    4,
                    285,
                    0
                ],
                "title": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero."
                },
                "authors": [
                    {
                        "name": "Ziming Yu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Sike Wang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "arxiv_comment": "ICCV 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15551v2",
                "updated": "2025-07-24T16:19:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    19,
                    32,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-21T12:28:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders"
                },
                "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Zhifang Fan"
                    },
                    {
                        "name": "Xiaoxie Zhu"
                    },
                    {
                        "name": "Yuchen Jiang"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Haoran Ding"
                    },
                    {
                        "name": "Xinmin Wang"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Zhen Gong"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Qiwei Chen"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Zuotao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuotao Liu"
                },
                "author": "Zuotao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18546v1",
                "updated": "2025-07-24T16:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    11,
                    14,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T16:11:14Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    11,
                    14,
                    3,
                    205,
                    0
                ],
                "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface"
                },
                "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2."
                },
                "authors": [
                    {
                        "name": "Urchade Zaratiana"
                    },
                    {
                        "name": "Gil Pasternak"
                    },
                    {
                        "name": "Oliver Boyd"
                    },
                    {
                        "name": "George Hurn-Maloney"
                    },
                    {
                        "name": "Ash Lewis"
                    }
                ],
                "author_detail": {
                    "name": "Ash Lewis"
                },
                "author": "Ash Lewis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18531v1",
                "updated": "2025-07-24T15:58:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    58,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:58:36Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    58,
                    36,
                    3,
                    205,
                    0
                ],
                "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning"
                },
                "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet."
                },
                "authors": [
                    {
                        "name": "Tianheng Qiu"
                    },
                    {
                        "name": "Jingchun Gao"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Huiyi Leong"
                    },
                    {
                        "name": "Xuan Huang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Xiaocheng Zhang"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Lan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Zhang"
                },
                "author": "Lan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12901v2",
                "updated": "2025-07-24T15:50:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    50,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-17T08:40:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a\nlarge-scale financial reasoning dataset characterized by its systematic CoT\nsynthesis optimization. We first introduce a comprehensive CoT synthesis\npipeline featuring Multi-perspective Knowledge Extraction (MKE) and\nSelf-Corrective Rewriting (SCR) to generate exhaustive and deep financial\nreasoning trajectories. Furthermore, a systematic investigation, termed CoT\nCube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-100K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-100K , hoping to advance the research in financial\nreasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a\nlarge-scale financial reasoning dataset characterized by its systematic CoT\nsynthesis optimization. We first introduce a comprehensive CoT synthesis\npipeline featuring Multi-perspective Knowledge Extraction (MKE) and\nSelf-Corrective Rewriting (SCR) to generate exhaustive and deep financial\nreasoning trajectories. Furthermore, a systematic investigation, termed CoT\nCube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-100K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-100K , hoping to advance the research in financial\nreasoning models."
                },
                "authors": [
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Lihong Wang"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaiyuan Zheng"
                    },
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Li"
                },
                "author": "Zhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02976v2",
                "updated": "2025-07-24T15:50:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    50,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-30T21:10:19Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    10,
                    19,
                    0,
                    181,
                    0
                ],
                "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench"
                },
                "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools."
                },
                "authors": [
                    {
                        "name": "Amirali Sajadi"
                    },
                    {
                        "name": "Kostadin Damevski"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18523v1",
                "updated": "2025-07-24T15:49:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:49:06Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "title": "The Moral Gap of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moral Gap of Large Language Models"
                },
                "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "arxiv_doi": "10.13140/RG.2.2.26221.70880",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.26221.70880",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20144v3",
                "updated": "2025-07-24T15:45:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    45,
                    59,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-27T14:35:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    35,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Robust sensitivity control in digital pathology via tile score\n  distribution matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust sensitivity control in digital pathology via tile score\n  distribution matching"
                },
                "summary": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems."
                },
                "authors": [
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Genevieve Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Camera ready version. Accepted at MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18515v1",
                "updated": "2025-07-24T15:36:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    36,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:36:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    36,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion:\n  Experience on WeChat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Dive into Retrieval-Augmented Generation for Code Completion:\n  Experience on WeChat"
                },
                "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments."
                },
                "authors": [
                    {
                        "name": "Zezhou Yang"
                    },
                    {
                        "name": "Ting Peng"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Hailiang Huang"
                    },
                    {
                        "name": "Yuetang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yuetang Deng"
                },
                "author": "Yuetang Deng",
                "arxiv_comment": "Accepted in ICSME 25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03572v3",
                "updated": "2025-07-25T08:05:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    5,
                    10,
                    4,
                    206,
                    0
                ],
                "published": "2024-05-06T15:48:14Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    48,
                    14,
                    0,
                    127,
                    0
                ],
                "title": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous\n  Driving Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous\n  Driving Research"
                },
                "summary": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license."
                },
                "authors": [
                    {
                        "name": "Mehdi Testouri"
                    },
                    {
                        "name": "Gamal Elghazaly"
                    },
                    {
                        "name": "Raphael Frank"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Frank"
                },
                "author": "Raphael Frank",
                "arxiv_doi": "10.1109/MITS.2025.3546755",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MITS.2025.3546755",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19780v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19780v5",
                "updated": "2025-07-24T15:23:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    23,
                    54,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-24T16:47:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    47,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model\n  Alignment"
                },
                "summary": "Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Yuhui Sun"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Zixi Li"
                    },
                    {
                        "name": "Zhenlong Yuan"
                    },
                    {
                        "name": "Jinman Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Zhao"
                },
                "arxiv_affiliation": "University of Toronto",
                "author": "Jinman Zhao",
                "arxiv_comment": "12 pages, 12 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19780v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19780v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18504v1",
                "updated": "2025-07-24T15:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:22:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    22,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyu Zhang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18489v1",
                "updated": "2025-07-24T15:02:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    2,
                    22,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T15:02:22Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    15,
                    2,
                    22,
                    3,
                    205,
                    0
                ],
                "title": "The Best is Yet to Come: Graph Convolution in the Testing Phase for\n  Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best is Yet to Come: Graph Convolution in the Testing Phase for\n  Multimodal Recommendation"
                },
                "summary": "The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems remain critical challenges, hindering their\npractical deployment in real-world scenarios. In the multimodal recommendation\n(MMRec) field, training GCNs requires more expensive time and space costs and\nexacerbates the gap between different modalities, resulting in sub-optimal\nrecommendation accuracy. This paper critically points out the inherent\nchallenges associated with adopting GCNs during the training phase in MMRec,\nrevealing that GCNs inevitably create unhelpful and even harmful pairs during\nmodel optimization and isolate different modalities. To this end, we propose\nFastMMRec, a highly efficient multimodal recommendation framework that deploys\ngraph convolutions exclusively during the testing phase, bypassing their use in\ntraining. We demonstrate that adopting GCNs solely in the testing phase\nsignificantly improves the model's efficiency and scalability while alleviating\nthe modality isolation problem often caused by using GCNs during the training\nphase. We conduct extensive experiments on three public datasets, consistently\ndemonstrating the performance superiority of FastMMRec over competitive\nbaselines while achieving efficiency and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems remain critical challenges, hindering their\npractical deployment in real-world scenarios. In the multimodal recommendation\n(MMRec) field, training GCNs requires more expensive time and space costs and\nexacerbates the gap between different modalities, resulting in sub-optimal\nrecommendation accuracy. This paper critically points out the inherent\nchallenges associated with adopting GCNs during the training phase in MMRec,\nrevealing that GCNs inevitably create unhelpful and even harmful pairs during\nmodel optimization and isolate different modalities. To this end, we propose\nFastMMRec, a highly efficient multimodal recommendation framework that deploys\ngraph convolutions exclusively during the testing phase, bypassing their use in\ntraining. We demonstrate that adopting GCNs solely in the testing phase\nsignificantly improves the model's efficiency and scalability while alleviating\nthe modality isolation problem often caused by using GCNs during the training\nphase. We conduct extensive experiments on three public datasets, consistently\ndemonstrating the performance superiority of FastMMRec over competitive\nbaselines while achieving efficiency and scalability."
                },
                "authors": [
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Zheyu Chen"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jinze Li"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    }
                ],
                "author_detail": {
                    "name": "Edith C. H. Ngai"
                },
                "author": "Edith C. H. Ngai",
                "arxiv_comment": "Accepted by MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18479v1",
                "updated": "2025-07-24T14:54:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:54:45Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts"
                },
                "summary": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Ngoc Luyen Le"
                    },
                    {
                        "name": "Marie-Hélène Abel"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Hélène Abel"
                },
                "author": "Marie-Hélène Abel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18476v1",
                "updated": "2025-07-24T14:50:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    50,
                    27,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:50:27Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    50,
                    27,
                    3,
                    205,
                    0
                ],
                "title": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning"
                },
                "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review."
                },
                "authors": [
                    {
                        "name": "Busra Icoz"
                    },
                    {
                        "name": "Goksel Biricik"
                    }
                ],
                "author_detail": {
                    "name": "Goksel Biricik"
                },
                "author": "Goksel Biricik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11188v2",
                "updated": "2025-07-24T14:39:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    39,
                    35,
                    3,
                    205,
                    0
                ],
                "published": "2024-12-15T13:45:11Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    45,
                    11,
                    6,
                    350,
                    0
                ],
                "title": "Assessing the Robustness and Resilience of U.S. Strategic Highways: A\n  Network Science Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Robustness and Resilience of U.S. Strategic Highways: A\n  Network Science Perspective"
                },
                "summary": "Network science is a powerful tool for analyzing transportation networks,\noffering insights into their structures and enabling the quantification of\nresilience and robustness. Understanding the underlying structures of\ntransportation networks is crucial for effective infrastructure planning and\nmaintenance. In military contexts, network science is valuable for analyzing\nlogistics networks, critical for the movement and supply of troops and\nequipment. The U.S. Army's logistical success, particularly in the\n\"fort-to-port\" phase, relies heavily on the Strategic Highway Network\n(STRAHNET) in the U.S., which is a system of public highways crucial for\nmilitary deployments. However, the shared nature of these networks with\ncivilian users introduces unique challenges, including vulnerabilities to\ncyberattacks and physical sabotage, which is highlighted by the concept of\ncontested logistics. This paper proposes a method using network science and\ngeographic information systems (GIS) to assess the robustness and resilience of\ntransportation networks, specifically applied to military logistics. Our\nfindings indicate that while the STRAHNET is robust against targeted\ndisruptions, it is more resilient to random disruptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network science is a powerful tool for analyzing transportation networks,\noffering insights into their structures and enabling the quantification of\nresilience and robustness. Understanding the underlying structures of\ntransportation networks is crucial for effective infrastructure planning and\nmaintenance. In military contexts, network science is valuable for analyzing\nlogistics networks, critical for the movement and supply of troops and\nequipment. The U.S. Army's logistical success, particularly in the\n\"fort-to-port\" phase, relies heavily on the Strategic Highway Network\n(STRAHNET) in the U.S., which is a system of public highways crucial for\nmilitary deployments. However, the shared nature of these networks with\ncivilian users introduces unique challenges, including vulnerabilities to\ncyberattacks and physical sabotage, which is highlighted by the concept of\ncontested logistics. This paper proposes a method using network science and\ngeographic information systems (GIS) to assess the robustness and resilience of\ntransportation networks, specifically applied to military logistics. Our\nfindings indicate that while the STRAHNET is robust against targeted\ndisruptions, it is more resilient to random disruptions."
                },
                "authors": [
                    {
                        "name": "Sukhwan Chung"
                    },
                    {
                        "name": "Daniel Sardak"
                    },
                    {
                        "name": "Jeffrey Cegan"
                    },
                    {
                        "name": "Igor Linkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Linkov"
                },
                "author": "Igor Linkov",
                "arxiv_doi": "10.1177/03611981251348445",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/03611981251348445",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "(2025). Assessing the Robustness and Resilience of U.S. Strategic\n  Highways: A Network Science Perspective. Transportation Research Record,\n  0(0).",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18455v1",
                "updated": "2025-07-24T14:36:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    36,
                    10,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:36:10Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    36,
                    10,
                    3,
                    205,
                    0
                ],
                "title": "LLM-based Embedders for Prior Case Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Embedders for Prior Case Retrieval"
                },
                "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models."
                },
                "authors": [
                    {
                        "name": "Damith Premasiri"
                    },
                    {
                        "name": "Tharindu Ranasinghe"
                    },
                    {
                        "name": "Ruslan Mitkov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Mitkov"
                },
                "author": "Ruslan Mitkov",
                "arxiv_comment": "Accepted in Recent Advancements in Natural Language Processing (RANLP\n  2025) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18452v1",
                "updated": "2025-07-24T14:35:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    35,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:35:52Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    35,
                    52,
                    3,
                    205,
                    0
                ],
                "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIFFA: Large Language Diffusion Models Can Listen and Understand"
                },
                "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git."
                },
                "authors": [
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Enzhi Wang"
                    },
                    {
                        "name": "Yujie Guo"
                    },
                    {
                        "name": "Haoqin Sun"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18442v1",
                "updated": "2025-07-24T14:26:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    26,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:26:41Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    26,
                    41,
                    3,
                    205,
                    0
                ],
                "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data"
                },
                "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data."
                },
                "authors": [
                    {
                        "name": "Rana Alshaikh"
                    },
                    {
                        "name": "Israa Alghanmi"
                    },
                    {
                        "name": "Shelan Jeawak"
                    }
                ],
                "author_detail": {
                    "name": "Shelan Jeawak"
                },
                "author": "Shelan Jeawak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06874v3",
                "updated": "2025-07-24T14:00:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    0,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-07T17:42:21Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    17,
                    42,
                    21,
                    5,
                    158,
                    0
                ],
                "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational\n  Dependencies on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational\n  Dependencies on Large Language Models"
                },
                "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts."
                },
                "authors": [
                    {
                        "name": "Ala Yankouskaya"
                    },
                    {
                        "name": "Areej B. Babiker"
                    },
                    {
                        "name": "Syeda W. F. Rizvi"
                    },
                    {
                        "name": "Sameha Alshakhsi"
                    },
                    {
                        "name": "Magnus Liebherr"
                    },
                    {
                        "name": "Raian Ali"
                    }
                ],
                "author_detail": {
                    "name": "Raian Ali"
                },
                "author": "Raian Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02452v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02452v3",
                "updated": "2025-07-24T13:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    59,
                    57,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-04T16:19:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    19,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models"
                },
                "summary": "Personalization of Large Vision-Language Models (LVLMs) involves customizing\nmodels to recognize specific users and object instances, and to generate\ncontextually tailored responses. Existing approaches typically rely on\ntime-consuming test-time training for each user or object, making them\nimpractical for real-world deployment, a limitation reflected in current\npersonalization benchmarks, which are focused on object-centric, single-concept\nevaluations. In this paper, we present a novel training-free approach to LVLM\npersonalization and introduce a comprehensive real-world benchmark designed to\nrigorously evaluate various aspects of the personalization task. Our method\nleverages pre-trained vision foundation models to extract distinctive features,\napplies retrieval-augmented generation (RAG) techniques to identify instances\nwithin visual inputs, and employs visual prompting strategies to guide model\noutputs. Our model-agnostic vision toolkit enables efficient and flexible\nmulti-concept personalization across both images and videos, without any\nadditional training. We achieve state-of-the-art results, surpassing existing\ntraining-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization of Large Vision-Language Models (LVLMs) involves customizing\nmodels to recognize specific users and object instances, and to generate\ncontextually tailored responses. Existing approaches typically rely on\ntime-consuming test-time training for each user or object, making them\nimpractical for real-world deployment, a limitation reflected in current\npersonalization benchmarks, which are focused on object-centric, single-concept\nevaluations. In this paper, we present a novel training-free approach to LVLM\npersonalization and introduce a comprehensive real-world benchmark designed to\nrigorously evaluate various aspects of the personalization task. Our method\nleverages pre-trained vision foundation models to extract distinctive features,\napplies retrieval-augmented generation (RAG) techniques to identify instances\nwithin visual inputs, and employs visual prompting strategies to guide model\noutputs. Our model-agnostic vision toolkit enables efficient and flexible\nmulti-concept personalization across both images and videos, without any\nadditional training. We achieve state-of-the-art results, surpassing existing\ntraining-based methods."
                },
                "authors": [
                    {
                        "name": "Soroush Seifi"
                    },
                    {
                        "name": "Vaggelis Dorovatas"
                    },
                    {
                        "name": "Daniel Olmeda Reino"
                    },
                    {
                        "name": "Rahaf Aljundi"
                    }
                ],
                "author_detail": {
                    "name": "Rahaf Aljundi"
                },
                "author": "Rahaf Aljundi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02452v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02452v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06735v2",
                "updated": "2025-07-24T13:57:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    8,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-09T10:48:00Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    48,
                    0,
                    2,
                    190,
                    0
                ],
                "title": "Residual Prior-driven Frequency-aware Network for Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual Prior-driven Frequency-aware Network for Image Fusion"
                },
                "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task."
                },
                "authors": [
                    {
                        "name": "Guan Zheng"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Wenhua Qian"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Runzhuo Ma"
                    }
                ],
                "author_detail": {
                    "name": "Runzhuo Ma"
                },
                "author": "Runzhuo Ma",
                "arxiv_doi": "10.1145/3746027.3754944",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3754944",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.06735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18417v1",
                "updated": "2025-07-24T13:57:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    5,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:57:05Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    5,
                    3,
                    205,
                    0
                ],
                "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs"
                },
                "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps)."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17727v2",
                "updated": "2025-07-24T13:55:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    55,
                    49,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-23T17:41:55Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    41,
                    55,
                    2,
                    204,
                    0
                ],
                "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust\n  Under-Canopy Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust\n  Under-Canopy Navigation"
                },
                "summary": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance."
                },
                "authors": [
                    {
                        "name": "Robel Mamo"
                    },
                    {
                        "name": "Taeyeong Choi"
                    }
                ],
                "author_detail": {
                    "name": "Taeyeong Choi"
                },
                "author": "Taeyeong Choi",
                "arxiv_comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07893v3",
                "updated": "2025-07-24T13:52:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    52,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T16:22:41Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    22,
                    41,
                    3,
                    191,
                    0
                ],
                "title": "An Integrated Framework of Prompt Engineering and Multidimensional\n  Knowledge Graphs for Legal Dispute Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Integrated Framework of Prompt Engineering and Multidimensional\n  Knowledge Graphs for Legal Dispute Analysis"
                },
                "summary": "Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems."
                },
                "authors": [
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jianglong Qing"
                    },
                    {
                        "name": "Qing xu"
                    },
                    {
                        "name": "Kaiwen Pan"
                    },
                    {
                        "name": "Ting luo"
                    }
                ],
                "author_detail": {
                    "name": "Ting luo"
                },
                "author": "Ting luo",
                "arxiv_comment": "19 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30, 91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; K.5.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15487v3",
                "updated": "2025-07-24T13:47:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    47,
                    56,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-21T14:23:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size."
                },
                "authors": [
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Serena Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Lucia Passaro"
                    },
                    {
                        "name": "Irene Sucameli"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "Accepted for publication in Findings of ACL 2025",
                "arxiv_journal_ref": "In Findings of the Association for Computational Linguistics: ACL\n  2025, pages 17335-17355, Vienna, Austria. Association for Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18392v1",
                "updated": "2025-07-24T13:15:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    15,
                    21,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:15:21Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    15,
                    21,
                    3,
                    205,
                    0
                ],
                "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy"
                },
                "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study."
                },
                "authors": [
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Lilach Eden"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    }
                ],
                "author_detail": {
                    "name": "Michal Shmueli-Scheuer"
                },
                "author": "Michal Shmueli-Scheuer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18391v1",
                "updated": "2025-07-24T13:14:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    14,
                    25,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T13:14:25Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    14,
                    25,
                    3,
                    205,
                    0
                ],
                "title": "Revisiting LLM Reasoning via Information Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting LLM Reasoning via Information Bottleneck"
                },
                "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance."
                },
                "authors": [
                    {
                        "name": "Shiye Lei"
                    },
                    {
                        "name": "Zhihao Cheng"
                    },
                    {
                        "name": "Kai Jia"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23276v2",
                "updated": "2025-07-24T13:13:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    13,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-29T15:02:47Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    2,
                    47,
                    6,
                    180,
                    0
                ],
                "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games"
                },
                "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim"
                },
                "authors": [
                    {
                        "name": "David Guzman Piedrahita"
                    },
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Giorgia Ramponi"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "arxiv_comment": "Published at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18368v1",
                "updated": "2025-07-24T12:47:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    47,
                    29,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:47:29Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    47,
                    29,
                    3,
                    205,
                    0
                ],
                "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios"
                },
                "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance."
                },
                "authors": [
                    {
                        "name": "Zhuang Qiang Bok"
                    },
                    {
                        "name": "Watson Wei Khong Chua"
                    }
                ],
                "author_detail": {
                    "name": "Watson Wei Khong Chua"
                },
                "author": "Watson Wei Khong Chua",
                "arxiv_comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18366v1",
                "updated": "2025-07-24T12:46:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    40,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:46:40Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    40,
                    3,
                    205,
                    0
                ],
                "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation"
                },
                "summary": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation."
                },
                "authors": [
                    {
                        "name": "Lakshmana Sri Harsha Nemani"
                    },
                    {
                        "name": "P. K. Srijith"
                    },
                    {
                        "name": "Tomasz Kuśmierczyk"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kuśmierczyk"
                },
                "author": "Tomasz Kuśmierczyk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18365v1",
                "updated": "2025-07-24T12:46:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecPS: Privacy Risk Scoring for Recommender Systems"
                },
                "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md."
                },
                "authors": [
                    {
                        "name": "Jiajie He"
                    },
                    {
                        "name": "Yuechun Gu"
                    },
                    {
                        "name": "Keke Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keke Chen"
                },
                "author": "Keke Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12988v3",
                "updated": "2025-07-24T12:42:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    42,
                    50,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-18T16:11:54Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs"
                },
                "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards."
                },
                "authors": [
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ishita Agrawal"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08017v3",
                "updated": "2025-07-24T12:23:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    23,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18343v1",
                "updated": "2025-07-24T12:16:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    16,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:16:52Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    16,
                    52,
                    3,
                    205,
                    0
                ],
                "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM\n  Pre-Annotations with Human Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Annotation for Propaganda Detection: Integrating LLM\n  Pre-Annotations with Human Intelligence"
                },
                "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository."
                },
                "authors": [
                    {
                        "name": "Ariana Sahitaj"
                    },
                    {
                        "name": "Premtim Sahitaj"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Jiaao Li"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "NLP4PI at ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07464v2",
                "updated": "2025-07-24T12:13:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    13,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-10T06:31:26Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    6,
                    31,
                    26,
                    3,
                    191,
                    0
                ],
                "title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind\n  Face Restoration in Adverse Weather Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind\n  Face Restoration in Adverse Weather Conditions"
                },
                "summary": "With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios."
                },
                "authors": [
                    {
                        "name": "Chang-Hwan Son"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Hwan Son"
                },
                "author": "Chang-Hwan Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18340v1",
                "updated": "2025-07-24T12:12:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    12,
                    4,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T12:12:04Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    12,
                    4,
                    3,
                    205,
                    0
                ],
                "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for\n  In-Context Learning"
                },
                "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR."
                },
                "authors": [
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Bingchen Huang"
                    },
                    {
                        "name": "Zhiling Wang"
                    },
                    {
                        "name": "Yuanchao Du"
                    },
                    {
                        "name": "Junfeng Luo"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Zhineng chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhineng chen"
                },
                "author": "Zhineng chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10062v2",
                "updated": "2025-07-24T11:58:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    58,
                    1,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-14T08:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "LLMShot: Reducing snapshot testing maintenance via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMShot: Reducing snapshot testing maintenance via LLMs"
                },
                "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages Vision-Language\nModels (VLMs) to automatically analyze snapshot test failures through semantic\nclassification of UI changes. To evaluate LLMShot's effectiveness, we developed\na comprehensive dataset using a feature-rich iOS application with configurable\nfeature flags, creating realistic scenarios that produce authentic snapshot\ndifferences representative of real development workflows. Our evaluation using\nGemma3 models demonstrates strong classification performance, with the 12B\nvariant achieving over 84% recall in identifying failure root causes while the\n4B model offers practical deployment advantages with acceptable performance for\ncontinuous integration environments. However, our exploration of selective\nignore mechanisms revealed significant limitations in current prompting-based\napproaches for controllable visual reasoning. LLMShot represents the first\nautomated approach to semantic snapshot test analysis, offering developers\nstructured insights that can substantially reduce manual triage effort and\nadvance toward more intelligent UI testing paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages Vision-Language\nModels (VLMs) to automatically analyze snapshot test failures through semantic\nclassification of UI changes. To evaluate LLMShot's effectiveness, we developed\na comprehensive dataset using a feature-rich iOS application with configurable\nfeature flags, creating realistic scenarios that produce authentic snapshot\ndifferences representative of real development workflows. Our evaluation using\nGemma3 models demonstrates strong classification performance, with the 12B\nvariant achieving over 84% recall in identifying failure root causes while the\n4B model offers practical deployment advantages with acceptable performance for\ncontinuous integration environments. However, our exploration of selective\nignore mechanisms revealed significant limitations in current prompting-based\napproaches for controllable visual reasoning. LLMShot represents the first\nautomated approach to semantic snapshot test analysis, offering developers\nstructured insights that can substantially reduce manual triage effort and\nadvance toward more intelligent UI testing paradigms."
                },
                "authors": [
                    {
                        "name": "Ergün Batuhan Kaynak"
                    },
                    {
                        "name": "Mayasah Lami"
                    },
                    {
                        "name": "Sahand Moslemi"
                    },
                    {
                        "name": "Anil Koyuncu"
                    }
                ],
                "author_detail": {
                    "name": "Anil Koyuncu"
                },
                "author": "Anil Koyuncu",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18328v1",
                "updated": "2025-07-24T11:54:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    54,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:54:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    54,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of\n  Information Optimization in Vehicular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of\n  Information Optimization in Vehicular Networks"
                },
                "summary": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI."
                },
                "authors": [
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE TMC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18326v1",
                "updated": "2025-07-24T11:51:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    51,
                    55,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:51:55Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    51,
                    55,
                    3,
                    205,
                    0
                ],
                "title": "A Concept for Efficient Scalability of Automated Driving Allowing for\n  Technical, Legal, Cultural, and Ethical Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concept for Efficient Scalability of Automated Driving Allowing for\n  Technical, Legal, Cultural, and Ethical Differences"
                },
                "summary": "Efficient scalability of automated driving (AD) is key to reducing costs,\nenhancing safety, conserving resources, and maximizing impact. However,\nresearch focuses on specific vehicles and context, while broad deployment\nrequires scalability across various configurations and environments.\nDifferences in vehicle types, sensors, actuators, but also traffic regulations,\nlegal requirements, cultural dynamics, or even ethical paradigms demand high\nflexibility of data-driven developed capabilities. In this paper, we address\nthe challenge of scalable adaptation of generic capabilities to desired systems\nand environments. Our concept follows a two-stage fine-tuning process. In the\nfirst stage, fine-tuning to the specific environment takes place through a\ncountry-specific reward model that serves as an interface between technological\nadaptations and socio-political requirements. In the second stage,\nvehicle-specific transfer learning facilitates system adaptation and governs\nthe validation of design decisions. In sum, our concept offers a data-driven\nprocess that integrates both technological and socio-political aspects,\nenabling effective scalability across technical, legal, cultural, and ethical\ndifferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient scalability of automated driving (AD) is key to reducing costs,\nenhancing safety, conserving resources, and maximizing impact. However,\nresearch focuses on specific vehicles and context, while broad deployment\nrequires scalability across various configurations and environments.\nDifferences in vehicle types, sensors, actuators, but also traffic regulations,\nlegal requirements, cultural dynamics, or even ethical paradigms demand high\nflexibility of data-driven developed capabilities. In this paper, we address\nthe challenge of scalable adaptation of generic capabilities to desired systems\nand environments. Our concept follows a two-stage fine-tuning process. In the\nfirst stage, fine-tuning to the specific environment takes place through a\ncountry-specific reward model that serves as an interface between technological\nadaptations and socio-political requirements. In the second stage,\nvehicle-specific transfer learning facilitates system adaptation and governs\nthe validation of design decisions. In sum, our concept offers a data-driven\nprocess that integrates both technological and socio-political aspects,\nenabling effective scalability across technical, legal, cultural, and ethical\ndifferences."
                },
                "authors": [
                    {
                        "name": "Lars Ullrich"
                    },
                    {
                        "name": "Michael Buchholz"
                    },
                    {
                        "name": "Jonathan Petit"
                    },
                    {
                        "name": "Klaus Dietmayer"
                    },
                    {
                        "name": "Knut Graichen"
                    }
                ],
                "author_detail": {
                    "name": "Knut Graichen"
                },
                "author": "Knut Graichen",
                "arxiv_comment": "Accepted to be published at 2025 28th IEEE International Conference\n  on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November\n  18-21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08621v2",
                "updated": "2025-07-24T11:49:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    49,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-11T14:23:40Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    23,
                    40,
                    4,
                    192,
                    0
                ],
                "title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1"
                },
                "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings."
                },
                "authors": [
                    {
                        "name": "Marcin Pietroń"
                    },
                    {
                        "name": "Rafał Olszowski"
                    },
                    {
                        "name": "Jakub Gomułka"
                    },
                    {
                        "name": "Filip Gampel"
                    },
                    {
                        "name": "Andrzej Tomski"
                    }
                ],
                "author_detail": {
                    "name": "Andrzej Tomski"
                },
                "author": "Andrzej Tomski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12102v3",
                "updated": "2025-07-24T11:44:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    44,
                    19,
                    3,
                    205,
                    0
                ],
                "published": "2023-12-19T12:26:57Z",
                "published_parsed": [
                    2023,
                    12,
                    19,
                    12,
                    26,
                    57,
                    1,
                    353,
                    0
                ],
                "title": "I-CEE: Tailoring Explanations of Image Classification Models to User\n  Expertise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-CEE: Tailoring Explanations of Image Classification Models to User\n  Expertise"
                },
                "summary": "Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI"
                },
                "authors": [
                    {
                        "name": "Yao Rong"
                    },
                    {
                        "name": "Peizhu Qian"
                    },
                    {
                        "name": "Vaibhav Unhelkar"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14019v3",
                "updated": "2025-07-24T11:38:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    38,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2024-12-18T16:37:51Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    37,
                    51,
                    2,
                    353,
                    0
                ],
                "title": "Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases"
                },
                "summary": "Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order."
                },
                "authors": [
                    {
                        "name": "Federico Baldo"
                    },
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18316v1",
                "updated": "2025-07-24T11:32:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    32,
                    31,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:32:31Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    32,
                    31,
                    3,
                    205,
                    0
                ],
                "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation"
                },
                "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs)."
                },
                "authors": [
                    {
                        "name": "Michael Konstantinou"
                    },
                    {
                        "name": "Renzo Degiovanni"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Mike Papadakis"
                    }
                ],
                "author_detail": {
                    "name": "Mike Papadakis"
                },
                "author": "Mike Papadakis",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v4",
                "updated": "2025-07-24T11:31:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    31,
                    3,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Yugui Lin"
                    },
                    {
                        "name": "Pietro Lio"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Code and data will be made available on request to the corresponding\n  author. V4 Update: Add Future Work; Improve Typesetting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18305v1",
                "updated": "2025-07-24T11:24:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    24,
                    35,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T11:24:35Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    24,
                    35,
                    3,
                    205,
                    0
                ],
                "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large\n  Reasoning Models for Fun or Profit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadReasoner: Planting Tunable Overthinking Backdoors into Large\n  Reasoning Models for Fun or Profit"
                },
                "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zekun Fei"
                    },
                    {
                        "name": "Jianing Geng"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Lihai Nie"
                    },
                    {
                        "name": "Zheli Liu"
                    },
                    {
                        "name": "Yiming Li"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Li"
                },
                "author": "Yiming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12972v2",
                "updated": "2025-07-24T11:14:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    14,
                    43,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-17T09:31:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    31,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning"
                },
                "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."
                },
                "authors": [
                    {
                        "name": "Junming Liu"
                    },
                    {
                        "name": "Siyuan Meng"
                    },
                    {
                        "name": "Yanting Gao"
                    },
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Yirong Chen"
                    },
                    {
                        "name": "Zilin Bian"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "14 pages, 7 figures, 6 tables; Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10009v2",
                "updated": "2025-07-24T11:09:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    9,
                    58,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-13T03:40:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    40,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM"
                },
                "summary": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Pengcheng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Luo"
                },
                "author": "Pengcheng Luo",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19795v2",
                "updated": "2025-07-24T11:08:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    8,
                    59,
                    3,
                    205,
                    0
                ],
                "published": "2024-07-29T08:38:46Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    8,
                    38,
                    46,
                    0,
                    211,
                    0
                ],
                "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks"
                },
                "summary": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "Junehyoung Kwon"
                    },
                    {
                        "name": "JungMin Yun"
                    },
                    {
                        "name": "Seunguk Yu"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "arxiv_comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17596v2",
                "updated": "2025-07-24T11:04:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    11,
                    4,
                    42,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-23T15:28:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    28,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving"
                },
                "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix."
                },
                "authors": [
                    {
                        "name": "Maciej K. Wozniak"
                    },
                    {
                        "name": "Lianhang Liu"
                    },
                    {
                        "name": "Yixi Cai"
                    },
                    {
                        "name": "Patric Jensfelt"
                    }
                ],
                "author_detail": {
                    "name": "Patric Jensfelt"
                },
                "author": "Patric Jensfelt",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18294v1",
                "updated": "2025-07-24T10:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    57,
                    32,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:57:32Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    57,
                    32,
                    3,
                    205,
                    0
                ],
                "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient\n  Stylistic Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient\n  Stylistic Transfer"
                },
                "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs."
                },
                "authors": [
                    {
                        "name": "Pritika Ramu"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Meghanath M Y"
                    },
                    {
                        "name": "Varsha Sankar"
                    },
                    {
                        "name": "Debraj Basu"
                    }
                ],
                "author_detail": {
                    "name": "Debraj Basu"
                },
                "author": "Debraj Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18290v1",
                "updated": "2025-07-24T10:52:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    52,
                    22,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:52:22Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    52,
                    22,
                    3,
                    205,
                    0
                ],
                "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights"
                },
                "summary": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance."
                },
                "authors": [
                    {
                        "name": "Antonino Rotolo"
                    },
                    {
                        "name": "Beatrice Ferrigno"
                    },
                    {
                        "name": "Jose Miguel Angel Garcia Godinez"
                    },
                    {
                        "name": "Claudio Novelli"
                    },
                    {
                        "name": "Giovanni Sartor"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Sartor"
                },
                "author": "Giovanni Sartor",
                "arxiv_comment": "24 pages, 1 figure. To be published in: The Philosophical Foundations\n  of Information Technology Law. Oxford University Press, Oxford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18289v1",
                "updated": "2025-07-24T10:51:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    51,
                    11,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:51:11Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    51,
                    11,
                    3,
                    205,
                    0
                ],
                "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling"
                },
                "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Wenzhang Yang"
                    },
                    {
                        "name": "Yuekun Wang"
                    },
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Shaohua Wang"
                    },
                    {
                        "name": "Yinxing Xue"
                    },
                    {
                        "name": "Lijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Zhang"
                },
                "author": "Lijun Zhang",
                "arxiv_comment": "15 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06788v2",
                "updated": "2025-07-24T10:29:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    29,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-10T18:59:58Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    58,
                    0,
                    41,
                    0
                ],
                "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models"
                },
                "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE."
                },
                "authors": [
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Haoge Deng"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "20 pages, 10 figures, Accepted by ICCV2025 (highlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18267v1",
                "updated": "2025-07-24T10:11:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    11,
                    45,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T10:11:45Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    11,
                    45,
                    3,
                    205,
                    0
                ],
                "title": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR)\n  Software Bugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR)\n  Software Bugs"
                },
                "summary": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types."
                },
                "authors": [
                    {
                        "name": "Zeqin Liao"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Peifan Reng"
                    },
                    {
                        "name": "Henglong Liang"
                    },
                    {
                        "name": "Zixu Gao"
                    },
                    {
                        "name": "Zhixiang Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yuhong Nan"
                    }
                ],
                "author_detail": {
                    "name": "Yuhong Nan"
                },
                "author": "Yuhong Nan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11361v2",
                "updated": "2025-07-24T10:05:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    5,
                    6,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-15T14:28:23Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    14,
                    28,
                    23,
                    1,
                    196,
                    0
                ],
                "title": "Adaptive Robust Optimization for European Electricity System Planning\n  Considering Regional Dunkelflaute Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Robust Optimization for European Electricity System Planning\n  Considering Regional Dunkelflaute Events"
                },
                "summary": "This study develops a capacity expansion model for a fully decarbonized\nEuropean electricity system using an Adaptive Robust Optimization (ARO)\nframework. The model endogenously identifies the worst regional Dunkelflaute\nevents, prolonged periods of low wind and solar availability, and incorporates\nmultiple extreme weather realizations within a single optimization run. Results\nshow that system costs rise nonlinearly with the geographic extent of these\nevents: a single worst-case regional disruption increases costs by 9%, but\nbroader disruptions across multiple regions lead to much sharper increases, up\nto 51%. As Dunkelflaute conditions extend across most of Europe, additional\ncost impacts level off, with a maximum increase of 71%. The optimal technology\nmix evolves with the severity of weather stress: while renewables, batteries,\nand interregional transmission are sufficient to manage localized events,\nlarge-scale disruptions require long-term hydrogen storage and load shedding to\nmaintain system resilience. Central European regions, especially Germany and\nFrance, emerge as systemic bottlenecks, while peripheral regions bear the cost\nof compensatory overbuilding. These findings underscore the need for a\ncoordinated European policy strategy that goes beyond national planning to\nsupport cross-border infrastructure investment, scale up flexible technologies\nsuch as long-duration storage, and promote a geographically balanced deployment\nof renewables to mitigate systemic risks associated with Dunkelflaute events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a capacity expansion model for a fully decarbonized\nEuropean electricity system using an Adaptive Robust Optimization (ARO)\nframework. The model endogenously identifies the worst regional Dunkelflaute\nevents, prolonged periods of low wind and solar availability, and incorporates\nmultiple extreme weather realizations within a single optimization run. Results\nshow that system costs rise nonlinearly with the geographic extent of these\nevents: a single worst-case regional disruption increases costs by 9%, but\nbroader disruptions across multiple regions lead to much sharper increases, up\nto 51%. As Dunkelflaute conditions extend across most of Europe, additional\ncost impacts level off, with a maximum increase of 71%. The optimal technology\nmix evolves with the severity of weather stress: while renewables, batteries,\nand interregional transmission are sufficient to manage localized events,\nlarge-scale disruptions require long-term hydrogen storage and load shedding to\nmaintain system resilience. Central European regions, especially Germany and\nFrance, emerge as systemic bottlenecks, while peripheral regions bear the cost\nof compensatory overbuilding. These findings underscore the need for a\ncoordinated European policy strategy that goes beyond national planning to\nsupport cross-border infrastructure investment, scale up flexible technologies\nsuch as long-duration storage, and promote a geographically balanced deployment\nof renewables to mitigate systemic risks associated with Dunkelflaute events."
                },
                "authors": [
                    {
                        "name": "Maximilian Bernecker"
                    },
                    {
                        "name": "Smaranda Sgarciu"
                    },
                    {
                        "name": "Xiaoming Kan"
                    },
                    {
                        "name": "Mehrnaz Anvari"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Felix Müsgens"
                    }
                ],
                "author_detail": {
                    "name": "Felix Müsgens"
                },
                "author": "Felix Müsgens",
                "arxiv_comment": "Code and data can be found on github:\n  https://github.com/bernemax/ARO_Dunkelflaute_Europe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14400v4",
                "updated": "2025-07-24T10:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    10,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-20T09:37:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    37,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPS: Hard Preference Sampling for Human Preference Alignment"
                },
                "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Wanyu Lin"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Pan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pan Zhou"
                },
                "author": "Pan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18252v1",
                "updated": "2025-07-24T09:49:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    49,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:49:53Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    49,
                    53,
                    3,
                    205,
                    0
                ],
                "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning"
                },
                "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics."
                },
                "authors": [
                    {
                        "name": "Dongyang Guo"
                    },
                    {
                        "name": "Yasmeen Abdrabou"
                    },
                    {
                        "name": "Enkeleda Thaqi"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16068v2",
                "updated": "2025-07-24T09:25:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    25,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-21T21:09:15Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    21,
                    9,
                    15,
                    0,
                    202,
                    0
                ],
                "title": "Compositional Coordination for Multi-Robot Teams with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Coordination for Multi-Robot Teams with Large Language\n  Models"
                },
                "summary": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb"
                },
                "authors": [
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Guangyao Shi"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "Gaurav S. Sukhatme"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav S. Sukhatme"
                },
                "author": "Gaurav S. Sukhatme",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18237v1",
                "updated": "2025-07-24T09:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    24,
                    29,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    24,
                    29,
                    3,
                    205,
                    0
                ],
                "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in\n  Collaborative Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in\n  Collaborative Perception"
                },
                "summary": "Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA."
                },
                "authors": [
                    {
                        "name": "Chengchang Tian"
                    },
                    {
                        "name": "Jianwei Ma"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Zhanye Chen"
                    },
                    {
                        "name": "Honghao Wei"
                    },
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Wei Hong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hong"
                },
                "author": "Wei Hong",
                "arxiv_comment": "ICCV 2025, accepted as poster. 22 pages including supplementary\n  materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11482v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11482v8",
                "updated": "2025-07-24T09:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    19,
                    38,
                    3,
                    205,
                    0
                ],
                "published": "2023-11-20T01:51:13Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    1,
                    51,
                    13,
                    0,
                    324,
                    0
                ],
                "title": "Meta Prompting for AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta Prompting for AI Systems"
                },
                "summary": "We introduce Meta Prompting (MP), a framework that elevates the reasoning\ncapabilities of large language models (LLMs) by focusing on the formal\nstructure of a task rather than content-specific examples. We establish a\ntheoretical foundation for this paradigm, formalizing MP as a functor that maps\na category of tasks to a category of structured prompts, thereby guaranteeing\nthat compositional problem-solving strategies can be systematically decomposed\ninto modular prompt structures. We extend this concept to Recursive Meta\nPrompting (RMP), an automated process where an LLM can generate and refine its\nown prompts. We model this self-improvement loop formally as a monad, providing\na principled framework for automated prompt engineering. Our claims are\nvalidated through extensive experiments demonstrating that a Qwen-72B base\nmodel, guided by a single, example-agnostic meta-prompt, achieves\nstate-of-the-art results on MATH, GSM8K, and Game of 24. These results are\nachieved with substantial token efficiency gains over traditional few-shot\nmethods. Project Page: https://github.com/meta-prompting/meta-prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Meta Prompting (MP), a framework that elevates the reasoning\ncapabilities of large language models (LLMs) by focusing on the formal\nstructure of a task rather than content-specific examples. We establish a\ntheoretical foundation for this paradigm, formalizing MP as a functor that maps\na category of tasks to a category of structured prompts, thereby guaranteeing\nthat compositional problem-solving strategies can be systematically decomposed\ninto modular prompt structures. We extend this concept to Recursive Meta\nPrompting (RMP), an automated process where an LLM can generate and refine its\nown prompts. We model this self-improvement loop formally as a monad, providing\na principled framework for automated prompt engineering. Our claims are\nvalidated through extensive experiments demonstrating that a Qwen-72B base\nmodel, guided by a single, example-agnostic meta-prompt, achieves\nstate-of-the-art results on MATH, GSM8K, and Game of 24. These results are\nachieved with substantial token efficiency gains over traditional few-shot\nmethods. Project Page: https://github.com/meta-prompting/meta-prompting."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Project Page: https://github.com/meta-prompting/meta-prompting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11482v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11482v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18224v1",
                "updated": "2025-07-24T09:17:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    17,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:17:41Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    17,
                    41,
                    3,
                    205,
                    0
                ],
                "title": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design\n  via Autoregressive Graph Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design\n  via Autoregressive Graph Generation"
                },
                "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have emerged\nas a powerful solution for dealing with complex problems across diverse\ndomains. The effectiveness of MAS is critically dependent on its collaboration\ntopology, which has become a focal point for automated design research.\nHowever, existing approaches are fundamentally constrained by their reliance on\na template graph modification paradigm with a predefined set of agents and\nhard-coded interaction structures, significantly limiting their adaptability to\ntask-specific requirements. To address these limitations, we reframe MAS design\nas a conditional autoregressive graph generation task, where both the system\ncomposition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing\nthe collaboration graph from scratch. Conditioned on a natural language task\nquery, ARG-Designer sequentially and dynamically determines the required number\nof agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative\napproach creates a customized topology in a flexible and extensible manner,\nprecisely tailored to the unique demands of different tasks. Extensive\nexperiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly\ngreater token efficiency and enhanced extensibility. The source code of\nARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) based on large language models (LLMs) have emerged\nas a powerful solution for dealing with complex problems across diverse\ndomains. The effectiveness of MAS is critically dependent on its collaboration\ntopology, which has become a focal point for automated design research.\nHowever, existing approaches are fundamentally constrained by their reliance on\na template graph modification paradigm with a predefined set of agents and\nhard-coded interaction structures, significantly limiting their adaptability to\ntask-specific requirements. To address these limitations, we reframe MAS design\nas a conditional autoregressive graph generation task, where both the system\ncomposition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing\nthe collaboration graph from scratch. Conditioned on a natural language task\nquery, ARG-Designer sequentially and dynamically determines the required number\nof agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative\napproach creates a customized topology in a flexible and extensible manner,\nprecisely tailored to the unique demands of different tasks. Extensive\nexperiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly\ngreater token efficiency and enhanced extensibility. The source code of\nARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer."
                },
                "authors": [
                    {
                        "name": "Shiyuan Li"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Chengqi Zhang"
                    },
                    {
                        "name": "Shirui Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shirui Pan"
                },
                "author": "Shirui Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18223v1",
                "updated": "2025-07-24T09:17:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    17,
                    13,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:17:13Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    17,
                    13,
                    3,
                    205,
                    0
                ],
                "title": "GenAI for Automotive Software Development: From Requirements to Wheels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI for Automotive Software Development: From Requirements to Wheels"
                },
                "summary": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities."
                },
                "authors": [
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Andre Schamschurko"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18219v1",
                "updated": "2025-07-24T09:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    15,
                    7,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:15:07Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    15,
                    7,
                    3,
                    205,
                    0
                ],
                "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with\n  Personalized Aggregation and Cluster-Aware Broadcasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with\n  Personalized Aggregation and Cluster-Aware Broadcasting"
                },
                "summary": "Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis."
                },
                "authors": [
                    {
                        "name": "Zhongzheng Yuan"
                    },
                    {
                        "name": "Lianshuai Guo"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Yinlin Zhu"
                    },
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Meixia Qu"
                    }
                ],
                "author_detail": {
                    "name": "Meixia Qu"
                },
                "author": "Meixia Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18215v1",
                "updated": "2025-07-24T09:09:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    9,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:09:36Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    9,
                    36,
                    3,
                    205,
                    0
                ],
                "title": "Information Security Based on LLM Approaches: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Security Based on LLM Approaches: A Review"
                },
                "summary": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system."
                },
                "authors": [
                    {
                        "name": "Chang Gong"
                    },
                    {
                        "name": "Zhongwen Li"
                    },
                    {
                        "name": "Xiaoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Li"
                },
                "author": "Xiaoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18212v1",
                "updated": "2025-07-24T09:07:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    7,
                    20,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T09:07:20Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    7,
                    20,
                    3,
                    205,
                    0
                ],
                "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with\n  Magnitude Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with\n  Magnitude Compensation"
                },
                "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%."
                },
                "authors": [
                    {
                        "name": "Xinrui Chen"
                    },
                    {
                        "name": "Hongxing Zhang"
                    },
                    {
                        "name": "Fanyi Zeng"
                    },
                    {
                        "name": "Yongxian Wei"
                    },
                    {
                        "name": "Yizhi Wang"
                    },
                    {
                        "name": "Xitong Ling"
                    },
                    {
                        "name": "Guanghao Li"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20658v2",
                "updated": "2025-07-24T09:02:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    9,
                    2,
                    40,
                    3,
                    205,
                    0
                ],
                "published": "2025-05-27T03:07:25Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    7,
                    25,
                    1,
                    147,
                    0
                ],
                "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic\n  Using LLMs with Diverse External Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Transformation from Natural Language to Signal Temporal Logic\n  Using LLMs with Diverse External Knowledge"
                },
                "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets."
                },
                "authors": [
                    {
                        "name": "Yue Fang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Jie An"
                    },
                    {
                        "name": "Hongshen Chen"
                    },
                    {
                        "name": "Xiaohong Chen"
                    },
                    {
                        "name": "Naijun Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Naijun Zhan"
                },
                "author": "Naijun Zhan",
                "arxiv_comment": "11 pages, 5 figures, published to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18203v1",
                "updated": "2025-07-24T08:58:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    58,
                    47,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    58,
                    47,
                    3,
                    205,
                    0
                ],
                "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to\n  Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to\n  Misinformation"
                },
                "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Kyubeen Han"
                    },
                    {
                        "name": "Junseo Jang"
                    },
                    {
                        "name": "Hongjin Kim"
                    },
                    {
                        "name": "Geunyeong Jeong"
                    },
                    {
                        "name": "Harksoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Harksoo Kim"
                },
                "author": "Harksoo Kim",
                "arxiv_comment": "ACL 2025 Main Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18202v1",
                "updated": "2025-07-24T08:58:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    58,
                    41,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:58:41Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    58,
                    41,
                    3,
                    205,
                    0
                ],
                "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token\n  Probability Method for Poisoned Document Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token\n  Probability Method for Poisoned Document Detection"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings."
                },
                "authors": [
                    {
                        "name": "San Kim"
                    },
                    {
                        "name": "Jonghwi Kim"
                    },
                    {
                        "name": "Yejin Jeon"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_comment": "18 pages, accepted to ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05086v2",
                "updated": "2025-07-24T08:52:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    52,
                    22,
                    3,
                    205,
                    0
                ],
                "published": "2025-05-08T09:34:15Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    34,
                    15,
                    3,
                    128,
                    0
                ],
                "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning"
                },
                "summary": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks."
                },
                "authors": [
                    {
                        "name": "Le-Trung Nguyen"
                    },
                    {
                        "name": "Ael Quelennec"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18182v1",
                "updated": "2025-07-24T08:28:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    28,
                    17,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:28:17Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    28,
                    17,
                    3,
                    205,
                    0
                ],
                "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations."
                },
                "authors": [
                    {
                        "name": "Wonjun Jeong"
                    },
                    {
                        "name": "Dongseok Kim"
                    },
                    {
                        "name": "Taegkeun Whangbo"
                    }
                ],
                "author_detail": {
                    "name": "Taegkeun Whangbo"
                },
                "author": "Taegkeun Whangbo",
                "arxiv_comment": "34 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18181v1",
                "updated": "2025-07-24T08:27:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    27,
                    53,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:27:53Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    27,
                    53,
                    3,
                    205,
                    0
                ],
                "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding"
                },
                "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy."
                },
                "authors": [
                    {
                        "name": "Linye Wei"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Songqiang Xu"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18178v1",
                "updated": "2025-07-24T08:24:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    24,
                    52,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:24:52Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    24,
                    52,
                    3,
                    205,
                    0
                ],
                "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using\n  Cognitive Dual-System Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using\n  Cognitive Dual-System Theory"
                },
                "summary": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning."
                },
                "authors": [
                    {
                        "name": "Mutian Yang"
                    },
                    {
                        "name": "Jiandong Gao"
                    },
                    {
                        "name": "Ji Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wu"
                },
                "author": "Ji Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16473v2",
                "updated": "2025-07-24T08:23:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    23,
                    56,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T11:22:58Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    11,
                    22,
                    58,
                    1,
                    203,
                    0
                ],
                "title": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol."
                },
                "authors": [
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Yaren Zhang"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Qiong Cao"
                    },
                    {
                        "name": "Chao Xue"
                    },
                    {
                        "name": "Xiaodong He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong He"
                },
                "author": "Xiaodong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18174v1",
                "updated": "2025-07-24T08:17:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    17,
                    37,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:17:37Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    17,
                    37,
                    3,
                    205,
                    0
                ],
                "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs"
                },
                "summary": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications."
                },
                "authors": [
                    {
                        "name": "Rashed Al Amin"
                    },
                    {
                        "name": "Roman Obermaisser"
                    }
                ],
                "author_detail": {
                    "name": "Roman Obermaisser"
                },
                "author": "Roman Obermaisser",
                "arxiv_comment": "This paper has been accepted for the 67th International Symposium on\n  ELMAR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18167v1",
                "updated": "2025-07-24T08:04:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    4,
                    39,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:04:39Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    4,
                    39,
                    3,
                    205,
                    0
                ],
                "title": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning"
                },
                "summary": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management."
                },
                "authors": [
                    {
                        "name": "Yuxuan Wen"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Maojun Zhang"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyang Zhang"
                },
                "author": "Zhaoyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17723v2",
                "updated": "2025-07-24T08:03:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    3,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-24T16:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    36,
                    19,
                    3,
                    114,
                    0
                ],
                "title": "Statistical Runtime Verification for LLMs via Robustness Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Runtime Verification for LLMs via Robustness Estimation"
                },
                "summary": "Adversarial robustness verification is essential for ensuring the safe\ndeployment of Large Language Models (LLMs) in runtime-critical applications.\nHowever, formal verification techniques remain computationally infeasible for\nmodern LLMs due to their exponential runtime and white-box access requirements.\nThis paper presents a case study adapting and extending the RoMA statistical\nverification framework to assess its feasibility as an online runtime\nrobustness monitor for LLMs in black-box deployment settings. Our adaptation of\nRoMA analyzes confidence score distributions under semantic perturbations to\nprovide quantitative robustness assessments with statistically validated\nbounds. Our empirical validation against formal verification baselines\ndemonstrates that RoMA achieves comparable accuracy (within 1\\% deviation), and\nreduces verification times from hours to minutes. We evaluate this framework\nacross semantic, categorial, and orthographic perturbation domains. Our results\ndemonstrate RoMA's effectiveness for robustness monitoring in operational LLM\ndeployments. These findings point to RoMA as a potentially scalable alternative\nwhen formal methods are infeasible, with promising implications for runtime\nverification in LLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial robustness verification is essential for ensuring the safe\ndeployment of Large Language Models (LLMs) in runtime-critical applications.\nHowever, formal verification techniques remain computationally infeasible for\nmodern LLMs due to their exponential runtime and white-box access requirements.\nThis paper presents a case study adapting and extending the RoMA statistical\nverification framework to assess its feasibility as an online runtime\nrobustness monitor for LLMs in black-box deployment settings. Our adaptation of\nRoMA analyzes confidence score distributions under semantic perturbations to\nprovide quantitative robustness assessments with statistically validated\nbounds. Our empirical validation against formal verification baselines\ndemonstrates that RoMA achieves comparable accuracy (within 1\\% deviation), and\nreduces verification times from hours to minutes. We evaluate this framework\nacross semantic, categorial, and orthographic perturbation domains. Our results\ndemonstrate RoMA's effectiveness for robustness monitoring in operational LLM\ndeployments. These findings point to RoMA as a potentially scalable alternative\nwhen formal methods are infeasible, with promising implications for runtime\nverification in LLM-based systems."
                },
                "authors": [
                    {
                        "name": "Natan Levy"
                    },
                    {
                        "name": "Adiel Ashrov"
                    },
                    {
                        "name": "Guy Katz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Katz"
                },
                "author": "Guy Katz",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18165v1",
                "updated": "2025-07-24T08:02:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    2,
                    35,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T08:02:35Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    2,
                    35,
                    3,
                    205,
                    0
                ],
                "title": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent"
                },
                "summary": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration."
                },
                "authors": [
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Xueli Shu"
                    },
                    {
                        "name": "Liwen Fan"
                    },
                    {
                        "name": "Lin Gao"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18160v1",
                "updated": "2025-07-24T07:54:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T07:54:45Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    54,
                    45,
                    3,
                    205,
                    0
                ],
                "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer\n  Vision and Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer\n  Vision and Convolutional Neural Networks"
                },
                "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning."
                },
                "authors": [
                    {
                        "name": "Luka Šiktar"
                    },
                    {
                        "name": "Branimir Ćaran"
                    },
                    {
                        "name": "Bojan Šekoranja"
                    },
                    {
                        "name": "Marko Švaco"
                    }
                ],
                "author_detail": {
                    "name": "Marko Švaco"
                },
                "author": "Marko Švaco",
                "arxiv_comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10371v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10371v5",
                "updated": "2025-07-24T07:53:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    53,
                    24,
                    3,
                    205,
                    0
                ],
                "published": "2024-11-15T17:19:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    19,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "A Survey of Event Causality Identification: Taxonomy, Challenges,\n  Assessment, and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Event Causality Identification: Taxonomy, Challenges,\n  Assessment, and Prospects"
                },
                "summary": "Event Causality Identification (ECI) has become an essential task in Natural\nLanguage Processing (NLP), focused on automatically detecting causal\nrelationships between events within texts. This comprehensive survey\nsystematically investigates fundamental concepts and models, developing a\nsystematic taxonomy and critically evaluating diverse models. We begin by\ndefining core concepts, formalizing the ECI problem, and outlining standard\nevaluation protocols. Our classification framework divides ECI models into two\nprimary tasks: Sentence-level Event Causality Identification (SECI) and\nDocument-level Event Causality Identification (DECI). For SECI, we review\nmodels employing feature pattern-based matching, machine learning classifiers,\ndeep semantic encoding, prompt-based fine-tuning, and causal knowledge\npre-training, alongside data augmentation strategies. For DECI, we focus on\napproaches utilizing deep semantic encoding, event graph reasoning, and\nprompt-based fine-tuning. Special attention is given to recent advancements in\nmulti-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large\nLanguage Models (LLMs). We analyze the strengths, limitations, and unresolved\nchallenges associated with each approach. Extensive quantitative evaluations\nare conducted on four benchmark datasets to rigorously assess the performance\nof various ECI models. We conclude by discussing future research directions and\nhighlighting opportunities to advance the field further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Causality Identification (ECI) has become an essential task in Natural\nLanguage Processing (NLP), focused on automatically detecting causal\nrelationships between events within texts. This comprehensive survey\nsystematically investigates fundamental concepts and models, developing a\nsystematic taxonomy and critically evaluating diverse models. We begin by\ndefining core concepts, formalizing the ECI problem, and outlining standard\nevaluation protocols. Our classification framework divides ECI models into two\nprimary tasks: Sentence-level Event Causality Identification (SECI) and\nDocument-level Event Causality Identification (DECI). For SECI, we review\nmodels employing feature pattern-based matching, machine learning classifiers,\ndeep semantic encoding, prompt-based fine-tuning, and causal knowledge\npre-training, alongside data augmentation strategies. For DECI, we focus on\napproaches utilizing deep semantic encoding, event graph reasoning, and\nprompt-based fine-tuning. Special attention is given to recent advancements in\nmulti-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large\nLanguage Models (LLMs). We analyze the strengths, limitations, and unresolved\nchallenges associated with each approach. Extensive quantitative evaluations\nare conducted on four benchmark datasets to rigorously assess the performance\nof various ECI models. We conclude by discussing future research directions and\nhighlighting opportunities to advance the field further."
                },
                "authors": [
                    {
                        "name": "Qing Cheng"
                    },
                    {
                        "name": "Zefan Zeng"
                    },
                    {
                        "name": "Xingchen Hu"
                    },
                    {
                        "name": "Yuehang Si"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10371v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10371v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18153v2",
                "updated": "2025-07-25T04:04:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    4,
                    4,
                    58,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-24T07:39:07Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    39,
                    7,
                    3,
                    205,
                    0
                ],
                "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label"
                },
                "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels."
                },
                "authors": [
                    {
                        "name": "Riting Xia"
                    },
                    {
                        "name": "Rucong Wang"
                    },
                    {
                        "name": "Yulin Liu"
                    },
                    {
                        "name": "Anchen Li"
                    },
                    {
                        "name": "Xueyan Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16362v2",
                "updated": "2025-07-24T07:30:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    30,
                    7,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-22T08:54:32Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    8,
                    54,
                    32,
                    1,
                    203,
                    0
                ],
                "title": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification\n  and Recognition Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification\n  and Recognition Network"
                },
                "summary": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability."
                },
                "authors": [
                    {
                        "name": "Guangzhu Xu"
                    },
                    {
                        "name": "Pengcheng Zuo"
                    },
                    {
                        "name": "Zhi Ke"
                    },
                    {
                        "name": "Bangjun Lei"
                    }
                ],
                "author_detail": {
                    "name": "Bangjun Lei"
                },
                "author": "Bangjun Lei",
                "arxiv_comment": "28 pages, 33 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16383v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16383v4",
                "updated": "2025-07-24T07:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    7,
                    27,
                    25,
                    3,
                    205,
                    0
                ],
                "published": "2025-06-19T15:12:58Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    15,
                    12,
                    58,
                    3,
                    170,
                    0
                ],
                "title": "Large Language Models in Argument Mining: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Argument Mining: A Survey"
                },
                "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "Work draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16383v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16383v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]