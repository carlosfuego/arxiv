[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v1",
                "updated": "2024-12-19T03:11:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption.\n  In this work, our main contribution is the introduction of the PB model\ncounter PBCount2, the first exact PB model counter with support for projected\nand incremental model counting. Our counter, PBCount2, uses our Least\nOccurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to\nsupport projected model counting and a cache mechanism to enable incremental\nmodel counting. In our evaluations, PBCount2 completed at least 1.40x the\nnumber of benchmarks of competing methods for projected model counting and at\nleast 1.18x of competing methods in incremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption.\n  In this work, our main contribution is the introduction of the PB model\ncounter PBCount2, the first exact PB model counter with support for projected\nand incremental model counting. Our counter, PBCount2, uses our Least\nOccurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to\nsupport projected model counting and a cache mechanism to enable incremental\nmodel counting. In our evaluations, PBCount2 completed at least 1.40x the\nnumber of benchmarks of competing methods for projected model counting and at\nleast 1.18x of competing methods in incremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.15208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15208v1",
                "updated": "2024-12-19T18:59:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    40,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:59:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
                },
                "summary": "Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Chengyuan Qian"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Kexin Tian"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15204v1",
                "updated": "2024-12-19T18:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks"
                },
                "summary": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "25 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08760v3",
                "updated": "2024-12-19T18:59:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    10,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-11T20:25:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    20,
                    25,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "Bayesian evidence for two peaks in the sound speed in cold dense QCD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian evidence for two peaks in the sound speed in cold dense QCD"
                },
                "summary": "I show that in addition to the well-known peak inside massive neutron stars,\nthe sound speed in cold dense QCD matter likely exhibits another peak above\nneutron star densities before it asymptotes to $c_s=\\sqrt{C_s}=\\sqrt{1/3}$.\nBased on the framework reported in arxiv:2408.16738, this approach does not\nrely on any assumption about the ultra-dense matter not realized in nature.\nCurrent multimessenger observation of neutron stars favors the two-peak\nscenario with a Bayes factor $5.1_{-0.7}^{+0.9}$, where the uncertainties are\nsystematics due to models of neutron star inner cores. This evidence grows to\n$27_{-8}^{+11}$ if the $2.6M_\\odot$ component of GW190814 is a neutron star.\nThe trough in $C_s$ separating the two peaks is inferred to be below\n$0.05^{+0.04}_{-0.02}$ (at the $50\\%$ level) if $2.6M_\\odot$ neutron stars\nexist. The second peak above $1/3$ beyond baryon chemical potential\n$\\mu_B=1.6-1.9$ GeV most likely signals non-perturbative effects in cold quark\nmatter, for instance color superconductivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I show that in addition to the well-known peak inside massive neutron stars,\nthe sound speed in cold dense QCD matter likely exhibits another peak above\nneutron star densities before it asymptotes to $c_s=\\sqrt{C_s}=\\sqrt{1/3}$.\nBased on the framework reported in arxiv:2408.16738, this approach does not\nrely on any assumption about the ultra-dense matter not realized in nature.\nCurrent multimessenger observation of neutron stars favors the two-peak\nscenario with a Bayes factor $5.1_{-0.7}^{+0.9}$, where the uncertainties are\nsystematics due to models of neutron star inner cores. This evidence grows to\n$27_{-8}^{+11}$ if the $2.6M_\\odot$ component of GW190814 is a neutron star.\nThe trough in $C_s$ separating the two peaks is inferred to be below\n$0.05^{+0.04}_{-0.02}$ (at the $50\\%$ level) if $2.6M_\\odot$ neutron stars\nexist. The second peak above $1/3$ beyond baryon chemical potential\n$\\mu_B=1.6-1.9$ GeV most likely signals non-perturbative effects in cold quark\nmatter, for instance color superconductivity."
                },
                "authors": [
                    {
                        "name": "Dake Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dake Zhou"
                },
                "author": "Dake Zhou",
                "arxiv_comment": "additional textual corrections and clarifications, results unchanged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15194v1",
                "updated": "2024-12-19T18:58:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    58,
                    4,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:58:04Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    58,
                    4,
                    3,
                    354,
                    0
                ],
                "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark"
                },
                "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF."
                },
                "authors": [
                    {
                        "name": "Qihao Zhao"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Qinzheng Sun"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Qiufeng Yin"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15189v1",
                "updated": "2024-12-19T18:57:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:57:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings"
                },
                "summary": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment."
                },
                "authors": [
                    {
                        "name": "Daniel Russo"
                    },
                    {
                        "name": "Stefano Menini"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Code and data at https://github.com/drusso98/face-the-facts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v1",
                "updated": "2024-12-19T18:56:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation"
                },
                "summary": "We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15184v1",
                "updated": "2024-12-19T18:55:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    55,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:55:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    55,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning"
                },
                "summary": "The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots."
                },
                "authors": [
                    {
                        "name": "Simon Frieder"
                    },
                    {
                        "name": "Jonas Bayer"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Julius Berner"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Andrs Juhsz"
                    },
                    {
                        "name": "Fabian Ruehle"
                    },
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Timothy Gowers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Gowers"
                },
                "author": "Timothy Gowers",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15178v1",
                "updated": "2024-12-19T18:52:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    52,
                    5,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:52:05Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    52,
                    5,
                    3,
                    354,
                    0
                ],
                "title": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages"
                },
                "summary": "Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date."
                },
                "authors": [
                    {
                        "name": "Aman Chaturvedi"
                    },
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15177v1",
                "updated": "2024-12-19T18:51:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    30,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:51:30Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    30,
                    3,
                    354,
                    0
                ],
                "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying"
                },
                "summary": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided."
                },
                "authors": [
                    {
                        "name": "Federico Castagna"
                    },
                    {
                        "name": "Isabel Sassoon"
                    },
                    {
                        "name": "Simon Parsons"
                    }
                ],
                "author_detail": {
                    "name": "Simon Parsons"
                },
                "author": "Simon Parsons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15176v1",
                "updated": "2024-12-19T18:51:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:51:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "Rethinking Uncertainty Estimation in Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Uncertainty Estimation in Natural Language Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field."
                },
                "authors": [
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06289v3",
                "updated": "2024-12-19T18:47:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    47,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-09T08:24:11Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity"
                },
                "summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Jixuan Leng"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15171v1",
                "updated": "2024-12-19T18:46:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    46,
                    55,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:46:55Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    46,
                    55,
                    3,
                    354,
                    0
                ],
                "title": "SqueezeMe: Efficient Gaussian Avatars for VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeMe: Efficient Gaussian Avatars for VR"
                },
                "summary": "Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme."
                },
                "authors": [
                    {
                        "name": "Shunsuke Saito"
                    },
                    {
                        "name": "Stanislav Pidhorskyi"
                    },
                    {
                        "name": "Igor Santesteban"
                    },
                    {
                        "name": "Forrest Iandola"
                    },
                    {
                        "name": "Divam Gupta"
                    },
                    {
                        "name": "Anuj Pahuja"
                    },
                    {
                        "name": "Nemanja Bartolovic"
                    },
                    {
                        "name": "Frank Yu"
                    },
                    {
                        "name": "Emanuel Garbin"
                    },
                    {
                        "name": "Tomas Simon"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Simon"
                },
                "author": "Tomas Simon",
                "arxiv_comment": "Initial version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05199v2",
                "updated": "2024-12-19T18:46:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    46,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-07T21:51:07Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    51,
                    7,
                    3,
                    312,
                    0
                ],
                "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"
                },
                "summary": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Tung Mai"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Saayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Saayan Mitra"
                },
                "author": "Saayan Mitra",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15156v1",
                "updated": "2024-12-19T18:32:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    32,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:32:21Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    32,
                    21,
                    3,
                    354,
                    0
                ],
                "title": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM"
                },
                "summary": "Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation."
                },
                "authors": [
                    {
                        "name": "Yatai Ji"
                    },
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Shilong Zhang"
                    },
                    {
                        "name": "Shoufa Chen"
                    },
                    {
                        "name": "Chongjian GE"
                    },
                    {
                        "name": "Peize Sun"
                    },
                    {
                        "name": "Weifeng Chen"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15151v1",
                "updated": "2024-12-19T18:28:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:28:41Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "title": "Language Models as Continuous Self-Evolving Data Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Continuous Self-Evolving Data Engineers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities."
                },
                "authors": [
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15146v1",
                "updated": "2024-12-19T18:27:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    27,
                    12,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:27:12Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    27,
                    12,
                    3,
                    354,
                    0
                ],
                "title": "Cruise Control: Dynamic Model Selection for ML-Based Network Traffic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cruise Control: Dynamic Model Selection for ML-Based Network Traffic\n  Analysis"
                },
                "summary": "Modern networks increasingly rely on machine learning models for real-time\ninsights, including traffic classification, application quality of experience\ninference, and intrusion detection. However, existing approaches prioritize\nprediction accuracy without considering deployment constraints or the dynamism\nof network traffic, leading to potentially suboptimal performance. Because of\nthis, deploying ML models in real-world networks with tight performance\nconstraints remains an open challenge. In contrast with existing work that aims\nto select an optimal candidate model for each task based on offline\ninformation, we propose an online, system-driven approach to dynamically select\nthe best ML model for network traffic analysis. To this end, we present Cruise\nControl, a system that pre-trains several models for a given task with\ndifferent accuracy-cost tradeoffs and selects the most appropriate model based\non lightweight signals representing the system's current traffic processing\nability. Experimental results using two real-world traffic analysis tasks\ndemonstrate Cruise Control's effectiveness in adapting to changing network\nconditions. Our evaluation shows that Cruise Control improves median accuracy\nby 2.78% while reducing packet loss by a factor of four compared to\noffline-selected models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern networks increasingly rely on machine learning models for real-time\ninsights, including traffic classification, application quality of experience\ninference, and intrusion detection. However, existing approaches prioritize\nprediction accuracy without considering deployment constraints or the dynamism\nof network traffic, leading to potentially suboptimal performance. Because of\nthis, deploying ML models in real-world networks with tight performance\nconstraints remains an open challenge. In contrast with existing work that aims\nto select an optimal candidate model for each task based on offline\ninformation, we propose an online, system-driven approach to dynamically select\nthe best ML model for network traffic analysis. To this end, we present Cruise\nControl, a system that pre-trains several models for a given task with\ndifferent accuracy-cost tradeoffs and selects the most appropriate model based\non lightweight signals representing the system's current traffic processing\nability. Experimental results using two real-world traffic analysis tasks\ndemonstrate Cruise Control's effectiveness in adapting to changing network\nconditions. Our evaluation shows that Cruise Control improves median accuracy\nby 2.78% while reducing packet loss by a factor of four compared to\noffline-selected models."
                },
                "authors": [
                    {
                        "name": "Johann Hugon"
                    },
                    {
                        "name": "Paul Schmitt"
                    },
                    {
                        "name": "Anthony Busson"
                    },
                    {
                        "name": "Francesco Bronzino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Bronzino"
                },
                "author": "Francesco Bronzino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15128v1",
                "updated": "2024-12-19T18:08:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:08:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "Estimating Heterogeneous Treatment Effects for Spatio-Temporal Causal\n  Inference: How Economic Assistance Moderates the Effects of Airstrikes on\n  Insurgent Violence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Heterogeneous Treatment Effects for Spatio-Temporal Causal\n  Inference: How Economic Assistance Moderates the Effects of Airstrikes on\n  Insurgent Violence"
                },
                "summary": "Scholars from diverse fields now increasingly rely on high-frequency\nspatio-temporal data. Yet, causal inference with these data remains challenging\ndue to the twin threats of spatial spillover and temporal carryover effects. We\ndevelop methods to estimate heterogeneous treatment effects by allowing for\narbitrary spatial and temporal causal dependencies. We focus on common settings\nwhere the treatment and outcomes are time-varying spatial point patterns and\nwhere moderators are either spatial or spatio-temporal in nature. We define\ncausal estimands based on stochastic interventions where researchers specify\ncounterfactual distributions of treatment events. We propose the Hajek-type\nestimator of the conditional average treatment effect (CATE) as a function of\nspatio-temporal moderator variables, and establish its asymptotic normality as\nthe number of time periods increases. We then introduce a statistical test of\nno heterogeneous treatment effects. Through simulations, we evaluate the\nfinite-sample performance of the proposed CATE estimator and its inferential\nproperties. Our motivating application examines the heterogeneous effects of US\nairstrikes on insurgent violence in Iraq. Drawing on declassified\nspatio-temporal data, we examine how prior aid distributions moderate airstrike\neffects. Contrary to expectations from counterinsurgency theories, we find that\nprior aid distribution, along with greater amounts of aid per capita, is\nassociated with increased insurgent attacks following airstrikes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholars from diverse fields now increasingly rely on high-frequency\nspatio-temporal data. Yet, causal inference with these data remains challenging\ndue to the twin threats of spatial spillover and temporal carryover effects. We\ndevelop methods to estimate heterogeneous treatment effects by allowing for\narbitrary spatial and temporal causal dependencies. We focus on common settings\nwhere the treatment and outcomes are time-varying spatial point patterns and\nwhere moderators are either spatial or spatio-temporal in nature. We define\ncausal estimands based on stochastic interventions where researchers specify\ncounterfactual distributions of treatment events. We propose the Hajek-type\nestimator of the conditional average treatment effect (CATE) as a function of\nspatio-temporal moderator variables, and establish its asymptotic normality as\nthe number of time periods increases. We then introduce a statistical test of\nno heterogeneous treatment effects. Through simulations, we evaluate the\nfinite-sample performance of the proposed CATE estimator and its inferential\nproperties. Our motivating application examines the heterogeneous effects of US\nairstrikes on insurgent violence in Iraq. Drawing on declassified\nspatio-temporal data, we examine how prior aid distributions moderate airstrike\neffects. Contrary to expectations from counterinsurgency theories, we find that\nprior aid distribution, along with greater amounts of aid per capita, is\nassociated with increased insurgent attacks following airstrikes."
                },
                "authors": [
                    {
                        "name": "Lingxiao Zhou"
                    },
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Jason Lyall"
                    },
                    {
                        "name": "Georgia Papadogeorgou"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Papadogeorgou"
                },
                "author": "Georgia Papadogeorgou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15127v1",
                "updated": "2024-12-19T18:08:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    4,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:08:04Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    4,
                    3,
                    354,
                    0
                ],
                "title": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness"
                },
                "summary": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Haotian Zheng"
                    },
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yushan Sun"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Yatong Han"
                    }
                ],
                "author_detail": {
                    "name": "Yatong Han"
                },
                "author": "Yatong Han",
                "arxiv_comment": "12 pages, 6 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15119v1",
                "updated": "2024-12-19T17:59:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    59,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:59:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    59,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "Parallelized Autoregressive Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallelized Autoregressive Visual Generation"
                },
                "summary": "Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Haoyuan Guo"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project page: https://epiphqny.github.io/PAR-project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11917v3",
                "updated": "2024-12-19T17:57:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    57,
                    59,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-16T16:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "title": "Does VLM Classification Benefit from LLM Description Semantics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does VLM Classification Benefit from LLM Description Semantics?"
                },
                "summary": "Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs."
                },
                "authors": [
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Lennart Rietdorf"
                    },
                    {
                        "name": "Dmytro Kotovenko"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "arxiv_comment": "AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15115v1",
                "updated": "2024-12-19T17:56:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "title": "Qwen2.5 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5 Technical Report"
                },
                "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models."
                },
                "authors": [
                    {
                        "name": "Qwen"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "arxiv_affiliation": "additional authors not shown",
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15113v1",
                "updated": "2024-12-19T17:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture"
                },
                "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale."
                },
                "authors": [
                    {
                        "name": "Thomas F Burns"
                    },
                    {
                        "name": "Tomoki Fukai"
                    },
                    {
                        "name": "Christopher J Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J Earls"
                },
                "author": "Christopher J Earls",
                "arxiv_comment": "18 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T01, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5; I.7; J.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15101v1",
                "updated": "2024-12-19T17:48:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:48:23Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    23,
                    3,
                    354,
                    0
                ],
                "title": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability"
                },
                "summary": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Xiangsen Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15100v1",
                "updated": "2024-12-19T17:48:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    3,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:48:03Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    3,
                    3,
                    354,
                    0
                ],
                "title": "Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks"
                },
                "summary": "Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914."
                },
                "authors": [
                    {
                        "name": "Noemi Anau Montel"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "arxiv_comment": "11 pages, 5 figures. Code available on github (NoemiAM/mist) at\n  https://github.com/NoemiAM/mist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15093v1",
                "updated": "2024-12-19T17:43:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    43,
                    27,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:43:27Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    43,
                    27,
                    3,
                    354,
                    0
                ],
                "title": "Nano-ESG: Extracting Corporate Sustainability Information from News\n  Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nano-ESG: Extracting Corporate Sustainability Information from News\n  Articles"
                },
                "summary": "Determining the sustainability impact of companies is a highly complex\nsubject which has garnered more and more attention over the past few years.\nToday, investors largely rely on sustainability-ratings from established\nrating-providers in order to analyze how responsibly a company acts. However,\nthose ratings have recently been criticized for being hard to understand and\nnearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of\ncompanies lies in the rich landscape of news article data. In this paper, we\nexplore a different approach to identify key opportunities and challenges of\ncompanies in the sustainability domain. We present a novel dataset of more than\n840,000 news articles which were gathered for major German companies between\nJanuary 2023 and September 2024. By applying a mixture of Natural Language\nProcessing techniques, we first identify relevant articles, before summarizing\nthem and extracting their sustainability-related sentiment and aspect using\nLarge Language Models (LLMs). Furthermore, we conduct an evaluation of the\nobtained data and determine that the LLM-produced answers are accurate. We\nrelease both datasets at https://github.com/Bailefan/Nano-ESG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the sustainability impact of companies is a highly complex\nsubject which has garnered more and more attention over the past few years.\nToday, investors largely rely on sustainability-ratings from established\nrating-providers in order to analyze how responsibly a company acts. However,\nthose ratings have recently been criticized for being hard to understand and\nnearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of\ncompanies lies in the rich landscape of news article data. In this paper, we\nexplore a different approach to identify key opportunities and challenges of\ncompanies in the sustainability domain. We present a novel dataset of more than\n840,000 news articles which were gathered for major German companies between\nJanuary 2023 and September 2024. By applying a mixture of Natural Language\nProcessing techniques, we first identify relevant articles, before summarizing\nthem and extracting their sustainability-related sentiment and aspect using\nLarge Language Models (LLMs). Furthermore, we conduct an evaluation of the\nobtained data and determine that the LLM-produced answers are accurate. We\nrelease both datasets at https://github.com/Bailefan/Nano-ESG."
                },
                "authors": [
                    {
                        "name": "Fabian Billert"
                    },
                    {
                        "name": "Stefan Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Conrad"
                },
                "author": "Stefan Conrad",
                "arxiv_comment": "To be published at ECIR 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15076v1",
                "updated": "2024-12-19T17:26:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    26,
                    2,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:26:02Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    26,
                    2,
                    3,
                    354,
                    0
                ],
                "title": "Digital N-of-1 Trials and their Application in Experimental Physiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital N-of-1 Trials and their Application in Experimental Physiology"
                },
                "summary": "Traditionally, studies in experimental physiology have been conducted in\nsmall groups of human participants, animal models or cell lines. Important\nchallenges include achieving sufficient statistical power in statistical\nhypothesis tests of small sample sizes and identifying optimal study designs.\nHere, we introduce N-of-1 trials as an innovative study design which can have\nhigh relevance to innovate and improve studies in experimental physiology.\nN-of-1 trials are multi-crossover trials in single participants that allow\nvalid statistical inference on the individual level. Also, series of N-of-1\ntrials conducted on multiple study participants can be aggregated for\npopulation-level inference and provide a more efficient study design compared\nto standard randomized controlled trials. In this manuscript, we first\nintroduce key components and design features of N-of-1 trials. Then we lay out\nhow N-of-1 trials can be analyzed statistically and give different examples of\ntheir applicability in experimental physiological studies. In summary, we\nprovide here an overview of all main components for designing N-of-1 trials,\ngive direct examples in experimental physiology and practical recommendations\non their proper use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, studies in experimental physiology have been conducted in\nsmall groups of human participants, animal models or cell lines. Important\nchallenges include achieving sufficient statistical power in statistical\nhypothesis tests of small sample sizes and identifying optimal study designs.\nHere, we introduce N-of-1 trials as an innovative study design which can have\nhigh relevance to innovate and improve studies in experimental physiology.\nN-of-1 trials are multi-crossover trials in single participants that allow\nvalid statistical inference on the individual level. Also, series of N-of-1\ntrials conducted on multiple study participants can be aggregated for\npopulation-level inference and provide a more efficient study design compared\nto standard randomized controlled trials. In this manuscript, we first\nintroduce key components and design features of N-of-1 trials. Then we lay out\nhow N-of-1 trials can be analyzed statistically and give different examples of\ntheir applicability in experimental physiological studies. In summary, we\nprovide here an overview of all main components for designing N-of-1 trials,\ngive direct examples in experimental physiology and practical recommendations\non their proper use."
                },
                "authors": [
                    {
                        "name": "Stefan Konigorski"
                    },
                    {
                        "name": "Christopher H Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Christopher H Schmid"
                },
                "author": "Christopher H Schmid",
                "arxiv_comment": "Accepted in Experimental Physiology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15072v1",
                "updated": "2024-12-19T17:22:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    22,
                    35,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:22:35Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    22,
                    35,
                    3,
                    354,
                    0
                ],
                "title": "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social\n  Media via Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social\n  Media via Chatbots"
                },
                "summary": "Social media platforms have become the hubs for various user interactions\ncovering a wide range of needs, including technical support and services\nrelated to brands, products, or user accounts. Unfortunately, there has been a\nrecent surge in scammers impersonating official services and providing fake\ntechnical support to users through these platforms. In this study, we focus on\nscammers engaging in such fake technical support to target users who are having\nproblems recovering their accounts. More specifically, we focus on users\nencountering access problems with social media profiles (e.g., on platforms\nsuch as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main\ncontribution of our work is the development of an automated system that\ninteracts with scammers via a chatbot that mimics different personas. By\ninitiating decoy interactions (e.g., through deceptive tweets), we have enticed\nscammers to interact with our system so that we can analyze their modus\noperandi. Our results show that scammers employ many social media profiles\nasking users to contact them via a few communication channels. Using a large\nlanguage model (LLM), our chatbot had conversations with 450 scammers and\nprovided valuable insights into their tactics and, most importantly, their\npayment profiles. This automated approach highlights how scammers use a variety\nof strategies, including role-playing, to trick victims into disclosing\npersonal or financial information. With this study, we lay the foundation for\nusing automated chat-based interactions with scammers to detect and study\nfraudulent activities at scale in an automated way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms have become the hubs for various user interactions\ncovering a wide range of needs, including technical support and services\nrelated to brands, products, or user accounts. Unfortunately, there has been a\nrecent surge in scammers impersonating official services and providing fake\ntechnical support to users through these platforms. In this study, we focus on\nscammers engaging in such fake technical support to target users who are having\nproblems recovering their accounts. More specifically, we focus on users\nencountering access problems with social media profiles (e.g., on platforms\nsuch as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main\ncontribution of our work is the development of an automated system that\ninteracts with scammers via a chatbot that mimics different personas. By\ninitiating decoy interactions (e.g., through deceptive tweets), we have enticed\nscammers to interact with our system so that we can analyze their modus\noperandi. Our results show that scammers employ many social media profiles\nasking users to contact them via a few communication channels. Using a large\nlanguage model (LLM), our chatbot had conversations with 450 scammers and\nprovided valuable insights into their tactics and, most importantly, their\npayment profiles. This automated approach highlights how scammers use a variety\nof strategies, including role-playing, to trick victims into disclosing\npersonal or financial information. With this study, we lay the foundation for\nusing automated chat-based interactions with scammers to detect and study\nfraudulent activities at scale in an automated way."
                },
                "authors": [
                    {
                        "name": "Bhupendra Acharya"
                    },
                    {
                        "name": "Dominik Sautter"
                    },
                    {
                        "name": "Muhammad Saad"
                    },
                    {
                        "name": "Thorsten Holz"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Holz"
                },
                "author": "Thorsten Holz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.03552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.03552v3",
                "updated": "2024-12-19T17:16:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    16,
                    20,
                    3,
                    354,
                    0
                ],
                "published": "2023-05-05T14:03:39Z",
                "published_parsed": [
                    2023,
                    5,
                    5,
                    14,
                    3,
                    39,
                    4,
                    125,
                    0
                ],
                "title": "Designing Proposal Distributions for Particle Filters using Integrated\n  Nested Laplace Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Proposal Distributions for Particle Filters using Integrated\n  Nested Laplace Approximation"
                },
                "summary": "State-space models are used to describe and analyse dynamical systems. They\nare ubiquitously used in many scientific fields such as signal processing,\nfinance and ecology to name a few. Particle filters are popular inferential\nmethods used for state-space methods. Integrated Nested Laplace Approximation\n(INLA), an approximate Bayesian inference method, can also be used for this\nkind of models in case the transition distribution is Gaussian. We present a\nway to use this framework in order to approximate the particle filter's\nproposal distribution that incorporates information about the observations,\nparameters and the previous latent variables. Further, we demonstrate the\nperformance of this proposal on data simulated from a Poisson state-space model\nused for count data. We also show how INLA can be used to estimate the\nparameters of certain state-space models (a task that is often challenging)\nthat would be used for Sequential Monte Carlo algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space models are used to describe and analyse dynamical systems. They\nare ubiquitously used in many scientific fields such as signal processing,\nfinance and ecology to name a few. Particle filters are popular inferential\nmethods used for state-space methods. Integrated Nested Laplace Approximation\n(INLA), an approximate Bayesian inference method, can also be used for this\nkind of models in case the transition distribution is Gaussian. We present a\nway to use this framework in order to approximate the particle filter's\nproposal distribution that incorporates information about the observations,\nparameters and the previous latent variables. Further, we demonstrate the\nperformance of this proposal on data simulated from a Poisson state-space model\nused for count data. We also show how INLA can be used to estimate the\nparameters of certain state-space models (a task that is often challenging)\nthat would be used for Sequential Monte Carlo algorithms."
                },
                "authors": [
                    {
                        "name": "Alaa Amri"
                    }
                ],
                "author_detail": {
                    "name": "Alaa Amri"
                },
                "author": "Alaa Amri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.03552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.03552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15060v1",
                "updated": "2024-12-19T17:08:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    8,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:08:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    8,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "ConfliBERT: A Language Model for Political Conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfliBERT: A Language Model for Political Conflict"
                },
                "summary": "Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD)."
                },
                "authors": [
                    {
                        "name": "Patrick T. Brandt"
                    },
                    {
                        "name": "Sultan Alsarra"
                    },
                    {
                        "name": "Vito J. D`Orazio"
                    },
                    {
                        "name": "Dagmar Heintze"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Shreyas Meher"
                    },
                    {
                        "name": "Javier Osorio"
                    },
                    {
                        "name": "Marcus Sianan"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Sianan"
                },
                "author": "Marcus Sianan",
                "arxiv_comment": "30 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15050v1",
                "updated": "2024-12-19T16:57:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    57,
                    45,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:57:45Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    57,
                    45,
                    3,
                    354,
                    0
                ],
                "title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion"
                },
                "summary": "Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area."
                },
                "authors": [
                    {
                        "name": "Zhifei Chen"
                    },
                    {
                        "name": "Tianshuo Xu"
                    },
                    {
                        "name": "Wenhang Ge"
                    },
                    {
                        "name": "Leyi Wu"
                    },
                    {
                        "name": "Dongyu Yan"
                    },
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Luozhou Wang"
                    },
                    {
                        "name": "Lu Zeng"
                    },
                    {
                        "name": "Shunsi Zhang"
                    },
                    {
                        "name": "Yingcong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingcong Chen"
                },
                "author": "Yingcong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15049v1",
                "updated": "2024-12-19T16:57:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    57,
                    43,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:57:43Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    57,
                    43,
                    3,
                    354,
                    0
                ],
                "title": "A linear regression model for quantile function data applied to paired\n  pulmonary 3d CT scans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A linear regression model for quantile function data applied to paired\n  pulmonary 3d CT scans"
                },
                "summary": "This paper introduces a new objective measure for assessing treatment\nresponse in asthmatic patients using computed tomography (CT) imaging data. For\neach patient, CT scans were obtained before and after one year of monoclonal\nantibody treatment. Following image segmentation, the Hounsfield unit (HU)\nvalues of the voxels were encoded through quantile functions. It is\nhypothesized that patients with improved conditions after treatment will\nexhibit better expiration, reflected in higher HU values and an upward shift in\nthe quantile curve. To objectively measure treatment response, a novel linear\nregression model on quantile functions is developed, drawing inspiration from\nVerde and Irpino (2010). Unlike their framework, the proposed model is\nparametric and incorporates distributional assumptions on the errors, enabling\nstatistical inference. The model allows for the explicit calculation of\nregression coefficient estimators and confidence intervals, similar to\nconventional linear regression. The corresponding data and R code are available\non GitHub to facilitate the reproducibility of the analyses presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new objective measure for assessing treatment\nresponse in asthmatic patients using computed tomography (CT) imaging data. For\neach patient, CT scans were obtained before and after one year of monoclonal\nantibody treatment. Following image segmentation, the Hounsfield unit (HU)\nvalues of the voxels were encoded through quantile functions. It is\nhypothesized that patients with improved conditions after treatment will\nexhibit better expiration, reflected in higher HU values and an upward shift in\nthe quantile curve. To objectively measure treatment response, a novel linear\nregression model on quantile functions is developed, drawing inspiration from\nVerde and Irpino (2010). Unlike their framework, the proposed model is\nparametric and incorporates distributional assumptions on the errors, enabling\nstatistical inference. The model allows for the explicit calculation of\nregression coefficient estimators and confidence intervals, similar to\nconventional linear regression. The corresponding data and R code are available\non GitHub to facilitate the reproducibility of the analyses presented."
                },
                "authors": [
                    {
                        "name": "Marie-Flicia Bclin"
                    },
                    {
                        "name": "Pierre Lafaye de Micheaux"
                    },
                    {
                        "name": "Nicolas Molinari"
                    },
                    {
                        "name": "Frdric Ouimet"
                    }
                ],
                "author_detail": {
                    "name": "Frdric Ouimet"
                },
                "author": "Frdric Ouimet",
                "arxiv_comment": "36 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E15, 62F03, 62J05, 62P10, 62R10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15042v1",
                "updated": "2024-12-19T16:51:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    51,
                    29,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    51,
                    29,
                    3,
                    354,
                    0
                ],
                "title": "Compiling C to Safe Rust, Formalized",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compiling C to Safe Rust, Formalized"
                },
                "summary": "The popularity of the Rust language continues to explode; yet, many critical\ncodebases remain authored in C, and cannot be realistically rewritten by hand.\nAutomatically translating C to Rust is thus an appealing course of action.\nSeveral works have gone down this path, handling an ever-increasing subset of C\nthrough a variety of Rust features, such as unsafe. While the prospect of\nautomation is appealing, producing code that relies on unsafe negates the\nmemory safety guarantees offered by Rust, and therefore the main advantages of\nporting existing codebases to memory-safe languages.\n  We instead explore a different path, and explore what it would take to\ntranslate C to safe Rust; that is, to produce code that is trivially memory\nsafe, because it abides by Rust's type system without caveats. Our work sports\nseveral original contributions: a type-directed translation from (a subset of)\nC to safe Rust; a novel static analysis based on \"split trees\" that allows\nexpressing C's pointer arithmetic using Rust's slices and splitting operations;\nan analysis that infers exactly which borrows need to be mutable; and a\ncompilation strategy for C's struct types that is compatible with Rust's\ndistinction between non-owned and owned allocations.\n  We apply our methodology to existing formally verified C codebases: the HACL*\ncryptographic library, and binary parsers and serializers from EverParse, and\nshow that the subset of C we support is sufficient to translate both\napplications to safe Rust. Our evaluation shows that for the few places that do\nviolate Rust's aliasing discipline, automated, surgical rewrites suffice; and\nthat the few strategic copies we insert have a negligible performance impact.\nOf particular note, the application of our approach to HACL* results in a\n80,000 line verified cryptographic library, written in pure Rust, that\nimplements all modern algorithms - the first of its kind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of the Rust language continues to explode; yet, many critical\ncodebases remain authored in C, and cannot be realistically rewritten by hand.\nAutomatically translating C to Rust is thus an appealing course of action.\nSeveral works have gone down this path, handling an ever-increasing subset of C\nthrough a variety of Rust features, such as unsafe. While the prospect of\nautomation is appealing, producing code that relies on unsafe negates the\nmemory safety guarantees offered by Rust, and therefore the main advantages of\nporting existing codebases to memory-safe languages.\n  We instead explore a different path, and explore what it would take to\ntranslate C to safe Rust; that is, to produce code that is trivially memory\nsafe, because it abides by Rust's type system without caveats. Our work sports\nseveral original contributions: a type-directed translation from (a subset of)\nC to safe Rust; a novel static analysis based on \"split trees\" that allows\nexpressing C's pointer arithmetic using Rust's slices and splitting operations;\nan analysis that infers exactly which borrows need to be mutable; and a\ncompilation strategy for C's struct types that is compatible with Rust's\ndistinction between non-owned and owned allocations.\n  We apply our methodology to existing formally verified C codebases: the HACL*\ncryptographic library, and binary parsers and serializers from EverParse, and\nshow that the subset of C we support is sufficient to translate both\napplications to safe Rust. Our evaluation shows that for the few places that do\nviolate Rust's aliasing discipline, automated, surgical rewrites suffice; and\nthat the few strategic copies we insert have a negligible performance impact.\nOf particular note, the application of our approach to HACL* results in a\n80,000 line verified cryptographic library, written in pure Rust, that\nimplements all modern algorithms - the first of its kind."
                },
                "authors": [
                    {
                        "name": "Aymeric Fromherz"
                    },
                    {
                        "name": "Jonathan Protzenko"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Protzenko"
                },
                "author": "Jonathan Protzenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v1",
                "updated": "2024-12-19T16:46:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14742v2",
                "updated": "2024-12-19T16:37:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    37,
                    38,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-20T21:13:39Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    21,
                    13,
                    39,
                    3,
                    172,
                    0
                ],
                "title": "Latent Variable Sequence Identification for Cognitive Models with Neural\n  Network Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Variable Sequence Identification for Cognitive Models with Neural\n  Network Estimators"
                },
                "summary": "Extracting time-varying latent variables from computational cognitive models\nis a key step in model-based neural analysis, which aims to understand the\nneural correlates of cognitive processes. However, existing methods only allow\nresearchers to infer latent variables that explain subjects' behavior in a\nrelatively small class of cognitive models. For example, a broad class of\nrelevant cognitive models with analytically intractable likelihood is currently\nout of reach from standard techniques, based on Maximum a Posteriori parameter\nestimation. Here, we present an approach that extends neural Bayes estimation\nto learn a direct mapping between experimental data and the targeted latent\nvariable space using recurrent neural networks and simulated datasets. We show\nthat our approach achieves competitive performance in inferring latent variable\nsequences in both tractable and intractable models. Furthermore, the approach\nis generalizable across different computational models and is adaptable for\nboth continuous and discrete latent spaces. We then demonstrate its\napplicability in real world datasets. Our work underscores that combining\nrecurrent neural networks and simulation-based inference to identify latent\nvariable sequences can enable researchers to access a wider class of cognitive\nmodels for model-based neural analyses, and thus test a broader set of\ntheories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting time-varying latent variables from computational cognitive models\nis a key step in model-based neural analysis, which aims to understand the\nneural correlates of cognitive processes. However, existing methods only allow\nresearchers to infer latent variables that explain subjects' behavior in a\nrelatively small class of cognitive models. For example, a broad class of\nrelevant cognitive models with analytically intractable likelihood is currently\nout of reach from standard techniques, based on Maximum a Posteriori parameter\nestimation. Here, we present an approach that extends neural Bayes estimation\nto learn a direct mapping between experimental data and the targeted latent\nvariable space using recurrent neural networks and simulated datasets. We show\nthat our approach achieves competitive performance in inferring latent variable\nsequences in both tractable and intractable models. Furthermore, the approach\nis generalizable across different computational models and is adaptable for\nboth continuous and discrete latent spaces. We then demonstrate its\napplicability in real world datasets. Our work underscores that combining\nrecurrent neural networks and simulation-based inference to identify latent\nvariable sequences can enable researchers to access a wider class of cognitive\nmodels for model-based neural analyses, and thus test a broader set of\ntheories."
                },
                "authors": [
                    {
                        "name": "Ti-Fen Pan"
                    },
                    {
                        "name": "Jing-Jing Li"
                    },
                    {
                        "name": "Bill Thompson"
                    },
                    {
                        "name": "Anne Collins"
                    }
                ],
                "author_detail": {
                    "name": "Anne Collins"
                },
                "author": "Anne Collins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14512v3",
                "updated": "2024-12-19T16:37:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    37,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-25T04:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    32,
                    45,
                    6,
                    238,
                    0
                ],
                "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings"
                },
                "summary": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
                },
                "authors": [
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Fengzhi Li"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2105.00879v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2105.00879v5",
                "updated": "2024-12-19T16:35:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    35,
                    33,
                    3,
                    354,
                    0
                ],
                "published": "2021-05-03T14:01:06Z",
                "published_parsed": [
                    2021,
                    5,
                    3,
                    14,
                    1,
                    6,
                    0,
                    123,
                    0
                ],
                "title": "Identification and Estimation of Average Causal Effects in Fixed Effects\n  Logit Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Estimation of Average Causal Effects in Fixed Effects\n  Logit Models"
                },
                "summary": "This paper studies identification and estimation of average causal effects,\nsuch as average marginal or treatment effects, in fixed effects logit models\nwith short panels. Relating the identified set of these effects to an extremal\nmoment problem, we first show how to obtain sharp bounds on such effects\nsimply, without any optimization. We also consider even simpler outer bounds,\nwhich, contrary to the sharp bounds, do not require any first-step\nnonparametric estimators. We build confidence intervals based on these two\napproaches and show their asymptotic validity. Monte Carlo simulations suggest\nthat both approaches work well in practice, the second being typically\ncompetitive in terms of interval length. Finally, we show that our method is\nalso useful to measure treatment effect heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies identification and estimation of average causal effects,\nsuch as average marginal or treatment effects, in fixed effects logit models\nwith short panels. Relating the identified set of these effects to an extremal\nmoment problem, we first show how to obtain sharp bounds on such effects\nsimply, without any optimization. We also consider even simpler outer bounds,\nwhich, contrary to the sharp bounds, do not require any first-step\nnonparametric estimators. We build confidence intervals based on these two\napproaches and show their asymptotic validity. Monte Carlo simulations suggest\nthat both approaches work well in practice, the second being typically\ncompetitive in terms of interval length. Finally, we show that our method is\nalso useful to measure treatment effect heterogeneity."
                },
                "authors": [
                    {
                        "name": "Laurent Davezies"
                    },
                    {
                        "name": "Xavier D'Haultfuille"
                    },
                    {
                        "name": "Louise Laage"
                    }
                ],
                "author_detail": {
                    "name": "Louise Laage"
                },
                "author": "Louise Laage",
                "arxiv_comment": "93 pages (online appendix starting at p.46). Major rewriting compared\n  to v3. In particular, addition of a literature review, study of general\n  parameters (not only the AME) in the identification, estimation and inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2105.00879v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2105.00879v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15009v1",
                "updated": "2024-12-19T16:22:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    22,
                    33,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:22:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    22,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Projection-based preprocessing for electrical impedance tomography to\n  reduce the effect of electrode contacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Projection-based preprocessing for electrical impedance tomography to\n  reduce the effect of electrode contacts"
                },
                "summary": "This work introduces a method for preprocessing measurements of electrical\nimpedance tomography to considerably reduce the effect uncertainties in the\nelectrode contacts have on the reconstruction quality, without a need to\nexplicitly estimate the contacts. The idea is to compute the Jacobian matrix of\nthe forward map with respect to the contact strengths and project the electrode\nmeasurements and the forward map onto the orthogonal complement of the range of\nthis Jacobian. Using the smoothened complete electrode model as the forward\nmodel, it is demonstrated that inverting the resulting projected equation with\nrespect to only the internal conductivity of the examined body results in good\nquality reconstructions both when resorting to a single step linearization with\na smoothness prior and when combining lagged diffusivity iteration with total\nvariation regularization. The quality of the reconstructions is further\nimproved if the range of the employed projection is also orthogonal to that of\nthe Jacobian with respect to the electrode positions. These results hold even\nif the projections are formed at internal and contact conductivities that\nsignificantly differ from the true ones; it is numerically demonstrated that\nthe orthogonal complement of the range of the contact Jacobian is almost\nindependent of the conductivity parameters at which it is evaluated. In\nparticular, our observations introduce a numerical technique for inferring\nwhether a change in the electrode measurements is caused by a change in the\ninternal conductivity or alterations in the electrode contacts, which has\npotential applications, e.g., in bedside monitoring of stroke patients. The\nideas are tested both on simulated data and on real-world water tank\nmeasurements with adjustable contact resistances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a method for preprocessing measurements of electrical\nimpedance tomography to considerably reduce the effect uncertainties in the\nelectrode contacts have on the reconstruction quality, without a need to\nexplicitly estimate the contacts. The idea is to compute the Jacobian matrix of\nthe forward map with respect to the contact strengths and project the electrode\nmeasurements and the forward map onto the orthogonal complement of the range of\nthis Jacobian. Using the smoothened complete electrode model as the forward\nmodel, it is demonstrated that inverting the resulting projected equation with\nrespect to only the internal conductivity of the examined body results in good\nquality reconstructions both when resorting to a single step linearization with\na smoothness prior and when combining lagged diffusivity iteration with total\nvariation regularization. The quality of the reconstructions is further\nimproved if the range of the employed projection is also orthogonal to that of\nthe Jacobian with respect to the electrode positions. These results hold even\nif the projections are formed at internal and contact conductivities that\nsignificantly differ from the true ones; it is numerically demonstrated that\nthe orthogonal complement of the range of the contact Jacobian is almost\nindependent of the conductivity parameters at which it is evaluated. In\nparticular, our observations introduce a numerical technique for inferring\nwhether a change in the electrode measurements is caused by a change in the\ninternal conductivity or alterations in the electrode contacts, which has\npotential applications, e.g., in bedside monitoring of stroke patients. The\nideas are tested both on simulated data and on real-world water tank\nmeasurements with adjustable contact resistances."
                },
                "authors": [
                    {
                        "name": "Altti Jskelinen"
                    },
                    {
                        "name": "Jussi Toivanen"
                    },
                    {
                        "name": "Asko Hnninen"
                    },
                    {
                        "name": "Ville Kolehmainen"
                    },
                    {
                        "name": "Nuutti Hyvnen"
                    }
                ],
                "author_detail": {
                    "name": "Nuutti Hyvnen"
                },
                "author": "Nuutti Hyvnen",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N21, 35R30, 35J25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v4",
                "updated": "2024-12-19T16:22:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    22,
                    7,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "AAAI 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10912v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10912v2",
                "updated": "2024-12-19T16:20:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    49,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-16T23:29:32Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    23,
                    29,
                    32,
                    5,
                    321,
                    0
                ],
                "title": "SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment"
                },
                "summary": "When different groups' values differ, one approach to model alignment is to\nsteer models at inference time towards each group's preferences. However,\ntechniques like in-context learning only consider similarity when drawing\nfew-shot examples and not cross-group differences in values. We propose SPICA,\na framework that accounts for group-level differences during in-context example\nretrieval. SPICA introduces three designs: scenario banks, group-informed\nretrieval metrics, and in-context alignment prompts. From an evaluation of\nSPICA on an alignment task collecting inputs from four demographic groups ($n =\n544$), our metrics retrieve in-context examples that more closely match\nobserved preferences, with the best prompt configuration using multiple\ncontrastive responses to demonstrate examples. In an end-to-end evaluation ($n\n= 120$), we observe that SPICA is higher rated than similarity-based retrieval,\nwith groups seeing up to a +0.16 point improvement on a 5 point scale.\nAdditionally, gains from SPICA were more uniform, with all groups benefiting\nfrom alignment rather than only some. Finally, we find that while a\ngroup-agnostic approach can align to aggregated values, it is not most suited\nfor divergent groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When different groups' values differ, one approach to model alignment is to\nsteer models at inference time towards each group's preferences. However,\ntechniques like in-context learning only consider similarity when drawing\nfew-shot examples and not cross-group differences in values. We propose SPICA,\na framework that accounts for group-level differences during in-context example\nretrieval. SPICA introduces three designs: scenario banks, group-informed\nretrieval metrics, and in-context alignment prompts. From an evaluation of\nSPICA on an alignment task collecting inputs from four demographic groups ($n =\n544$), our metrics retrieve in-context examples that more closely match\nobserved preferences, with the best prompt configuration using multiple\ncontrastive responses to demonstrate examples. In an end-to-end evaluation ($n\n= 120$), we observe that SPICA is higher rated than similarity-based retrieval,\nwith groups seeing up to a +0.16 point improvement on a 5 point scale.\nAdditionally, gains from SPICA were more uniform, with all groups benefiting\nfrom alignment rather than only some. Finally, we find that while a\ngroup-agnostic approach can align to aggregated values, it is not most suited\nfor divergent groups."
                },
                "authors": [
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10912v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10912v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15004v1",
                "updated": "2024-12-19T16:20:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:20:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Large Language Models and Code Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Code Security: A Systematic Literature Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks."
                },
                "authors": [
                    {
                        "name": "Enna Basic"
                    },
                    {
                        "name": "Alberto Giaretta"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Giaretta"
                },
                "author": "Alberto Giaretta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08406v2",
                "updated": "2024-12-19T16:09:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    9,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-09-12T21:39:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    39,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tagging with Large Language Model based Multi-Agent System"
                },
                "summary": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Ethan Chang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14995v1",
                "updated": "2024-12-19T16:07:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    7,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:07:00Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    7,
                    0,
                    3,
                    354,
                    0
                ],
                "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven\n  Harmony Search and Genetic Algorithm Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven\n  Harmony Search and Genetic Algorithm Using LLMs"
                },
                "summary": "Automatic Heuristic Design (AHD) is an active research area due to its\nutility in solving complex search and NP-hard combinatorial optimization\nproblems in the real world. The recent advancements in Large Language Models\n(LLMs) introduce new possibilities by coupling LLMs with evolutionary\ncomputation to automatically generate heuristics, known as LLM-based\nEvolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained\ngreat performance on various tasks, there is still a gap in understanding the\nproperties of heuristic search spaces and achieving a balance between\nexploration and exploitation, which is a critical factor in large heuristic\nsearch spaces. In this study, we address this gap by proposing two diversity\nmeasurement metrics and perform an analysis on previous LLM-EPS approaches,\nincluding FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal\nthat while EoH demonstrates higher diversity than FunSearch and ReEvo, its\nobjective score is unstable. Conversely, ReEvo's reflection mechanism yields\ngood objective scores but fails to optimize diversity effectively. With this\nfinding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that\nmaintains a balance between diversity and convergence with a harmony search\nalgorithm. Through experimentation, we find that HSEvo achieved high diversity\nindices and good objective scores while remaining cost-effective. These results\nunderscore the importance of balancing exploration and exploitation and\nunderstanding heuristic search spaces in designing frameworks in LLM-EPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Heuristic Design (AHD) is an active research area due to its\nutility in solving complex search and NP-hard combinatorial optimization\nproblems in the real world. The recent advancements in Large Language Models\n(LLMs) introduce new possibilities by coupling LLMs with evolutionary\ncomputation to automatically generate heuristics, known as LLM-based\nEvolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained\ngreat performance on various tasks, there is still a gap in understanding the\nproperties of heuristic search spaces and achieving a balance between\nexploration and exploitation, which is a critical factor in large heuristic\nsearch spaces. In this study, we address this gap by proposing two diversity\nmeasurement metrics and perform an analysis on previous LLM-EPS approaches,\nincluding FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal\nthat while EoH demonstrates higher diversity than FunSearch and ReEvo, its\nobjective score is unstable. Conversely, ReEvo's reflection mechanism yields\ngood objective scores but fails to optimize diversity effectively. With this\nfinding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that\nmaintains a balance between diversity and convergence with a harmony search\nalgorithm. Through experimentation, we find that HSEvo achieved high diversity\nindices and good objective scores while remaining cost-effective. These results\nunderscore the importance of balancing exploration and exploitation and\nunderstanding heuristic search spaces in designing frameworks in LLM-EPS."
                },
                "authors": [
                    {
                        "name": "Pham Vu Tuan Dat"
                    },
                    {
                        "name": "Long Doan"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "18 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14991v1",
                "updated": "2024-12-19T16:04:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    4,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:04:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    4,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "Neutrino electromagnetic properties and sterile dipole portal in light\n  of the first solar CE$$NS data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutrino electromagnetic properties and sterile dipole portal in light\n  of the first solar CE$$NS data"
                },
                "summary": "Despite being neutral particles, neutrinos can acquire non-zero\nelectromagnetic properties from radiative corrections that can be induced by\nthe presence of new physics. Electromagnetic neutrino processes induce spectral\ndistortions in neutrino scattering data, which are especially visible at\nexperiments characterized by low recoil thresholds. We investigate how neutrino\nelectromagnetic properties confront the recent indication of coherent elastic\nneutrino-nucleus scattering (CE$\\nu$NS) from $^8$B solar neutrinos in dark\nmatter direct detection experiments. We focus on three possibilities: neutrino\nmagnetic moments, neutrino electric charges, and the active-sterile transition\nmagnetic moment portal. We analyze recent XENONnT and PandaX-4T data and infer\nthe first \\cevns-based constraints on electromagnetic properties using solar\n$^8$B neutrinos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being neutral particles, neutrinos can acquire non-zero\nelectromagnetic properties from radiative corrections that can be induced by\nthe presence of new physics. Electromagnetic neutrino processes induce spectral\ndistortions in neutrino scattering data, which are especially visible at\nexperiments characterized by low recoil thresholds. We investigate how neutrino\nelectromagnetic properties confront the recent indication of coherent elastic\nneutrino-nucleus scattering (CE$\\nu$NS) from $^8$B solar neutrinos in dark\nmatter direct detection experiments. We focus on three possibilities: neutrino\nmagnetic moments, neutrino electric charges, and the active-sterile transition\nmagnetic moment portal. We analyze recent XENONnT and PandaX-4T data and infer\nthe first \\cevns-based constraints on electromagnetic properties using solar\n$^8$B neutrinos."
                },
                "authors": [
                    {
                        "name": "Valentina De Romeri"
                    },
                    {
                        "name": "Dimitrios K. Papoulias"
                    },
                    {
                        "name": "Gonzalo Sanchez Garcia"
                    },
                    {
                        "name": "Christoph A. Ternes"
                    },
                    {
                        "name": "Mariam Trtola"
                    }
                ],
                "author_detail": {
                    "name": "Mariam Trtola"
                },
                "author": "Mariam Trtola",
                "arxiv_comment": "v1: 16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14989v1",
                "updated": "2024-12-19T16:00:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    0,
                    43,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:00:43Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    0,
                    43,
                    3,
                    354,
                    0
                ],
                "title": "RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots\n  using Foundation Models for Perception and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots\n  using Foundation Models for Perception and Planning"
                },
                "summary": "We present the approaches and contributions of the winning team NimbRo@Home\nat the RoboCup@Home 2024 competition in the Open Platform League held in\nEindhoven, NL. Further, we describe our hardware setup and give an overview of\nthe results for the task stages and the final demonstration. For this year's\ncompetition, we put a special emphasis on open-vocabulary object segmentation\nand grasping approaches that overcome the labeling overhead of supervised\nvision approaches, commonly used in RoboCup@Home. We successfully demonstrated\nthat we can segment and grasp non-labeled objects by text descriptions.\nFurther, we extensively employed LLMs for natural language understanding and\ntask planning. Throughout the competition, our approaches showed robustness and\ngeneralization capabilities. A video of our performance can be found online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the approaches and contributions of the winning team NimbRo@Home\nat the RoboCup@Home 2024 competition in the Open Platform League held in\nEindhoven, NL. Further, we describe our hardware setup and give an overview of\nthe results for the task stages and the final demonstration. For this year's\ncompetition, we put a special emphasis on open-vocabulary object segmentation\nand grasping approaches that overcome the labeling overhead of supervised\nvision approaches, commonly used in RoboCup@Home. We successfully demonstrated\nthat we can segment and grasp non-labeled objects by text descriptions.\nFurther, we extensively employed LLMs for natural language understanding and\ntask planning. Throughout the competition, our approaches showed robustness and\ngeneralization capabilities. A video of our performance can be found online."
                },
                "authors": [
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Jan Nogga"
                    },
                    {
                        "name": "Bastian Ptzold"
                    },
                    {
                        "name": "Evgenii Kruzhkov"
                    },
                    {
                        "name": "Simon Bultmann"
                    },
                    {
                        "name": "Michael Schreiber"
                    },
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bertan Karacora"
                    },
                    {
                        "name": "Juhui Park"
                    },
                    {
                        "name": "Alena Savinykh"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "12 pages, 8 figures, RoboCup 2024 Champion Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14986v1",
                "updated": "2024-12-19T15:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    58,
                    53,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:58:53Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    58,
                    53,
                    3,
                    354,
                    0
                ],
                "title": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts"
                },
                "summary": "Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference."
                },
                "authors": [
                    {
                        "name": "Ioana Buhnila"
                    },
                    {
                        "name": "Georgeta Cislaru"
                    },
                    {
                        "name": "Amalia Todirascu"
                    }
                ],
                "author_detail": {
                    "name": "Amalia Todirascu"
                },
                "author": "Amalia Todirascu",
                "arxiv_comment": "Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,\n  Cognitive Science, and NLP) co-located with COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14978v1",
                "updated": "2024-12-19T15:53:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    53,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:53:21Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    53,
                    21,
                    3,
                    354,
                    0
                ],
                "title": "Spectrum-based Modality Representation Fusion Graph Convolutional\n  Network for Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum-based Modality Representation Fusion Graph Convolutional\n  Network for Multimodal Recommendation"
                },
                "summary": "Incorporating multi-modal features as side information has recently become a\ntrend in recommender systems. To elucidate user-item preferences, recent\nstudies focus on fusing modalities via concatenation, element-wise sum, or\nattention mechanisms. Despite having notable success, existing approaches do\nnot account for the modality-specific noise encapsulated within each modality.\nAs a result, direct fusion of modalities will lead to the amplification of\ncross-modality noise. Moreover, the variation of noise that is unique within\neach modality results in noise alleviation and fusion being more challenging.\nIn this work, we propose a new Spectrum-based Modality Representation (SMORE)\nfusion graph recommender that aims to capture both uni-modal and fusion\npreferences while simultaneously suppressing modality noise. Specifically,\nSMORE projects the multi-modal features into the frequency domain and leverages\nthe spectral space for fusion. To reduce dynamic contamination that is unique\nto each modality, we introduce a filter to attenuate and suppress the modality\nnoise adaptively while capturing the universal modality patterns effectively.\nFurthermore, we explore the item latent structures by designing a new\nmulti-modal graph learning module to capture associative semantic correlations\nand universal fusion patterns among similar items. Finally, we formulate a new\nmodality-aware preference module, which infuses behavioral features and\nbalances the uni- and multi-modal features for precise preference modeling.\nThis empowers SMORE with the ability to infer both user modality-specific and\nfusion preferences more accurately. Experiments on three real-world datasets\nshow the efficacy of our proposed model. The source code for this work has been\nmade publicly available at https://github.com/kennethorq/SMORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating multi-modal features as side information has recently become a\ntrend in recommender systems. To elucidate user-item preferences, recent\nstudies focus on fusing modalities via concatenation, element-wise sum, or\nattention mechanisms. Despite having notable success, existing approaches do\nnot account for the modality-specific noise encapsulated within each modality.\nAs a result, direct fusion of modalities will lead to the amplification of\ncross-modality noise. Moreover, the variation of noise that is unique within\neach modality results in noise alleviation and fusion being more challenging.\nIn this work, we propose a new Spectrum-based Modality Representation (SMORE)\nfusion graph recommender that aims to capture both uni-modal and fusion\npreferences while simultaneously suppressing modality noise. Specifically,\nSMORE projects the multi-modal features into the frequency domain and leverages\nthe spectral space for fusion. To reduce dynamic contamination that is unique\nto each modality, we introduce a filter to attenuate and suppress the modality\nnoise adaptively while capturing the universal modality patterns effectively.\nFurthermore, we explore the item latent structures by designing a new\nmulti-modal graph learning module to capture associative semantic correlations\nand universal fusion patterns among similar items. Finally, we formulate a new\nmodality-aware preference module, which infuses behavioral features and\nbalances the uni- and multi-modal features for precise preference modeling.\nThis empowers SMORE with the ability to infer both user modality-specific and\nfusion preferences more accurately. Experiments on three real-world datasets\nshow the efficacy of our proposed model. The source code for this work has been\nmade publicly available at https://github.com/kennethorq/SMORE."
                },
                "authors": [
                    {
                        "name": "Rongqing Kenneth Ong"
                    },
                    {
                        "name": "Andy W. H. Khong"
                    }
                ],
                "author_detail": {
                    "name": "Andy W. H. Khong"
                },
                "author": "Andy W. H. Khong",
                "arxiv_doi": "10.1145/3701551.3703561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.14978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Web Search and Data Mining (WSDM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13765v2",
                "updated": "2024-12-19T15:50:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    50,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-18T12:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms"
                },
                "summary": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama."
                },
                "authors": [
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ahmed Abdelmoneim Mazrou"
                    },
                    {
                        "name": "Mohamed Shaltout"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Shaltout"
                },
                "author": "Mohamed Shaltout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14964v1",
                "updated": "2024-12-19T15:44:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    44,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:44:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    44,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "Knowledge Injection via Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Injection via Prompt Distillation"
                },
                "summary": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights."
                },
                "authors": [
                    {
                        "name": "Kalle Kujanp"
                    },
                    {
                        "name": "Harri Valpola"
                    },
                    {
                        "name": "Alexander Ilin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Ilin"
                },
                "author": "Alexander Ilin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14959v1",
                "updated": "2024-12-19T15:39:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    39,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:39:31Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    39,
                    31,
                    3,
                    354,
                    0
                ],
                "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction"
                },
                "summary": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/."
                },
                "authors": [
                    {
                        "name": "Qingjie Zhang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Haoting Qian"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10839v2",
                "updated": "2024-12-19T15:25:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    25,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-20T13:34:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Benchmarking Large Language Models for Math Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Math Reasoning Tasks"
                },
                "summary": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research."
                },
                "authors": [
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Yao Rong"
                    },
                    {
                        "name": "Emek Gzlkl"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v4",
                "updated": "2024-12-19T15:16:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    16,
                    49,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02231v2",
                "updated": "2024-12-19T15:15:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    15,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-04T16:24:23Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    24,
                    23,
                    0,
                    309,
                    0
                ],
                "title": "Sharp Bounds for Continuous-Valued Treatment Effects with Unobserved\n  Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Bounds for Continuous-Valued Treatment Effects with Unobserved\n  Confounders"
                },
                "summary": "In causal inference, treatment effects are typically estimated under the\nignorability, or unconfoundedness, assumption, which is often unrealistic in\nobservational data. By relaxing this assumption and conducting a sensitivity\nanalysis, we introduce novel bounds and derive confidence intervals for the\nAverage Potential Outcome (APO) - a standard metric for evaluating\ncontinuous-valued treatment or exposure effects. We demonstrate that these\nbounds are sharp under a continuous sensitivity model, in the sense that they\ngive the smallest possible interval under this model, and propose a doubly\nrobust version of our estimators. In a comparative analysis with the method of\nJesson et al. (2022) (arXiv:2204.10022), using both simulated and real\ndatasets, we show that our approach not only yields sharper bounds but also\nachieves good coverage of the true APO, with significantly reduced computation\ntimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, treatment effects are typically estimated under the\nignorability, or unconfoundedness, assumption, which is often unrealistic in\nobservational data. By relaxing this assumption and conducting a sensitivity\nanalysis, we introduce novel bounds and derive confidence intervals for the\nAverage Potential Outcome (APO) - a standard metric for evaluating\ncontinuous-valued treatment or exposure effects. We demonstrate that these\nbounds are sharp under a continuous sensitivity model, in the sense that they\ngive the smallest possible interval under this model, and propose a doubly\nrobust version of our estimators. In a comparative analysis with the method of\nJesson et al. (2022) (arXiv:2204.10022), using both simulated and real\ndatasets, we show that our approach not only yields sharper bounds but also\nachieves good coverage of the true APO, with significantly reduced computation\ntimes."
                },
                "authors": [
                    {
                        "name": "Jean-Baptiste Baitairian"
                    },
                    {
                        "name": "Bernard Sebastien"
                    },
                    {
                        "name": "Rana Jreich"
                    },
                    {
                        "name": "Sandrine Katsahian"
                    },
                    {
                        "name": "Agathe Guilloux"
                    }
                ],
                "author_detail": {
                    "name": "Agathe Guilloux"
                },
                "author": "Agathe Guilloux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04693v2",
                "updated": "2024-12-19T15:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    11,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-05T17:56:38Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    17,
                    56,
                    38,
                    4,
                    187,
                    0
                ],
                "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted by NeurIPS 2024. Dataset, code, and model are released at\n  https://github.com/open-compass/ANAH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v3",
                "updated": "2024-12-19T15:11:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    11,
                    46,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09423v3",
                "updated": "2024-12-19T15:10:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    10,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2023-07-18T16:43:03Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    16,
                    43,
                    3,
                    1,
                    199,
                    0
                ],
                "title": "Scaling Laws for Imitation Learning in Single-Agent Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Imitation Learning in Single-Agent Games"
                },
                "summary": "Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems."
                },
                "authors": [
                    {
                        "name": "Jens Tuyls"
                    },
                    {
                        "name": "Dhruv Madeka"
                    },
                    {
                        "name": "Kari Torkkola"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Sham Kakade"
                    }
                ],
                "author_detail": {
                    "name": "Sham Kakade"
                },
                "author": "Sham Kakade",
                "arxiv_comment": "Accepted at TMLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v5",
                "updated": "2024-12-19T15:07:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    7,
                    38,
                    3,
                    354,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14922v1",
                "updated": "2024-12-19T15:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    0,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:00:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    0,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response"
                },
                "summary": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13574v2",
                "updated": "2024-12-19T14:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    55,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-18T07:48:47Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    48,
                    47,
                    2,
                    353,
                    0
                ],
                "title": "Revisiting Interactions of Multiple Driver States in Heterogenous\n  Population and Cognitive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Interactions of Multiple Driver States in Heterogenous\n  Population and Cognitive Tasks"
                },
                "summary": "In real-world driving scenarios, multiple states occur simultaneously due to\nindividual differences and environmental factors, complicating the analysis and\nestimation of driver states. Previous studies, limited by experimental design\nand analytical methods, may not be able to disentangle the relationships among\nmultiple driver states and environmental factors. This paper introduces the\nDouble Machine Learning (DML) analysis method to the field of driver state\nanalysis to tackle this challenge. To train and test the DML model, a driving\nsimulator experiment with 42 participants was conducted. All participants drove\nSAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour\ndriving experiment. Drivers' subjective cognitive load and drowsiness levels\nwere collected throughout the experiment. Then, we isolated individual and\nenvironmental factors affecting driver state variations and the factors\naffecting drivers' physiological and eye-tracking metrics when they are under\nspecific states. The results show that our approach successfully decoupled and\ninferred the complex causal relationships between multiple types of drowsiness\nand cognitive load. Additionally, we identified key physiological and\neye-tracking indicators in the presence of multiple driver states and under the\ninfluence of a single state, excluding the influence of other driver states,\nenvironmental factors, and individual characteristics. Our causal inference\nanalytical framework can offer new insights for subsequent analysis of drivers'\nstates. Further, the updated causal relation graph based on the DML analysis\ncan provide theoretical bases for driver state monitoring based on\nphysiological and eye-tracking measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world driving scenarios, multiple states occur simultaneously due to\nindividual differences and environmental factors, complicating the analysis and\nestimation of driver states. Previous studies, limited by experimental design\nand analytical methods, may not be able to disentangle the relationships among\nmultiple driver states and environmental factors. This paper introduces the\nDouble Machine Learning (DML) analysis method to the field of driver state\nanalysis to tackle this challenge. To train and test the DML model, a driving\nsimulator experiment with 42 participants was conducted. All participants drove\nSAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour\ndriving experiment. Drivers' subjective cognitive load and drowsiness levels\nwere collected throughout the experiment. Then, we isolated individual and\nenvironmental factors affecting driver state variations and the factors\naffecting drivers' physiological and eye-tracking metrics when they are under\nspecific states. The results show that our approach successfully decoupled and\ninferred the complex causal relationships between multiple types of drowsiness\nand cognitive load. Additionally, we identified key physiological and\neye-tracking indicators in the presence of multiple driver states and under the\ninfluence of a single state, excluding the influence of other driver states,\nenvironmental factors, and individual characteristics. Our causal inference\nanalytical framework can offer new insights for subsequent analysis of drivers'\nstates. Further, the updated causal relation graph based on the DML analysis\ncan provide theoretical bases for driver state monitoring based on\nphysiological and eye-tracking measures."
                },
                "authors": [
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Ange Wang"
                    },
                    {
                        "name": "Song Yan"
                    },
                    {
                        "name": "Dengbo He"
                    },
                    {
                        "name": "Kaishun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kaishun Wu"
                },
                "author": "Kaishun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14905v1",
                "updated": "2024-12-19T14:37:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    37,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T14:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    37,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation"
                },
                "summary": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks."
                },
                "authors": [
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Shengnan An"
                    },
                    {
                        "name": "Zeqi Lin"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Bing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xie"
                },
                "author": "Bing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11544v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11544v4",
                "updated": "2024-12-19T14:33:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    33,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-17T13:42:28Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    42,
                    28,
                    0,
                    169,
                    0
                ],
                "title": "Do Parameters Reveal More than Loss for Membership Inference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Parameters Reveal More than Loss for Membership Inference?"
                },
                "summary": "Membership inference attacks are used as a key tool for disclosure auditing.\nThey aim to infer whether an individual record was used to train a model. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide tight bounds on\nleakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for\nstochastic gradient descent, and that optimal membership inference indeed\nrequires white-box access. Our theoretical results lead to a new white-box\ninference attack, IHA (Inverse Hessian Attack), that explicitly uses model\nparameters by taking advantage of computing inverse-Hessian vector products.\nOur results show that both auditors and adversaries may be able to benefit from\naccess to model parameters, and we advocate for further research into white-box\nmethods for membership inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks are used as a key tool for disclosure auditing.\nThey aim to infer whether an individual record was used to train a model. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide tight bounds on\nleakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for\nstochastic gradient descent, and that optimal membership inference indeed\nrequires white-box access. Our theoretical results lead to a new white-box\ninference attack, IHA (Inverse Hessian Attack), that explicitly uses model\nparameters by taking advantage of computing inverse-Hessian vector products.\nOur results show that both auditors and adversaries may be able to benefit from\naccess to model parameters, and we advocate for further research into white-box\nmethods for membership inference."
                },
                "authors": [
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "David Evans"
                    }
                ],
                "author_detail": {
                    "name": "David Evans"
                },
                "author": "David Evans",
                "arxiv_comment": "Accepted to Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11544v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11544v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00467v3",
                "updated": "2024-12-19T14:29:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    29,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-01T07:49:24Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    49,
                    24,
                    1,
                    275,
                    0
                ],
                "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Planning for LLM-based Graphical User Interface Automation"
                },
                "summary": "The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT."
                },
                "authors": [
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14867v1",
                "updated": "2024-12-19T14:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    3,
                    22,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T14:03:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    3,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering"
                },
                "summary": "Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14860v1",
                "updated": "2024-12-19T13:55:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling"
                },
                "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14849v1",
                "updated": "2024-12-19T13:39:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:39:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis"
                },
                "summary": "Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods."
                },
                "authors": [
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14848v1",
                "updated": "2024-12-19T13:39:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    36,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:39:36Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    36,
                    3,
                    354,
                    0
                ],
                "title": "ElectraSight: Smart Glasses with Fully Onboard Non-Invasive Eye Tracking\n  Using Hybrid Contact and Contactless EOG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElectraSight: Smart Glasses with Fully Onboard Non-Invasive Eye Tracking\n  Using Hybrid Contact and Contactless EOG"
                },
                "summary": "Smart glasses with integrated eye tracking technology are revolutionizing\ndiverse fields, from immersive augmented reality experiences to cutting-edge\nhealth monitoring solutions. However, traditional eye tracking systems rely\nheavily on cameras and significant computational power, leading to high-energy\ndemand and privacy issues. Alternatively, systems based on electrooculography\n(EOG) provide superior battery life but are less accurate and primarily\neffective for detecting blinks, while being highly invasive. The paper\nintroduces ElectraSight, a non-invasive plug-and-play low-power eye tracking\nsystem for smart glasses. The hardware-software co-design of the system is\ndetailed, along with the integration of a hybrid EOG (hEOG) solution that\nincorporates both contact and contactless electrodes. Within 79 kB of memory,\nthe proposed tinyML model performs real-time eye movement classification with\n81% accuracy for 10 classes and 92% for 6 classes, not requiring any\ncalibration or user-specific fine-tuning. Experimental results demonstrate that\nElectraSight delivers high accuracy in eye movement and blink classification,\nwith minimal overall movement detection latency (90% within 60 ms) and an\nultra-low computing time (301 {\\mu}s). The power consumption settles down to\n7.75 mW for continuous data acquisition and 46 mJ for the tinyML inference.\nThis efficiency enables continuous operation for over 3 days on a compact 175\nmAh battery. This work opens new possibilities for eye tracking in commercial\napplications, offering an unobtrusive solution that enables advancements in\nuser interfaces, health diagnostics, and hands-free control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart glasses with integrated eye tracking technology are revolutionizing\ndiverse fields, from immersive augmented reality experiences to cutting-edge\nhealth monitoring solutions. However, traditional eye tracking systems rely\nheavily on cameras and significant computational power, leading to high-energy\ndemand and privacy issues. Alternatively, systems based on electrooculography\n(EOG) provide superior battery life but are less accurate and primarily\neffective for detecting blinks, while being highly invasive. The paper\nintroduces ElectraSight, a non-invasive plug-and-play low-power eye tracking\nsystem for smart glasses. The hardware-software co-design of the system is\ndetailed, along with the integration of a hybrid EOG (hEOG) solution that\nincorporates both contact and contactless electrodes. Within 79 kB of memory,\nthe proposed tinyML model performs real-time eye movement classification with\n81% accuracy for 10 classes and 92% for 6 classes, not requiring any\ncalibration or user-specific fine-tuning. Experimental results demonstrate that\nElectraSight delivers high accuracy in eye movement and blink classification,\nwith minimal overall movement detection latency (90% within 60 ms) and an\nultra-low computing time (301 {\\mu}s). The power consumption settles down to\n7.75 mW for continuous data acquisition and 46 mJ for the tinyML inference.\nThis efficiency enables continuous operation for over 3 days on a compact 175\nmAh battery. This work opens new possibilities for eye tracking in commercial\napplications, offering an unobtrusive solution that enables advancements in\nuser interfaces, health diagnostics, and hands-free control systems."
                },
                "authors": [
                    {
                        "name": "Nicolas Schrer"
                    },
                    {
                        "name": "Federico Villani"
                    },
                    {
                        "name": "Aishwarya Melatur"
                    },
                    {
                        "name": "Steven Peter"
                    },
                    {
                        "name": "Tommaso Polonelli"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14846v1",
                "updated": "2024-12-19T13:38:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    38,
                    20,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:38:20Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    38,
                    20,
                    3,
                    354,
                    0
                ],
                "title": "Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet"
                },
                "summary": "Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code."
                },
                "authors": [
                    {
                        "name": "Litingyu Wang"
                    },
                    {
                        "name": "Wenjun Liao"
                    },
                    {
                        "name": "Shichuan Zhang"
                    },
                    {
                        "name": "Guotai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guotai Wang"
                },
                "author": "Guotai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14843v1",
                "updated": "2024-12-19T13:36:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    36,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:36:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    36,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas"
                },
                "summary": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training."
                },
                "authors": [
                    {
                        "name": "Pietro Bernardelle"
                    },
                    {
                        "name": "Leon Frhling"
                    },
                    {
                        "name": "Stefano Civelli"
                    },
                    {
                        "name": "Riccardo Lunardi"
                    },
                    {
                        "name": "Kevin Roiter"
                    },
                    {
                        "name": "Gianluca Demartini"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Demartini"
                },
                "author": "Gianluca Demartini",
                "arxiv_comment": "4 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11778v2",
                "updated": "2024-12-19T13:34:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    56,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-21T17:08:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    8,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Sum of Squares Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sum of Squares Circuits"
                },
                "summary": "Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation."
                },
                "authors": [
                    {
                        "name": "Lorenzo Loconte"
                    },
                    {
                        "name": "Stefan Mengel"
                    },
                    {
                        "name": "Antonio Vergari"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Vergari"
                },
                "author": "Antonio Vergari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14841v1",
                "updated": "2024-12-19T13:34:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:34:14Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis"
                },
                "summary": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools."
                },
                "authors": [
                    {
                        "name": "Greta Dolcetti"
                    },
                    {
                        "name": "Vincenzo Arceri"
                    },
                    {
                        "name": "Eleonora Iotti"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Agostino Cortesi"
                    },
                    {
                        "name": "Enea Zaffanella"
                    }
                ],
                "author_detail": {
                    "name": "Enea Zaffanella"
                },
                "author": "Enea Zaffanella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12446v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12446v3",
                "updated": "2024-12-19T13:33:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    33,
                    49,
                    3,
                    354,
                    0
                ],
                "published": "2023-10-19T03:53:41Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    3,
                    53,
                    41,
                    3,
                    292,
                    0
                ],
                "title": "Electromagnetic Information Theory-Based Statistical Channel Model for\n  Improved Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromagnetic Information Theory-Based Statistical Channel Model for\n  Improved Channel Estimation"
                },
                "summary": "Electromagnetic information theory (EIT) is an emerging interdisciplinary\nsubject that integrates classical Maxwell electromagnetics and Shannon\ninformation theory. The goal of EIT is to uncover the information transmission\nmechanisms from an electromagnetic (EM) perspective in wireless systems.\nExisting works on EIT are mainly focused on the analysis of EM channel\ncharacteristics, degrees-of-freedom, and system capacity. However, these works\ndo not clarify how to integrate EIT knowledge into the design and optimization\nof wireless systems. To fill in this gap, in this paper, we propose an\nEIT-based statistical channel model with simplified parameterization. Thanks to\nthe simplified closed-form expression of the EMCF, it can be readily applied to\nvarious channel modeling and inference tasks. Specifically, by averaging the\nsolutions of Maxwell's equations over a tunable von Mises distribution, we\nobtain a spatio-temporal correlation function (STCF) model of the EM channel,\nwhich we name as the EMCF. Furthermore, by tuning the parameters of the EMCF,\nwe propose an EIT-based covariance estimator (EIT-Cov) to accurately capture\nthe channel covariance. Since classical MMSE estimators can exploit prior\ninformation contained in the channel covariance matrix, we further propose the\nEIT-MMSE channel estimator by substituting EMCF for the covariance matrix.\nSimulation results show that both the proposed EIT-Cov covariance estimator and\nthe EIT-MMSE channel estimator outperform their baseline algorithms, thus\nproving that EIT is beneficial to wireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromagnetic information theory (EIT) is an emerging interdisciplinary\nsubject that integrates classical Maxwell electromagnetics and Shannon\ninformation theory. The goal of EIT is to uncover the information transmission\nmechanisms from an electromagnetic (EM) perspective in wireless systems.\nExisting works on EIT are mainly focused on the analysis of EM channel\ncharacteristics, degrees-of-freedom, and system capacity. However, these works\ndo not clarify how to integrate EIT knowledge into the design and optimization\nof wireless systems. To fill in this gap, in this paper, we propose an\nEIT-based statistical channel model with simplified parameterization. Thanks to\nthe simplified closed-form expression of the EMCF, it can be readily applied to\nvarious channel modeling and inference tasks. Specifically, by averaging the\nsolutions of Maxwell's equations over a tunable von Mises distribution, we\nobtain a spatio-temporal correlation function (STCF) model of the EM channel,\nwhich we name as the EMCF. Furthermore, by tuning the parameters of the EMCF,\nwe propose an EIT-based covariance estimator (EIT-Cov) to accurately capture\nthe channel covariance. Since classical MMSE estimators can exploit prior\ninformation contained in the channel covariance matrix, we further propose the\nEIT-MMSE channel estimator by substituting EMCF for the covariance matrix.\nSimulation results show that both the proposed EIT-Cov covariance estimator and\nthe EIT-MMSE channel estimator outperform their baseline algorithms, thus\nproving that EIT is beneficial to wireless communication systems."
                },
                "authors": [
                    {
                        "name": "Jieao Zhu"
                    },
                    {
                        "name": "Zhongzhichao Wan"
                    },
                    {
                        "name": "Linglong Dai"
                    },
                    {
                        "name": "Tie Jun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Tie Jun Cui"
                },
                "author": "Tie Jun Cui",
                "arxiv_comment": "Electromagnetic information theory (EIT) is an emerging\n  interdisciplinary subject, aiming at providing a unified analytical framework\n  for wireless systems as well as guiding practical system design. This paper\n  answers the question: \"Whether can we improve wireless communication systems\n  via EIT\"?",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12446v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12446v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14837v1",
                "updated": "2024-12-19T13:27:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    27,
                    58,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:27:58Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    27,
                    58,
                    3,
                    354,
                    0
                ],
                "title": "ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects"
                },
                "summary": "3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models."
                },
                "authors": [
                    {
                        "name": "Qihang Cao"
                    },
                    {
                        "name": "Huangxun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huangxun Chen"
                },
                "author": "Huangxun Chen",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14834v1",
                "updated": "2024-12-19T13:24:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    24,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:24:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    24,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning"
                },
                "summary": "Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks."
                },
                "authors": [
                    {
                        "name": "Mohammadreza nakhaei"
                    },
                    {
                        "name": "Aidan Scannell"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "arxiv_comment": "7 Pages, Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14821v1",
                "updated": "2024-12-19T13:12:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    12,
                    15,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:12:15Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    12,
                    15,
                    3,
                    354,
                    0
                ],
                "title": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation"
                },
                "summary": "Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}"
                },
                "authors": [
                    {
                        "name": "Shoumeng Qiu"
                    },
                    {
                        "name": "Xinrun Li"
                    },
                    {
                        "name": "XiangYang Xue"
                    },
                    {
                        "name": "Jian Pu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Pu"
                },
                "author": "Jian Pu",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14815v1",
                "updated": "2024-12-19T13:09:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    46,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:09:46Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    46,
                    3,
                    354,
                    0
                ],
                "title": "Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via\n  Infrared Side Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via\n  Infrared Side Channel"
                },
                "summary": "Virtual Reality (VR) technologies are increasingly employed in numerous\napplications across various areas. Therefore, it is essential to ensure the\nsecurity of interactions between users and VR devices. In this paper, we\ndisclose a new side-channel leakage in the constellation tracking system of\nmainstream VR platforms, where the infrared (IR) signals emitted from the VR\ncontrollers for controller-headset interactions can be maliciously exploited to\nreconstruct unconstrained input keystrokes on the virtual keyboard\nnon-intrusively. We propose a novel keystroke inference attack named VRecKey to\ndemonstrate the feasibility and practicality of this novel infrared side\nchannel. Specifically, VRecKey leverages a customized 2D IR sensor array to\nintercept ambient IR signals emitted from VR controllers and subsequently\ninfers (i) character-level key presses on the virtual keyboard and (ii)\nword-level keystrokes along with their typing trajectories. We extensively\nevaluate the effectiveness of VRecKey with two commercial VR devices, and the\nresults indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in\ninferring character-level and word-level keystrokes with varying lengths,\nrespectively. In addition, empirical results show that VRecKey is resilient to\nseveral practical impact factors and presents effectiveness in various\nreal-world scenarios, which provides a complementary and orthogonal attack\nsurface for the exploration of keystroke inference attacks in VR platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) technologies are increasingly employed in numerous\napplications across various areas. Therefore, it is essential to ensure the\nsecurity of interactions between users and VR devices. In this paper, we\ndisclose a new side-channel leakage in the constellation tracking system of\nmainstream VR platforms, where the infrared (IR) signals emitted from the VR\ncontrollers for controller-headset interactions can be maliciously exploited to\nreconstruct unconstrained input keystrokes on the virtual keyboard\nnon-intrusively. We propose a novel keystroke inference attack named VRecKey to\ndemonstrate the feasibility and practicality of this novel infrared side\nchannel. Specifically, VRecKey leverages a customized 2D IR sensor array to\nintercept ambient IR signals emitted from VR controllers and subsequently\ninfers (i) character-level key presses on the virtual keyboard and (ii)\nword-level keystrokes along with their typing trajectories. We extensively\nevaluate the effectiveness of VRecKey with two commercial VR devices, and the\nresults indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in\ninferring character-level and word-level keystrokes with varying lengths,\nrespectively. In addition, empirical results show that VRecKey is resilient to\nseveral practical impact factors and presents effectiveness in various\nreal-world scenarios, which provides a complementary and orthogonal attack\nsurface for the exploration of keystroke inference attacks in VR platforms."
                },
                "authors": [
                    {
                        "name": "Tao Ni"
                    },
                    {
                        "name": "Yuefeng Du"
                    },
                    {
                        "name": "Qingchuan Zhao"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14814v1",
                "updated": "2024-12-19T13:09:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:09:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "Answer Set Networks: Casting Answer Set Programming into Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Set Networks: Casting Answer Set Programming into Deep Learning"
                },
                "summary": "Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments."
                },
                "authors": [
                    {
                        "name": "Arseny Skryagin"
                    },
                    {
                        "name": "Daniel Ochs"
                    },
                    {
                        "name": "Phillip Deibert"
                    },
                    {
                        "name": "Simon Kohaut"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T37, 68T30, 68T27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14812v1",
                "updated": "2024-12-19T13:03:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    3,
                    4,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:03:04Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    3,
                    4,
                    3,
                    354,
                    0
                ],
                "title": "Generative CKM Construction using Partially Observed Data with Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative CKM Construction using Partially Observed Data with Diffusion\n  Model"
                },
                "summary": "Channel knowledge map (CKM) is a promising technique that enables\nenvironment-aware wireless networks by utilizing location-specific channel\nprior information to improve communication and sensing performance. A\nfundamental problem for CKM construction is how to utilize partially observed\nchannel knowledge data to reconstruct a complete CKM for all possible locations\nof interest. This problem resembles the long-standing ill-posed inverse\nproblem, which tries to infer from a set of limited observations the cause\nfactors that produced them. By utilizing the recent advances of solving inverse\nproblems with generative artificial intelligence (AI), in this paper, we\npropose generative CKM construction method using partially observed data by\nsolving inverse problems with diffusion models. Simulation results show that\nthe proposed method significantly improves the performance of CKM construction\ncompared with benchmarking schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel knowledge map (CKM) is a promising technique that enables\nenvironment-aware wireless networks by utilizing location-specific channel\nprior information to improve communication and sensing performance. A\nfundamental problem for CKM construction is how to utilize partially observed\nchannel knowledge data to reconstruct a complete CKM for all possible locations\nof interest. This problem resembles the long-standing ill-posed inverse\nproblem, which tries to infer from a set of limited observations the cause\nfactors that produced them. By utilizing the recent advances of solving inverse\nproblems with generative artificial intelligence (AI), in this paper, we\npropose generative CKM construction method using partially observed data by\nsolving inverse problems with diffusion models. Simulation results show that\nthe proposed method significantly improves the performance of CKM construction\ncompared with benchmarking schemes."
                },
                "authors": [
                    {
                        "name": "Shen Fu"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yong Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zeng"
                },
                "author": "Yong Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14809v1",
                "updated": "2024-12-19T12:57:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:57:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zeao Tu"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14790v1",
                "updated": "2024-12-19T12:29:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    29,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:29:31Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    29,
                    31,
                    3,
                    354,
                    0
                ],
                "title": "YOLOv11 Optimization for Efficient Resource Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOv11 Optimization for Efficient Resource Utilization"
                },
                "summary": "The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository"
                },
                "authors": [
                    {
                        "name": "Areeg Fagad Rasheed"
                    },
                    {
                        "name": "M. Zarkoosh"
                    }
                ],
                "author_detail": {
                    "name": "M. Zarkoosh"
                },
                "author": "M. Zarkoosh",
                "arxiv_comment": "12 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16302v2",
                "updated": "2024-12-19T12:14:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    14,
                    3,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-23T08:57:11Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    57,
                    11,
                    1,
                    205,
                    0
                ],
                "title": "DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions"
                },
                "summary": "Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings."
                },
                "authors": [
                    {
                        "name": "Aditya Kapoor"
                    },
                    {
                        "name": "Harshad Khadilkar"
                    },
                    {
                        "name": "Jayvardhana Gubbi"
                    }
                ],
                "author_detail": {
                    "name": "Jayvardhana Gubbi"
                },
                "author": "Jayvardhana Gubbi",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10769v2",
                "updated": "2024-12-19T12:12:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    12,
                    28,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-15T14:50:20Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    50,
                    20,
                    0,
                    197,
                    0
                ],
                "title": "Quantum Data Centers in the Presence of Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Data Centers in the Presence of Noise"
                },
                "summary": "Quantum data centres (QDCs) could overcome the scalability challenges of\nmodern quantum computers. Single-processor monolithic quantum computers are\naffected by increased cross talk and difficulty of implementing gates when the\nnumber of qubits is increased. In a QDC, multiple quantum processing units\n(QPUs) are linked together over short distances, allowing the total number of\ncomputational qubits to be increased without increasing the number of qubits on\nany one processor. In doing so, the error incurred by operations at each QPU\ncan be kept small, however additional noise will be added to the system due to\nthe latency cost and errors incurred during inter-QPU entanglement\ndistribution. We investigate the relative impact of these different types of\nnoise using a classically simulated QDC with two QPUs and compare the\nrobustness to noise of the two main ways of implementing remote gates, cat-comm\nand TP-comm. We find that considering the quantity of gates or inter-QPU\nentangled links is often inadequate to predict the output fidelity from a\nquantum circuit and infer that an improved understanding of error propagation\nduring distributed quantum circuits may represent a significant optimisation\nopportunity for compilation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum data centres (QDCs) could overcome the scalability challenges of\nmodern quantum computers. Single-processor monolithic quantum computers are\naffected by increased cross talk and difficulty of implementing gates when the\nnumber of qubits is increased. In a QDC, multiple quantum processing units\n(QPUs) are linked together over short distances, allowing the total number of\ncomputational qubits to be increased without increasing the number of qubits on\nany one processor. In doing so, the error incurred by operations at each QPU\ncan be kept small, however additional noise will be added to the system due to\nthe latency cost and errors incurred during inter-QPU entanglement\ndistribution. We investigate the relative impact of these different types of\nnoise using a classically simulated QDC with two QPUs and compare the\nrobustness to noise of the two main ways of implementing remote gates, cat-comm\nand TP-comm. We find that considering the quantity of gates or inter-QPU\nentangled links is often inadequate to predict the output fidelity from a\nquantum circuit and infer that an improved understanding of error propagation\nduring distributed quantum circuits may represent a significant optimisation\nopportunity for compilation."
                },
                "authors": [
                    {
                        "name": "K. Campbell"
                    },
                    {
                        "name": "A. Lawey"
                    },
                    {
                        "name": "M. Razavi"
                    }
                ],
                "author_detail": {
                    "name": "M. Razavi"
                },
                "author": "M. Razavi",
                "arxiv_doi": "10.1088/2058-9565/ad9cb8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/2058-9565/ad9cb8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.10769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 18 figures. Accepted for publication in IOP Quantum Science\n  and Technology. The following changes have been made for v2: legends were\n  added to figures in addition to the annotations; some typos have been\n  corrected; some minor additional information has been provided in places to\n  clarify comments made; and the style has been changed from single-column to\n  two-column",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09265v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09265v4",
                "updated": "2024-12-19T12:11:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    11,
                    13,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-12T13:22:02Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    22,
                    2,
                    3,
                    347,
                    0
                ],
                "title": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation"
                },
                "summary": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks."
                },
                "authors": [
                    {
                        "name": "Bofang Jia"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Pengfang Qian"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09265v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09265v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04677v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04677v3",
                "updated": "2024-12-19T12:08:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    8,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-07T13:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    3,
                    21,
                    3,
                    312,
                    0
                ],
                "title": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval"
                },
                "summary": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir."
                },
                "authors": [
                    {
                        "name": "Ferdinand Schlatt"
                    },
                    {
                        "name": "Maik Frbe"
                    },
                    {
                        "name": "Matthias Hagen"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hagen"
                },
                "author": "Matthias Hagen",
                "arxiv_doi": "10.1145/3701551.3704118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3704118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.04677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04677v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as a demo at WSDM'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14780v1",
                "updated": "2024-12-19T12:06:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    6,
                    24,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:06:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    6,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning"
                },
                "summary": "When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT)."
                },
                "authors": [
                    {
                        "name": "Ziang Ye"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Jianxin Ma"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05428v2",
                "updated": "2024-12-19T11:57:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    57,
                    19,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-10T04:21:04Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    4,
                    21,
                    4,
                    5,
                    223,
                    0
                ],
                "title": "Generalized Encouragement-Based Instrumental Variables for\n  Counterfactual Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Encouragement-Based Instrumental Variables for\n  Counterfactual Regression"
                },
                "summary": "In causal inference, encouragement designs (EDs) are widely used to analyze\ncausal effects, when randomized controlled trials (RCTs) are impractical or\ncompliance to treatment cannot be perfectly enforced. Unlike RCTs, which\ndirectly allocate treatments, EDs randomly assign encouragement policies that\npositively motivate individuals to engage in a specific treatment. These random\nencouragements act as instrumental variables (IVs), facilitating the\nidentification of causal effects through leveraging exogenous perturbations in\ndiscrete treatment scenarios. However, real-world applications of encouragement\ndesigns often face challenges such as incomplete randomization, limited\nexperimental data, and significantly fewer encouragements compared to\ntreatments, hindering precise causal effect estimation. To address this, this\npaper introduces novel theories and algorithms for identifying the Conditional\nAverage Treatment Effect (CATE) using variations in encouragement. Further, by\nleveraging both observational and encouragement data, we propose a generalized\nIV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),\nto effectively estimate the causal effects. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the superiority of EnCounteR over\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, encouragement designs (EDs) are widely used to analyze\ncausal effects, when randomized controlled trials (RCTs) are impractical or\ncompliance to treatment cannot be perfectly enforced. Unlike RCTs, which\ndirectly allocate treatments, EDs randomly assign encouragement policies that\npositively motivate individuals to engage in a specific treatment. These random\nencouragements act as instrumental variables (IVs), facilitating the\nidentification of causal effects through leveraging exogenous perturbations in\ndiscrete treatment scenarios. However, real-world applications of encouragement\ndesigns often face challenges such as incomplete randomization, limited\nexperimental data, and significantly fewer encouragements compared to\ntreatments, hindering precise causal effect estimation. To address this, this\npaper introduces novel theories and algorithms for identifying the Conditional\nAverage Treatment Effect (CATE) using variations in encouragement. Further, by\nleveraging both observational and encouragement data, we propose a generalized\nIV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),\nto effectively estimate the causal effects. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the superiority of EnCounteR over\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Anpeng Wu"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Ruoxuan Xiong"
                    },
                    {
                        "name": "Xiangwei Chen"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14771v1",
                "updated": "2024-12-19T11:55:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    55,
                    51,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:55:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    55,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Rabee Qasem"
                    },
                    {
                        "name": "Mohannad Hendi"
                    },
                    {
                        "name": "Banan Tantour"
                    }
                ],
                "author_detail": {
                    "name": "Banan Tantour"
                },
                "author": "Banan Tantour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14764v1",
                "updated": "2024-12-19T11:48:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    48,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:48:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    48,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question\n  Answering"
                },
                "summary": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically\ndesigned for evaluating repository-level question-answering capabilities in the\nfield of software engineering. CodeRepoQA encompasses five programming\nlanguages and covers a wide range of scenarios, enabling comprehensive\nevaluation of language models. To construct this dataset, we crawl data from 30\nwell-known repositories in GitHub, the largest platform for hosting and\ncollaborating on code, and carefully filter raw data. In total, CodeRepoQA is a\nmulti-turn question-answering benchmark with 585,687 entries, covering a\ndiverse array of software engineering scenarios, with an average of 6.62\ndialogue turns per entry.\n  We evaluate ten popular large language models on our dataset and provide\nin-depth analysis. We find that LLMs still have limitations in\nquestion-answering capabilities in the field of software engineering, and\nmedium-length contexts are more conducive to LLMs' performance. The entire\nbenchmark is publicly available at\nhttps://github.com/kinesiatricssxilm14/CodeRepoQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically\ndesigned for evaluating repository-level question-answering capabilities in the\nfield of software engineering. CodeRepoQA encompasses five programming\nlanguages and covers a wide range of scenarios, enabling comprehensive\nevaluation of language models. To construct this dataset, we crawl data from 30\nwell-known repositories in GitHub, the largest platform for hosting and\ncollaborating on code, and carefully filter raw data. In total, CodeRepoQA is a\nmulti-turn question-answering benchmark with 585,687 entries, covering a\ndiverse array of software engineering scenarios, with an average of 6.62\ndialogue turns per entry.\n  We evaluate ten popular large language models on our dataset and provide\nin-depth analysis. We find that LLMs still have limitations in\nquestion-answering capabilities in the field of software engineering, and\nmedium-length contexts are more conducive to LLMs' performance. The entire\nbenchmark is publicly available at\nhttps://github.com/kinesiatricssxilm14/CodeRepoQA."
                },
                "authors": [
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Qinyun Wu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14758v1",
                "updated": "2024-12-19T11:39:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    39,
                    10,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:39:10Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    39,
                    10,
                    3,
                    354,
                    0
                ],
                "title": "Semantics Foundation of Reductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantics Foundation of Reductive Reasoning"
                },
                "summary": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    },
                    {
                        "name": "David J. Pym"
                    }
                ],
                "author_detail": {
                    "name": "David J. Pym"
                },
                "author": "David J. Pym",
                "arxiv_journal_ref": "Topoi 2024, Special issue 'Meaning and Understanding via Proofs'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14751v1",
                "updated": "2024-12-19T11:30:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    30,
                    7,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:30:07Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    30,
                    7,
                    3,
                    354,
                    0
                ],
                "title": "Query pipeline optimization for cancer patient question answering\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query pipeline optimization for cancer patient question answering\n  systems"
                },
                "summary": "Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems."
                },
                "authors": [
                    {
                        "name": "Maolin He"
                    },
                    {
                        "name": "Rena Gao"
                    },
                    {
                        "name": "Mike Conway"
                    },
                    {
                        "name": "Brian E. Chapman"
                    }
                ],
                "author_detail": {
                    "name": "Brian E. Chapman"
                },
                "author": "Brian E. Chapman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14741v1",
                "updated": "2024-12-19T11:17:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    17,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:17:31Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    17,
                    31,
                    3,
                    354,
                    0
                ],
                "title": "Active Inference and Human--Computer Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference and Human--Computer Interaction"
                },
                "summary": "Active Inference is a closed-loop computational theoretical basis for\nunderstanding behaviour, based on agents with internal probabilistic generative\nmodels that encode their beliefs about how hidden states in their environment\ncause their sensations. We review Active Inference and how it could be applied\nto model the human-computer interaction loop. Active Inference provides a\ncoherent framework for managing generative models of humans, their\nenvironments, sensors and interface components. It informs off-line design and\nsupports real-time, online adaptation. It provides model-based explanations for\nbehaviours observed in HCI, and new tools to measure important concepts such as\nagency and engagement. We discuss how Active Inference offers a new basis for a\ntheory of interaction in HCI, tools for design of modern, complex sensor-based\nsystems, and integration of artificial intelligence technologies, enabling it\nto cope with diversity in human users and contexts. We discuss the practical\nchallenges in implementing such Active Inference-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference is a closed-loop computational theoretical basis for\nunderstanding behaviour, based on agents with internal probabilistic generative\nmodels that encode their beliefs about how hidden states in their environment\ncause their sensations. We review Active Inference and how it could be applied\nto model the human-computer interaction loop. Active Inference provides a\ncoherent framework for managing generative models of humans, their\nenvironments, sensors and interface components. It informs off-line design and\nsupports real-time, online adaptation. It provides model-based explanations for\nbehaviours observed in HCI, and new tools to measure important concepts such as\nagency and engagement. We discuss how Active Inference offers a new basis for a\ntheory of interaction in HCI, tools for design of modern, complex sensor-based\nsystems, and integration of artificial intelligence technologies, enabling it\nto cope with diversity in human users and contexts. We discuss the practical\nchallenges in implementing such Active Inference-based systems."
                },
                "authors": [
                    {
                        "name": "Roderick Murray-Smith"
                    },
                    {
                        "name": "John H. Williamson"
                    },
                    {
                        "name": "Sebastian Stein"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stein"
                },
                "author": "Sebastian Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14737v1",
                "updated": "2024-12-19T11:10:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    10,
                    36,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:10:36Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    10,
                    36,
                    3,
                    354,
                    0
                ],
                "title": "On Verbalized Confidence Scores for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Verbalized Confidence Scores for LLMs"
                },
                "summary": "The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq ."
                },
                "authors": [
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Yao-Hung Hubert Tsai"
                    },
                    {
                        "name": "Makoto Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Makoto Yamada"
                },
                "author": "Makoto Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18403v2",
                "updated": "2024-12-19T11:07:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    7,
                    9,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-26T14:56:13Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    56,
                    13,
                    2,
                    178,
                    0
                ],
                "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks"
                },
                "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators."
                },
                "authors": [
                    {
                        "name": "Anna Bavaresco"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Raquel Fernndez"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Esam Ghaleb"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Aditya K Surikuchi"
                    },
                    {
                        "name": "Ece Takmaz"
                    },
                    {
                        "name": "Alberto Testoni"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Testoni"
                },
                "author": "Alberto Testoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14720v1",
                "updated": "2024-12-19T10:41:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    41,
                    53,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T10:41:53Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    41,
                    53,
                    3,
                    354,
                    0
                ],
                "title": "MICG-AI: A multidimensional index of child growth based on digital\n  phenotyping with Bayesian artificial intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MICG-AI: A multidimensional index of child growth based on digital\n  phenotyping with Bayesian artificial intelligence"
                },
                "summary": "This document proposes an algorithm for a mobile application designed to\nmonitor multidimensional child growth through digital phenotyping. Digital\nphenotyping offers a unique opportunity to collect and analyze high-frequency\ndata in real time, capturing behavioral, psychological, and physiological\nstates of children in naturalistic settings. Traditional models of child growth\nprimarily focus on physical metrics, often overlooking multidimensional aspects\nsuch as emotional, social, and cognitive development. In this paper, we\nintroduce a Bayesian artificial intelligence (AI) algorithm that leverages\ndigital phenotyping to create a Multidimensional Index of Child Growth (MICG).\nThis index integrates data from various dimensions of child development,\nincluding physical, emotional, cognitive, and environmental factors. By\nincorporating probabilistic modeling, the proposed algorithm dynamically\nupdates its learning based on data collected by the mobile app used by mothers\nand children. The app also infers uncertainty from response times, adjusting\nthe importance of each dimension of child growth accordingly. Our contribution\napplies state-of-the-art technology to track multidimensional child\ndevelopment, enabling families and healthcare providers to make more informed\ndecisions in real time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This document proposes an algorithm for a mobile application designed to\nmonitor multidimensional child growth through digital phenotyping. Digital\nphenotyping offers a unique opportunity to collect and analyze high-frequency\ndata in real time, capturing behavioral, psychological, and physiological\nstates of children in naturalistic settings. Traditional models of child growth\nprimarily focus on physical metrics, often overlooking multidimensional aspects\nsuch as emotional, social, and cognitive development. In this paper, we\nintroduce a Bayesian artificial intelligence (AI) algorithm that leverages\ndigital phenotyping to create a Multidimensional Index of Child Growth (MICG).\nThis index integrates data from various dimensions of child development,\nincluding physical, emotional, cognitive, and environmental factors. By\nincorporating probabilistic modeling, the proposed algorithm dynamically\nupdates its learning based on data collected by the mobile app used by mothers\nand children. The app also infers uncertainty from response times, adjusting\nthe importance of each dimension of child growth accordingly. Our contribution\napplies state-of-the-art technology to track multidimensional child\ndevelopment, enabling families and healthcare providers to make more informed\ndecisions in real time."
                },
                "authors": [
                    {
                        "name": "Rolando Gonzales Martinez"
                    },
                    {
                        "name": "Hinke Haisma"
                    }
                ],
                "author_detail": {
                    "name": "Hinke Haisma"
                },
                "author": "Hinke Haisma",
                "arxiv_comment": "15 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11242v2",
                "updated": "2024-12-19T10:33:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    33,
                    13,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-15T16:47:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    16,
                    47,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs"
                },
                "summary": "Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14708v1",
                "updated": "2024-12-19T10:20:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    20,
                    34,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T10:20:34Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    20,
                    34,
                    3,
                    354,
                    0
                ],
                "title": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A\n  Survey"
                },
                "summary": "Smart spaces are ubiquitous computing environments that integrate diverse\nsensing and communication technologies to enhance space functionality, optimize\nenergy utilization, and improve user comfort and well-being. The integration of\nemerging AI methodologies into these environments facilitates the formation of\nAI-driven smart spaces, which further enhance functionalities of the spaces by\nenabling advanced applications such as personalized comfort settings,\ninteractive living spaces, and automatization of the space systems, all\nresulting in enhanced indoor experiences of the users. In this paper, we\npresent a systematic survey of existing research on the foundational components\nof AI-driven smart spaces, including sensor technologies, data communication\nprotocols, sensor network management and maintenance strategies, as well as the\ndata collection, processing and analytics. Given the pivotal role of AI in\nestablishing AI-powered smart spaces, we explore the opportunities and\nchallenges associated with traditional machine learning (ML) approaches, such\nas deep learning (DL), and emerging methodologies including large language\nmodels (LLMs). Finally, we provide key insights necessary for the development\nof AI-driven smart spaces, propose future research directions, and sheds light\non the path forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart spaces are ubiquitous computing environments that integrate diverse\nsensing and communication technologies to enhance space functionality, optimize\nenergy utilization, and improve user comfort and well-being. The integration of\nemerging AI methodologies into these environments facilitates the formation of\nAI-driven smart spaces, which further enhance functionalities of the spaces by\nenabling advanced applications such as personalized comfort settings,\ninteractive living spaces, and automatization of the space systems, all\nresulting in enhanced indoor experiences of the users. In this paper, we\npresent a systematic survey of existing research on the foundational components\nof AI-driven smart spaces, including sensor technologies, data communication\nprotocols, sensor network management and maintenance strategies, as well as the\ndata collection, processing and analytics. Given the pivotal role of AI in\nestablishing AI-powered smart spaces, we explore the opportunities and\nchallenges associated with traditional machine learning (ML) approaches, such\nas deep learning (DL), and emerging methodologies including large language\nmodels (LLMs). Finally, we provide key insights necessary for the development\nof AI-driven smart spaces, propose future research directions, and sheds light\non the path forward."
                },
                "authors": [
                    {
                        "name": "Aygn Varol"
                    },
                    {
                        "name": "Naser Hossein Motlagh"
                    },
                    {
                        "name": "Mirka Leino"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    },
                    {
                        "name": "Johanna Virkki"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Virkki"
                },
                "author": "Johanna Virkki",
                "arxiv_comment": "39 pages, 3 figures, 1 table, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; C.0; I.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v3",
                "updated": "2024-12-19T10:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    11,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18551v3",
                "updated": "2024-12-19T09:55:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    55,
                    5,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-26T07:04:30Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    7,
                    4,
                    30,
                    4,
                    208,
                    0
                ],
                "title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature\n  Enhancement Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature\n  Enhancement Network"
                },
                "summary": "Trajectory prediction is crucial for autonomous driving as it aims to\nforecast the future movements of traffic participants. Traditional methods\nusually perform holistic inference on the trajectories of agents, neglecting\nthe differences in prediction difficulty among agents. This paper proposes a\nnovel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages\nthe prediction difficulty differences among agents for multi-agent trajectory\nprediction. Firstly, we employ spatio-temporal feature encoding and interaction\nto capture rich spatio-temporal features. Secondly, a difficulty-guided decoder\ncontrols the flow of future trajectories into subsequent modules, obtaining\nreliable future trajectories. Then, feature interaction and fusion are\nperformed through the future feature interaction module. Finally, the fused\nagent features are fed into the final predictor to generate the predicted\ntrajectory distributions for multiple participants. Experimental results\ndemonstrate that our DGFNet achieves state-of-the-art performance on the\nArgoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate\nthe effectiveness of each module. Moreover, compared with SOTA methods, our\nmethod balances trajectory prediction accuracy and real-time inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction is crucial for autonomous driving as it aims to\nforecast the future movements of traffic participants. Traditional methods\nusually perform holistic inference on the trajectories of agents, neglecting\nthe differences in prediction difficulty among agents. This paper proposes a\nnovel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages\nthe prediction difficulty differences among agents for multi-agent trajectory\nprediction. Firstly, we employ spatio-temporal feature encoding and interaction\nto capture rich spatio-temporal features. Secondly, a difficulty-guided decoder\ncontrols the flow of future trajectories into subsequent modules, obtaining\nreliable future trajectories. Then, feature interaction and fusion are\nperformed through the future feature interaction module. Finally, the fused\nagent features are fed into the final predictor to generate the predicted\ntrajectory distributions for multiple participants. Experimental results\ndemonstrate that our DGFNet achieves state-of-the-art performance on the\nArgoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate\nthe effectiveness of each module. Moreover, compared with SOTA methods, our\nmethod balances trajectory prediction accuracy and real-time inference speed."
                },
                "authors": [
                    {
                        "name": "Guipeng Xin"
                    },
                    {
                        "name": "Duanfeng Chu"
                    },
                    {
                        "name": "Liping Lu"
                    },
                    {
                        "name": "Zejian Deng"
                    },
                    {
                        "name": "Yuang Lu"
                    },
                    {
                        "name": "Xigang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xigang Wu"
                },
                "author": "Xigang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14692v1",
                "updated": "2024-12-19T09:51:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    51,
                    45,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:51:45Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    51,
                    45,
                    3,
                    354,
                    0
                ],
                "title": "Explicit Relational Reasoning Network for Scene Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Relational Reasoning Network for Scene Text Detection"
                },
                "summary": "Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed."
                },
                "authors": [
                    {
                        "name": "Yuchen Su"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Yongkun Du"
                    },
                    {
                        "name": "Zhilong Ji"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jinfeng Bai"
                    },
                    {
                        "name": "Xieping Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xieping Gao"
                },
                "author": "Xieping Gao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02597v2",
                "updated": "2024-12-19T09:40:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    40,
                    39,
                    3,
                    354,
                    0
                ],
                "published": "2024-09-04T10:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    28,
                    5,
                    2,
                    248,
                    0
                ],
                "title": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models"
                },
                "summary": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality."
                },
                "authors": [
                    {
                        "name": "Pujing Yang"
                    },
                    {
                        "name": "Guangyi Zhang"
                    },
                    {
                        "name": "Yunlong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yunlong Cai"
                },
                "author": "Yunlong Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14675v1",
                "updated": "2024-12-19T09:29:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    29,
                    8,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:29:08Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    29,
                    8,
                    3,
                    354,
                    0
                ],
                "title": "LLMs as mediators: Can they diagnose conflicts accurately?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as mediators: Can they diagnose conflicts accurately?"
                },
                "summary": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes."
                },
                "authors": [
                    {
                        "name": "zgecan Koak"
                    },
                    {
                        "name": "Phanish Puranam"
                    },
                    {
                        "name": "Afar Yegin"
                    }
                ],
                "author_detail": {
                    "name": "Afar Yegin"
                },
                "arxiv_affiliation": "Kadir Has University",
                "author": "Afar Yegin",
                "arxiv_comment": "27 pages, 2 appendices, 21 tables (incl appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.15204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15204v1",
                "updated": "2024-12-19T18:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks"
                },
                "summary": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "25 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15194v1",
                "updated": "2024-12-19T18:58:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    58,
                    4,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:58:04Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    58,
                    4,
                    3,
                    354,
                    0
                ],
                "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-CF: A Contamination-free Multi-task Language Understanding\n  Benchmark"
                },
                "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF."
                },
                "authors": [
                    {
                        "name": "Qihao Zhao"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Qinzheng Sun"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Qiufeng Yin"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15189v1",
                "updated": "2024-12-19T18:57:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:57:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings"
                },
                "summary": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment."
                },
                "authors": [
                    {
                        "name": "Daniel Russo"
                    },
                    {
                        "name": "Stefano Menini"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Code and data at https://github.com/drusso98/face-the-facts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v1",
                "updated": "2024-12-19T18:56:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation"
                },
                "summary": "We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15184v1",
                "updated": "2024-12-19T18:55:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    55,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:55:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    55,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning"
                },
                "summary": "The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots."
                },
                "authors": [
                    {
                        "name": "Simon Frieder"
                    },
                    {
                        "name": "Jonas Bayer"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Julius Berner"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Andrs Juhsz"
                    },
                    {
                        "name": "Fabian Ruehle"
                    },
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Timothy Gowers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Gowers"
                },
                "author": "Timothy Gowers",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15182v1",
                "updated": "2024-12-19T18:54:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    54,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    54,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning"
                },
                "summary": "Robot learning is witnessing a significant increase in the size, diversity,\nand complexity of pre-collected datasets, mirroring trends in domains such as\nnatural language processing and computer vision. Many robot learning methods\ntreat such datasets as multi-task expert data and learn a multi-task,\ngeneralist policy by training broadly across them. Notably, while these\ngeneralist policies can improve the average performance across many tasks, the\nperformance of generalist policies on any one task is often suboptimal due to\nnegative transfer between partitions of the data, compared to task-specific\nspecialist policies. In this work, we argue for the paradigm of training\npolicies during deployment given the scenarios they encounter: rather than\ndeploying pre-trained policies to unseen problems in a zero-shot manner, we\nnon-parametrically retrieve and train models directly on relevant data at test\ntime. Furthermore, we show that many robotics tasks share considerable amounts\nof low-level behaviors and that retrieval at the \"sub\"-trajectory granularity\nenables significantly improved data utilization, generalization, and robustness\nin adapting policies to novel problems. In contrast, existing full-trajectory\nretrieval methods tend to underutilize the data and miss out on shared\ncross-task content. This work proposes STRAP, a technique for leveraging\npre-trained vision foundation models and dynamic time warping to retrieve\nsub-sequences of trajectories from large training corpora in a robust fashion.\nSTRAP outperforms both prior retrieval algorithms and multi-task learning\nmethods in simulated and real experiments, showing the ability to scale to much\nlarger offline datasets in the real world as well as the ability to learn\nrobust control policies with just a handful of real-world demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot learning is witnessing a significant increase in the size, diversity,\nand complexity of pre-collected datasets, mirroring trends in domains such as\nnatural language processing and computer vision. Many robot learning methods\ntreat such datasets as multi-task expert data and learn a multi-task,\ngeneralist policy by training broadly across them. Notably, while these\ngeneralist policies can improve the average performance across many tasks, the\nperformance of generalist policies on any one task is often suboptimal due to\nnegative transfer between partitions of the data, compared to task-specific\nspecialist policies. In this work, we argue for the paradigm of training\npolicies during deployment given the scenarios they encounter: rather than\ndeploying pre-trained policies to unseen problems in a zero-shot manner, we\nnon-parametrically retrieve and train models directly on relevant data at test\ntime. Furthermore, we show that many robotics tasks share considerable amounts\nof low-level behaviors and that retrieval at the \"sub\"-trajectory granularity\nenables significantly improved data utilization, generalization, and robustness\nin adapting policies to novel problems. In contrast, existing full-trajectory\nretrieval methods tend to underutilize the data and miss out on shared\ncross-task content. This work proposes STRAP, a technique for leveraging\npre-trained vision foundation models and dynamic time warping to retrieve\nsub-sequences of trajectories from large training corpora in a robust fashion.\nSTRAP outperforms both prior retrieval algorithms and multi-task learning\nmethods in simulated and real experiments, showing the ability to scale to much\nlarger offline datasets in the real world as well as the ability to learn\nrobust control policies with just a handful of real-world demonstrations."
                },
                "authors": [
                    {
                        "name": "Marius Memmel"
                    },
                    {
                        "name": "Jacob Berg"
                    },
                    {
                        "name": "Bingqing Chen"
                    },
                    {
                        "name": "Abhishek Gupta"
                    },
                    {
                        "name": "Jonathan Francis"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Francis"
                },
                "author": "Jonathan Francis",
                "arxiv_comment": "Project website at https://weirdlabuw.github.io/strap/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15178v1",
                "updated": "2024-12-19T18:52:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    52,
                    5,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:52:05Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    52,
                    5,
                    3,
                    354,
                    0
                ],
                "title": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages"
                },
                "summary": "Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date."
                },
                "authors": [
                    {
                        "name": "Aman Chaturvedi"
                    },
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15177v1",
                "updated": "2024-12-19T18:51:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    30,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:51:30Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    30,
                    3,
                    354,
                    0
                ],
                "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative\n  Querying"
                },
                "summary": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided."
                },
                "authors": [
                    {
                        "name": "Federico Castagna"
                    },
                    {
                        "name": "Isabel Sassoon"
                    },
                    {
                        "name": "Simon Parsons"
                    }
                ],
                "author_detail": {
                    "name": "Simon Parsons"
                },
                "author": "Simon Parsons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15176v1",
                "updated": "2024-12-19T18:51:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:51:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "Rethinking Uncertainty Estimation in Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Uncertainty Estimation in Natural Language Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field."
                },
                "authors": [
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06289v3",
                "updated": "2024-12-19T18:47:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    47,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-09T08:24:11Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity"
                },
                "summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Jixuan Leng"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05199v2",
                "updated": "2024-12-19T18:46:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    46,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-07T21:51:07Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    51,
                    7,
                    3,
                    312,
                    0
                ],
                "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement"
                },
                "summary": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Tung Mai"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Saayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Saayan Mitra"
                },
                "author": "Saayan Mitra",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15156v1",
                "updated": "2024-12-19T18:32:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    32,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:32:21Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    32,
                    21,
                    3,
                    354,
                    0
                ],
                "title": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM"
                },
                "summary": "Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation."
                },
                "authors": [
                    {
                        "name": "Yatai Ji"
                    },
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Shilong Zhang"
                    },
                    {
                        "name": "Shoufa Chen"
                    },
                    {
                        "name": "Chongjian GE"
                    },
                    {
                        "name": "Peize Sun"
                    },
                    {
                        "name": "Weifeng Chen"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15151v1",
                "updated": "2024-12-19T18:28:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:28:41Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "title": "Language Models as Continuous Self-Evolving Data Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Continuous Self-Evolving Data Engineers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities."
                },
                "authors": [
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15146v1",
                "updated": "2024-12-19T18:27:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    27,
                    12,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:27:12Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    27,
                    12,
                    3,
                    354,
                    0
                ],
                "title": "Cruise Control: Dynamic Model Selection for ML-Based Network Traffic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cruise Control: Dynamic Model Selection for ML-Based Network Traffic\n  Analysis"
                },
                "summary": "Modern networks increasingly rely on machine learning models for real-time\ninsights, including traffic classification, application quality of experience\ninference, and intrusion detection. However, existing approaches prioritize\nprediction accuracy without considering deployment constraints or the dynamism\nof network traffic, leading to potentially suboptimal performance. Because of\nthis, deploying ML models in real-world networks with tight performance\nconstraints remains an open challenge. In contrast with existing work that aims\nto select an optimal candidate model for each task based on offline\ninformation, we propose an online, system-driven approach to dynamically select\nthe best ML model for network traffic analysis. To this end, we present Cruise\nControl, a system that pre-trains several models for a given task with\ndifferent accuracy-cost tradeoffs and selects the most appropriate model based\non lightweight signals representing the system's current traffic processing\nability. Experimental results using two real-world traffic analysis tasks\ndemonstrate Cruise Control's effectiveness in adapting to changing network\nconditions. Our evaluation shows that Cruise Control improves median accuracy\nby 2.78% while reducing packet loss by a factor of four compared to\noffline-selected models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern networks increasingly rely on machine learning models for real-time\ninsights, including traffic classification, application quality of experience\ninference, and intrusion detection. However, existing approaches prioritize\nprediction accuracy without considering deployment constraints or the dynamism\nof network traffic, leading to potentially suboptimal performance. Because of\nthis, deploying ML models in real-world networks with tight performance\nconstraints remains an open challenge. In contrast with existing work that aims\nto select an optimal candidate model for each task based on offline\ninformation, we propose an online, system-driven approach to dynamically select\nthe best ML model for network traffic analysis. To this end, we present Cruise\nControl, a system that pre-trains several models for a given task with\ndifferent accuracy-cost tradeoffs and selects the most appropriate model based\non lightweight signals representing the system's current traffic processing\nability. Experimental results using two real-world traffic analysis tasks\ndemonstrate Cruise Control's effectiveness in adapting to changing network\nconditions. Our evaluation shows that Cruise Control improves median accuracy\nby 2.78% while reducing packet loss by a factor of four compared to\noffline-selected models."
                },
                "authors": [
                    {
                        "name": "Johann Hugon"
                    },
                    {
                        "name": "Paul Schmitt"
                    },
                    {
                        "name": "Anthony Busson"
                    },
                    {
                        "name": "Francesco Bronzino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Bronzino"
                },
                "author": "Francesco Bronzino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15127v1",
                "updated": "2024-12-19T18:08:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    4,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T18:08:04Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    8,
                    4,
                    3,
                    354,
                    0
                ],
                "title": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness"
                },
                "summary": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Haotian Zheng"
                    },
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yushan Sun"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Yatong Han"
                    }
                ],
                "author_detail": {
                    "name": "Yatong Han"
                },
                "author": "Yatong Han",
                "arxiv_comment": "12 pages, 6 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11917v3",
                "updated": "2024-12-19T17:57:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    57,
                    59,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-16T16:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "title": "Does VLM Classification Benefit from LLM Description Semantics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does VLM Classification Benefit from LLM Description Semantics?"
                },
                "summary": "Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs."
                },
                "authors": [
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Lennart Rietdorf"
                    },
                    {
                        "name": "Dmytro Kotovenko"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "arxiv_comment": "AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15115v1",
                "updated": "2024-12-19T17:56:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "title": "Qwen2.5 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5 Technical Report"
                },
                "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models."
                },
                "authors": [
                    {
                        "name": "Qwen"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "arxiv_affiliation": "additional authors not shown",
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15113v1",
                "updated": "2024-12-19T17:55:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture"
                },
                "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale."
                },
                "authors": [
                    {
                        "name": "Thomas F Burns"
                    },
                    {
                        "name": "Tomoki Fukai"
                    },
                    {
                        "name": "Christopher J Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J Earls"
                },
                "author": "Christopher J Earls",
                "arxiv_comment": "18 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T01, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5; I.7; J.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05068v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05068v3",
                "updated": "2024-12-19T17:48:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-09-08T12:10:13Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    10,
                    13,
                    6,
                    252,
                    0
                ],
                "title": "Improving Early Detection of Gravitational Waves from Binary Neutron\n  Stars Using CNNs and FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Early Detection of Gravitational Waves from Binary Neutron\n  Stars Using CNNs and FPGAs"
                },
                "summary": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines."
                },
                "authors": [
                    {
                        "name": "Ana Martins"
                    },
                    {
                        "name": "Melissa Lopez"
                    },
                    {
                        "name": "Quirijn Meijer"
                    },
                    {
                        "name": "Gregory Baltus"
                    },
                    {
                        "name": "Marc van der Sluys"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Caudill"
                },
                "author": "Sarah Caudill",
                "arxiv_comment": "22 pages, 7 figures, 3 tables, submitted to Machine Learning Science\n  and Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05068v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15101v1",
                "updated": "2024-12-19T17:48:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:48:23Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    23,
                    3,
                    354,
                    0
                ],
                "title": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering\n  with Temporal Adaptability"
                },
                "summary": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising\nsolution to multi-hop question answering(QA) tasks since it enables large\nlanguage models (LLMs) to incorporate external knowledge and mitigate their\ninherent knowledge deficiencies. Despite this progress, existing RAG\nframeworks, which usually follows the retrieve-then-read paradigm, often\nstruggle with multi-hop QA with temporal information since it has difficulty\nretrieving and synthesizing accurate time-related information. To address the\nchallenge, this paper proposes a novel framework called review-then-refine,\nwhich aims to enhance LLM performance in multi-hop QA scenarios with temporal\ninformation. Our approach begins with a review phase, where decomposed\nsub-queries are dynamically rewritten with temporal information, allowing for\nsubsequent adaptive retrieval and reasoning process. In addition, we implement\nadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing\nthe potential for hallucinations. In the subsequent refine phase, the LLM\nsynthesizes the retrieved information from each sub-query along with its\ninternal knowledge to formulate a coherent answer. Extensive experimental\nresults across multiple datasets demonstrate the effectiveness of our proposed\nframework, highlighting its potential to significantly improve multi-hop QA\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Xiangsen Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15093v1",
                "updated": "2024-12-19T17:43:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    43,
                    27,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:43:27Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    43,
                    27,
                    3,
                    354,
                    0
                ],
                "title": "Nano-ESG: Extracting Corporate Sustainability Information from News\n  Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nano-ESG: Extracting Corporate Sustainability Information from News\n  Articles"
                },
                "summary": "Determining the sustainability impact of companies is a highly complex\nsubject which has garnered more and more attention over the past few years.\nToday, investors largely rely on sustainability-ratings from established\nrating-providers in order to analyze how responsibly a company acts. However,\nthose ratings have recently been criticized for being hard to understand and\nnearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of\ncompanies lies in the rich landscape of news article data. In this paper, we\nexplore a different approach to identify key opportunities and challenges of\ncompanies in the sustainability domain. We present a novel dataset of more than\n840,000 news articles which were gathered for major German companies between\nJanuary 2023 and September 2024. By applying a mixture of Natural Language\nProcessing techniques, we first identify relevant articles, before summarizing\nthem and extracting their sustainability-related sentiment and aspect using\nLarge Language Models (LLMs). Furthermore, we conduct an evaluation of the\nobtained data and determine that the LLM-produced answers are accurate. We\nrelease both datasets at https://github.com/Bailefan/Nano-ESG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the sustainability impact of companies is a highly complex\nsubject which has garnered more and more attention over the past few years.\nToday, investors largely rely on sustainability-ratings from established\nrating-providers in order to analyze how responsibly a company acts. However,\nthose ratings have recently been criticized for being hard to understand and\nnearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of\ncompanies lies in the rich landscape of news article data. In this paper, we\nexplore a different approach to identify key opportunities and challenges of\ncompanies in the sustainability domain. We present a novel dataset of more than\n840,000 news articles which were gathered for major German companies between\nJanuary 2023 and September 2024. By applying a mixture of Natural Language\nProcessing techniques, we first identify relevant articles, before summarizing\nthem and extracting their sustainability-related sentiment and aspect using\nLarge Language Models (LLMs). Furthermore, we conduct an evaluation of the\nobtained data and determine that the LLM-produced answers are accurate. We\nrelease both datasets at https://github.com/Bailefan/Nano-ESG."
                },
                "authors": [
                    {
                        "name": "Fabian Billert"
                    },
                    {
                        "name": "Stefan Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Conrad"
                },
                "author": "Stefan Conrad",
                "arxiv_comment": "To be published at ECIR 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15072v1",
                "updated": "2024-12-19T17:22:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    22,
                    35,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:22:35Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    22,
                    35,
                    3,
                    354,
                    0
                ],
                "title": "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social\n  Media via Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social\n  Media via Chatbots"
                },
                "summary": "Social media platforms have become the hubs for various user interactions\ncovering a wide range of needs, including technical support and services\nrelated to brands, products, or user accounts. Unfortunately, there has been a\nrecent surge in scammers impersonating official services and providing fake\ntechnical support to users through these platforms. In this study, we focus on\nscammers engaging in such fake technical support to target users who are having\nproblems recovering their accounts. More specifically, we focus on users\nencountering access problems with social media profiles (e.g., on platforms\nsuch as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main\ncontribution of our work is the development of an automated system that\ninteracts with scammers via a chatbot that mimics different personas. By\ninitiating decoy interactions (e.g., through deceptive tweets), we have enticed\nscammers to interact with our system so that we can analyze their modus\noperandi. Our results show that scammers employ many social media profiles\nasking users to contact them via a few communication channels. Using a large\nlanguage model (LLM), our chatbot had conversations with 450 scammers and\nprovided valuable insights into their tactics and, most importantly, their\npayment profiles. This automated approach highlights how scammers use a variety\nof strategies, including role-playing, to trick victims into disclosing\npersonal or financial information. With this study, we lay the foundation for\nusing automated chat-based interactions with scammers to detect and study\nfraudulent activities at scale in an automated way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms have become the hubs for various user interactions\ncovering a wide range of needs, including technical support and services\nrelated to brands, products, or user accounts. Unfortunately, there has been a\nrecent surge in scammers impersonating official services and providing fake\ntechnical support to users through these platforms. In this study, we focus on\nscammers engaging in such fake technical support to target users who are having\nproblems recovering their accounts. More specifically, we focus on users\nencountering access problems with social media profiles (e.g., on platforms\nsuch as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main\ncontribution of our work is the development of an automated system that\ninteracts with scammers via a chatbot that mimics different personas. By\ninitiating decoy interactions (e.g., through deceptive tweets), we have enticed\nscammers to interact with our system so that we can analyze their modus\noperandi. Our results show that scammers employ many social media profiles\nasking users to contact them via a few communication channels. Using a large\nlanguage model (LLM), our chatbot had conversations with 450 scammers and\nprovided valuable insights into their tactics and, most importantly, their\npayment profiles. This automated approach highlights how scammers use a variety\nof strategies, including role-playing, to trick victims into disclosing\npersonal or financial information. With this study, we lay the foundation for\nusing automated chat-based interactions with scammers to detect and study\nfraudulent activities at scale in an automated way."
                },
                "authors": [
                    {
                        "name": "Bhupendra Acharya"
                    },
                    {
                        "name": "Dominik Sautter"
                    },
                    {
                        "name": "Muhammad Saad"
                    },
                    {
                        "name": "Thorsten Holz"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Holz"
                },
                "author": "Thorsten Holz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15060v1",
                "updated": "2024-12-19T17:08:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    8,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T17:08:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    8,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "ConfliBERT: A Language Model for Political Conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfliBERT: A Language Model for Political Conflict"
                },
                "summary": "Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conflict scholars have used rule-based approaches to extract information\nabout political violence from news reports and texts. Recent Natural Language\nProcessing developments move beyond rigid rule-based approaches. We review our\nrecent ConfliBERT language model (Hu et al. 2022) to process political and\nviolence related texts. The model can be used to extract actor and action\nclassifications from texts about political conflict. When fine-tuned, results\nshow that ConfliBERT has superior performance in accuracy, precision and recall\nover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama\n3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also\nhundreds of times faster than these more generalist LLMs. These results are\nillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset\n(GTD)."
                },
                "authors": [
                    {
                        "name": "Patrick T. Brandt"
                    },
                    {
                        "name": "Sultan Alsarra"
                    },
                    {
                        "name": "Vito J. D`Orazio"
                    },
                    {
                        "name": "Dagmar Heintze"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Shreyas Meher"
                    },
                    {
                        "name": "Javier Osorio"
                    },
                    {
                        "name": "Marcus Sianan"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Sianan"
                },
                "author": "Marcus Sianan",
                "arxiv_comment": "30 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v1",
                "updated": "2024-12-19T16:46:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14512v3",
                "updated": "2024-12-19T16:37:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    37,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-25T04:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    32,
                    45,
                    6,
                    238,
                    0
                ],
                "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings"
                },
                "summary": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
                },
                "authors": [
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Fengzhi Li"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10868v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10868v4",
                "updated": "2024-12-19T16:22:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    22,
                    7,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-16T09:36:32Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    36,
                    32,
                    6,
                    168,
                    0
                ],
                "title": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Query-Relevant Neurons in Large Language Models for\n  Long-Form Texts"
                },
                "summary": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess vast amounts of knowledge within their\nparameters, prompting research into methods for locating and editing this\nknowledge. Previous work has largely focused on locating entity-related (often\nsingle-token) facts in smaller models. However, several key questions remain\nunanswered: (1) How can we effectively locate query-relevant neurons in\ndecoder-only LLMs, such as Llama and Mistral? (2) How can we address the\nchallenge of long-form (or free-form) text generation? (3) Are there localized\nknowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron\nCluster Attribution (QRNCA), a novel architecture-agnostic framework capable of\nidentifying query-relevant neurons in LLMs. QRNCA allows for the examination of\nlong-form answers beyond triplet facts by employing the proxy task of\nmulti-choice question answering. To evaluate the effectiveness of our detected\nneurons, we build two multi-choice QA datasets spanning diverse domains and\nlanguages. Empirical evaluations demonstrate that our method outperforms\nbaseline methods significantly. Further, analysis of neuron distributions\nreveals the presence of visible localized regions, particularly within\ndifferent domains. Finally, we show potential applications of our detected\nneurons in knowledge editing and neuron-based prediction."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Adam Dejl"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "AAAI 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10868v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10868v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15004v1",
                "updated": "2024-12-19T16:20:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:20:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Large Language Models and Code Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Code Security: A Systematic Literature Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks."
                },
                "authors": [
                    {
                        "name": "Enna Basic"
                    },
                    {
                        "name": "Alberto Giaretta"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Giaretta"
                },
                "author": "Alberto Giaretta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08406v2",
                "updated": "2024-12-19T16:09:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    9,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-09-12T21:39:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    39,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tagging with Large Language Model based Multi-Agent System"
                },
                "summary": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Ethan Chang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14995v1",
                "updated": "2024-12-19T16:07:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    7,
                    0,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:07:00Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    7,
                    0,
                    3,
                    354,
                    0
                ],
                "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven\n  Harmony Search and Genetic Algorithm Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven\n  Harmony Search and Genetic Algorithm Using LLMs"
                },
                "summary": "Automatic Heuristic Design (AHD) is an active research area due to its\nutility in solving complex search and NP-hard combinatorial optimization\nproblems in the real world. The recent advancements in Large Language Models\n(LLMs) introduce new possibilities by coupling LLMs with evolutionary\ncomputation to automatically generate heuristics, known as LLM-based\nEvolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained\ngreat performance on various tasks, there is still a gap in understanding the\nproperties of heuristic search spaces and achieving a balance between\nexploration and exploitation, which is a critical factor in large heuristic\nsearch spaces. In this study, we address this gap by proposing two diversity\nmeasurement metrics and perform an analysis on previous LLM-EPS approaches,\nincluding FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal\nthat while EoH demonstrates higher diversity than FunSearch and ReEvo, its\nobjective score is unstable. Conversely, ReEvo's reflection mechanism yields\ngood objective scores but fails to optimize diversity effectively. With this\nfinding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that\nmaintains a balance between diversity and convergence with a harmony search\nalgorithm. Through experimentation, we find that HSEvo achieved high diversity\nindices and good objective scores while remaining cost-effective. These results\nunderscore the importance of balancing exploration and exploitation and\nunderstanding heuristic search spaces in designing frameworks in LLM-EPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Heuristic Design (AHD) is an active research area due to its\nutility in solving complex search and NP-hard combinatorial optimization\nproblems in the real world. The recent advancements in Large Language Models\n(LLMs) introduce new possibilities by coupling LLMs with evolutionary\ncomputation to automatically generate heuristics, known as LLM-based\nEvolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained\ngreat performance on various tasks, there is still a gap in understanding the\nproperties of heuristic search spaces and achieving a balance between\nexploration and exploitation, which is a critical factor in large heuristic\nsearch spaces. In this study, we address this gap by proposing two diversity\nmeasurement metrics and perform an analysis on previous LLM-EPS approaches,\nincluding FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal\nthat while EoH demonstrates higher diversity than FunSearch and ReEvo, its\nobjective score is unstable. Conversely, ReEvo's reflection mechanism yields\ngood objective scores but fails to optimize diversity effectively. With this\nfinding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that\nmaintains a balance between diversity and convergence with a harmony search\nalgorithm. Through experimentation, we find that HSEvo achieved high diversity\nindices and good objective scores while remaining cost-effective. These results\nunderscore the importance of balancing exploration and exploitation and\nunderstanding heuristic search spaces in designing frameworks in LLM-EPS."
                },
                "authors": [
                    {
                        "name": "Pham Vu Tuan Dat"
                    },
                    {
                        "name": "Long Doan"
                    },
                    {
                        "name": "Huynh Thi Thanh Binh"
                    }
                ],
                "author_detail": {
                    "name": "Huynh Thi Thanh Binh"
                },
                "author": "Huynh Thi Thanh Binh",
                "arxiv_comment": "18 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14989v1",
                "updated": "2024-12-19T16:00:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    0,
                    43,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T16:00:43Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    0,
                    43,
                    3,
                    354,
                    0
                ],
                "title": "RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots\n  using Foundation Models for Perception and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots\n  using Foundation Models for Perception and Planning"
                },
                "summary": "We present the approaches and contributions of the winning team NimbRo@Home\nat the RoboCup@Home 2024 competition in the Open Platform League held in\nEindhoven, NL. Further, we describe our hardware setup and give an overview of\nthe results for the task stages and the final demonstration. For this year's\ncompetition, we put a special emphasis on open-vocabulary object segmentation\nand grasping approaches that overcome the labeling overhead of supervised\nvision approaches, commonly used in RoboCup@Home. We successfully demonstrated\nthat we can segment and grasp non-labeled objects by text descriptions.\nFurther, we extensively employed LLMs for natural language understanding and\ntask planning. Throughout the competition, our approaches showed robustness and\ngeneralization capabilities. A video of our performance can be found online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the approaches and contributions of the winning team NimbRo@Home\nat the RoboCup@Home 2024 competition in the Open Platform League held in\nEindhoven, NL. Further, we describe our hardware setup and give an overview of\nthe results for the task stages and the final demonstration. For this year's\ncompetition, we put a special emphasis on open-vocabulary object segmentation\nand grasping approaches that overcome the labeling overhead of supervised\nvision approaches, commonly used in RoboCup@Home. We successfully demonstrated\nthat we can segment and grasp non-labeled objects by text descriptions.\nFurther, we extensively employed LLMs for natural language understanding and\ntask planning. Throughout the competition, our approaches showed robustness and\ngeneralization capabilities. A video of our performance can be found online."
                },
                "authors": [
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Jan Nogga"
                    },
                    {
                        "name": "Bastian Ptzold"
                    },
                    {
                        "name": "Evgenii Kruzhkov"
                    },
                    {
                        "name": "Simon Bultmann"
                    },
                    {
                        "name": "Michael Schreiber"
                    },
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bertan Karacora"
                    },
                    {
                        "name": "Juhui Park"
                    },
                    {
                        "name": "Alena Savinykh"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "12 pages, 8 figures, RoboCup 2024 Champion Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14986v1",
                "updated": "2024-12-19T15:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    58,
                    53,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:58:53Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    58,
                    53,
                    3,
                    354,
                    0
                ],
                "title": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small\n  Language Models Write Young Students Texts"
                },
                "summary": "Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been used to generate texts in response to\ndifferent writing tasks: reports, essays, story telling. However, language\nmodels do not have a meta-representation of the text writing process, nor\ninherent communication learning needs, comparable to those of young human\nstudents. This paper introduces a fine-grained linguistic and textual analysis\nof multilingual Small Language Models' (SLMs) writing. With our method,\nChain-of-MetaWriting, SLMs can imitate some steps of the human writing process,\nsuch as planning and evaluation. We mainly focused on short story and essay\nwriting tasks in French for schoolchildren and undergraduate students\nrespectively. Our results show that SLMs encounter difficulties in assisting\nyoung students on sensitive topics such as violence in the schoolyard, and they\nsometimes use words too complex for the target audience. In particular, the\noutput is quite different from the human produced texts in term of text\ncohesion and coherence regarding temporal connectors, topic progression,\nreference."
                },
                "authors": [
                    {
                        "name": "Ioana Buhnila"
                    },
                    {
                        "name": "Georgeta Cislaru"
                    },
                    {
                        "name": "Amalia Todirascu"
                    }
                ],
                "author_detail": {
                    "name": "Amalia Todirascu"
                },
                "author": "Amalia Todirascu",
                "arxiv_comment": "Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,\n  Cognitive Science, and NLP) co-located with COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13765v2",
                "updated": "2024-12-19T15:50:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    50,
                    54,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-18T12:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    1,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms"
                },
                "summary": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama."
                },
                "authors": [
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ahmed Abdelmoneim Mazrou"
                    },
                    {
                        "name": "Mohamed Shaltout"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Shaltout"
                },
                "author": "Mohamed Shaltout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14964v1",
                "updated": "2024-12-19T15:44:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    44,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:44:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    44,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "Knowledge Injection via Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Injection via Prompt Distillation"
                },
                "summary": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights."
                },
                "authors": [
                    {
                        "name": "Kalle Kujanp"
                    },
                    {
                        "name": "Harri Valpola"
                    },
                    {
                        "name": "Alexander Ilin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Ilin"
                },
                "author": "Alexander Ilin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14959v1",
                "updated": "2024-12-19T15:39:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    39,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:39:31Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    39,
                    31,
                    3,
                    354,
                    0
                ],
                "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction"
                },
                "summary": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/."
                },
                "authors": [
                    {
                        "name": "Qingjie Zhang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Haoting Qian"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10839v2",
                "updated": "2024-12-19T15:25:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    25,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-20T13:34:17Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    34,
                    17,
                    1,
                    233,
                    0
                ],
                "title": "Benchmarking Large Language Models for Math Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Math Reasoning Tasks"
                },
                "summary": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research."
                },
                "authors": [
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Yao Rong"
                    },
                    {
                        "name": "Emek Gzlkl"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v4",
                "updated": "2024-12-19T15:16:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    16,
                    49,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.11947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.11947v2",
                "updated": "2024-12-19T15:13:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    13,
                    46,
                    3,
                    354,
                    0
                ],
                "published": "2023-02-23T11:44:43Z",
                "published_parsed": [
                    2023,
                    2,
                    23,
                    11,
                    44,
                    43,
                    3,
                    54,
                    0
                ],
                "title": "Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks"
                },
                "summary": "The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing."
                },
                "authors": [
                    {
                        "name": "Tuomas Jalonen"
                    },
                    {
                        "name": "Mohammad Al-Sa'd"
                    },
                    {
                        "name": "Roope Mellanen"
                    },
                    {
                        "name": "Serkan Kiranyaz"
                    },
                    {
                        "name": "Moncef Gabbouj"
                    }
                ],
                "author_detail": {
                    "name": "Moncef Gabbouj"
                },
                "author": "Moncef Gabbouj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.11947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.11947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04693v2",
                "updated": "2024-12-19T15:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    11,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-05T17:56:38Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    17,
                    56,
                    38,
                    4,
                    187,
                    0
                ],
                "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted by NeurIPS 2024. Dataset, code, and model are released at\n  https://github.com/open-compass/ANAH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16300v3",
                "updated": "2024-12-19T15:11:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    11,
                    46,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-25T11:35:08Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    35,
                    8,
                    0,
                    330,
                    0
                ],
                "title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment"
                },
                "summary": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Kehao Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09423v3",
                "updated": "2024-12-19T15:10:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    10,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2023-07-18T16:43:03Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    16,
                    43,
                    3,
                    1,
                    199,
                    0
                ],
                "title": "Scaling Laws for Imitation Learning in Single-Agent Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Imitation Learning in Single-Agent Games"
                },
                "summary": "Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems."
                },
                "authors": [
                    {
                        "name": "Jens Tuyls"
                    },
                    {
                        "name": "Dhruv Madeka"
                    },
                    {
                        "name": "Kari Torkkola"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Sham Kakade"
                    }
                ],
                "author_detail": {
                    "name": "Sham Kakade"
                },
                "author": "Sham Kakade",
                "arxiv_comment": "Accepted at TMLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v5",
                "updated": "2024-12-19T15:07:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    7,
                    38,
                    3,
                    354,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14922v1",
                "updated": "2024-12-19T15:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    0,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T15:00:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    15,
                    0,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under\n  Noisy Response"
                },
                "summary": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\nempirical experiments, the collected data inevitably contains noise in\npractical applications, which poses significant challenges to model performance\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\nframework to enhance model capabilities in downstream tasks. To address this\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\ndetection and relabeling on downstream task data. For noise identification, our\napproach employs a multi-expert collaborative system with inference-enhanced\nmodels to achieve superior noise detection. In the denoising phase, we utilize\na context-enhanced strategy, which incorporates the most relevant and confident\nknowledge followed by careful assessment to generate reliable annotations.\nAdditionally, we introduce an effective data selection mechanism based on\nresponse entropy, ensuring only high-quality samples are retained for\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14905v1",
                "updated": "2024-12-19T14:37:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    37,
                    11,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T14:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    37,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dehallucinating Parallel Context Extension for Retrieval-Augmented\n  Generation"
                },
                "summary": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks."
                },
                "authors": [
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Shengnan An"
                    },
                    {
                        "name": "Zeqi Lin"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Bing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xie"
                },
                "author": "Bing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00467v3",
                "updated": "2024-12-19T14:29:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    29,
                    31,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-01T07:49:24Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    49,
                    24,
                    1,
                    275,
                    0
                ],
                "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Planning for LLM-based Graphical User Interface Automation"
                },
                "summary": "The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT."
                },
                "authors": [
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14867v1",
                "updated": "2024-12-19T14:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    3,
                    22,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T14:03:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    14,
                    3,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Convolutional Networks: Named Entity Recognition and Large\n  Language Model Embedding in Document Clustering"
                },
                "summary": "Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in machine learning, particularly Large Language Models\n(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve\ntext representation. However, current document clustering approaches often\nignore the deeper relationships between named entities (NEs) and the potential\nof LLM embeddings. This paper proposes a novel approach that integrates Named\nEntity Recognition (NER) and LLM embeddings within a graph-based framework for\ndocument clustering. The method builds a graph with nodes representing\ndocuments and edges weighted by named entity similarity, optimized using a\ngraph-convolutional network (GCN). This ensures a more effective grouping of\nsemantically related documents. Experimental results indicate that our approach\noutperforms conventional co-occurrence-based methods in clustering, notably for\ndocuments rich in named entities."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14860v1",
                "updated": "2024-12-19T13:55:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling"
                },
                "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reflect on the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Models to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14849v1",
                "updated": "2024-12-19T13:39:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:39:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    39,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for\n  Few-Shot Aspect-Based Sentiment Analysis"
                },
                "summary": "Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently developed large language models (LLMs) have presented promising new\navenues to address data scarcity in low-resource scenarios. In few-shot\naspect-based sentiment analysis (ABSA), previous efforts have explored data\naugmentation techniques, which prompt LLMs to generate new samples by modifying\nexisting ones. However, these methods fail to produce adequately diverse data,\nimpairing their effectiveness. Besides, some studies apply in-context learning\nfor ABSA by using specific instructions and a few selected examples as prompts.\nThough promising, LLMs often yield labels that deviate from task requirements.\nTo overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data\nsynthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize\ndata from two complementary perspectives: \\textit{key-point-driven} and\n\\textit{instance-driven}, which effectively generate diverse and high-quality\nABSA samples in low-resource settings. Furthermore, a \\textit{label refinement}\nmodule is integrated to improve the synthetic labels. Extensive experiments\ndemonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA\nsolutions and other LLM-oriented data generation methods."
                },
                "authors": [
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14843v1",
                "updated": "2024-12-19T13:36:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    36,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:36:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    36,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas"
                },
                "summary": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training."
                },
                "authors": [
                    {
                        "name": "Pietro Bernardelle"
                    },
                    {
                        "name": "Leon Frhling"
                    },
                    {
                        "name": "Stefano Civelli"
                    },
                    {
                        "name": "Riccardo Lunardi"
                    },
                    {
                        "name": "Kevin Roiter"
                    },
                    {
                        "name": "Gianluca Demartini"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Demartini"
                },
                "author": "Gianluca Demartini",
                "arxiv_comment": "4 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14841v1",
                "updated": "2024-12-19T13:34:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:34:14Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis"
                },
                "summary": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools."
                },
                "authors": [
                    {
                        "name": "Greta Dolcetti"
                    },
                    {
                        "name": "Vincenzo Arceri"
                    },
                    {
                        "name": "Eleonora Iotti"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Agostino Cortesi"
                    },
                    {
                        "name": "Enea Zaffanella"
                    }
                ],
                "author_detail": {
                    "name": "Enea Zaffanella"
                },
                "author": "Enea Zaffanella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14837v1",
                "updated": "2024-12-19T13:27:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    27,
                    58,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:27:58Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    27,
                    58,
                    3,
                    354,
                    0
                ],
                "title": "ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects"
                },
                "summary": "3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models."
                },
                "authors": [
                    {
                        "name": "Qihang Cao"
                    },
                    {
                        "name": "Huangxun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huangxun Chen"
                },
                "author": "Huangxun Chen",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14821v1",
                "updated": "2024-12-19T13:12:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    12,
                    15,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:12:15Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    12,
                    15,
                    3,
                    354,
                    0
                ],
                "title": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation"
                },
                "summary": "Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}"
                },
                "authors": [
                    {
                        "name": "Shoumeng Qiu"
                    },
                    {
                        "name": "Xinrun Li"
                    },
                    {
                        "name": "XiangYang Xue"
                    },
                    {
                        "name": "Jian Pu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Pu"
                },
                "author": "Jian Pu",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14814v1",
                "updated": "2024-12-19T13:09:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:09:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    9,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "Answer Set Networks: Casting Answer Set Programming into Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer Set Networks: Casting Answer Set Programming into Deep Learning"
                },
                "summary": "Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments."
                },
                "authors": [
                    {
                        "name": "Arseny Skryagin"
                    },
                    {
                        "name": "Daniel Ochs"
                    },
                    {
                        "name": "Phillip Deibert"
                    },
                    {
                        "name": "Simon Kohaut"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T37, 68T30, 68T27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14809v1",
                "updated": "2024-12-19T12:57:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:57:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zeao Tu"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14793v1",
                "updated": "2024-12-19T12:35:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    35,
                    36,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:35:36Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    35,
                    36,
                    3,
                    354,
                    0
                ],
                "title": "DCL-Sparse: Distributed Range-only Cooperative Localization of\n  Multi-Robots in Noisy and Sparse Sensing Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCL-Sparse: Distributed Range-only Cooperative Localization of\n  Multi-Robots in Noisy and Sparse Sensing Graphs"
                },
                "summary": "This paper presents a novel approach to range-based cooperative localization\nfor robot swarms in GPS-denied environments, addressing the limitations of\ncurrent methods in noisy and sparse settings. We propose a robust multi-layered\nlocalization framework that combines shadow edge localization techniques with\nthe strategic deployment of UAVs. This approach not only addresses the\nchallenges associated with nonrigid and poorly connected graphs but also\nenhances the convergence rate of the localization process. We introduce two key\nconcepts: the S1-Edge approach in our distributed protocol to address the\nrigidity problem of sparse graphs and the concept of a powerful UAV node to\nincrease the sensing and localization capability of the multi-robot system. Our\napproach leverages the advantages of the distributed localization methods,\nenhancing scalability and adaptability in large robot networks. We establish\ntheoretical conditions for the new S1-Edge that ensure solutions exist even in\nthe presence of noise, thereby validating the effectiveness of shadow edge\nlocalization. Extensive simulation experiments confirm the superior performance\nof our method compared to state-of-the-art techniques, resulting in up to 95\\%\nreduction in localization error, demonstrating substantial improvements in\nlocalization accuracy and robustness to sparse graphs. This work provides a\ndecisive advancement in the field of multi-robot localization, offering a\npowerful tool for high-performance and reliable operations in challenging\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to range-based cooperative localization\nfor robot swarms in GPS-denied environments, addressing the limitations of\ncurrent methods in noisy and sparse settings. We propose a robust multi-layered\nlocalization framework that combines shadow edge localization techniques with\nthe strategic deployment of UAVs. This approach not only addresses the\nchallenges associated with nonrigid and poorly connected graphs but also\nenhances the convergence rate of the localization process. We introduce two key\nconcepts: the S1-Edge approach in our distributed protocol to address the\nrigidity problem of sparse graphs and the concept of a powerful UAV node to\nincrease the sensing and localization capability of the multi-robot system. Our\napproach leverages the advantages of the distributed localization methods,\nenhancing scalability and adaptability in large robot networks. We establish\ntheoretical conditions for the new S1-Edge that ensure solutions exist even in\nthe presence of noise, thereby validating the effectiveness of shadow edge\nlocalization. Extensive simulation experiments confirm the superior performance\nof our method compared to state-of-the-art techniques, resulting in up to 95\\%\nreduction in localization error, demonstrating substantial improvements in\nlocalization accuracy and robustness to sparse graphs. This work provides a\ndecisive advancement in the field of multi-robot localization, offering a\npowerful tool for high-performance and reliable operations in challenging\nenvironments."
                },
                "authors": [
                    {
                        "name": "Atharva Sagale"
                    },
                    {
                        "name": "Tohid Kargar Tasooji"
                    },
                    {
                        "name": "Ramviyas Parasuraman"
                    }
                ],
                "author_detail": {
                    "name": "Ramviyas Parasuraman"
                },
                "author": "Ramviyas Parasuraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14780v1",
                "updated": "2024-12-19T12:06:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    6,
                    24,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T12:06:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    6,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model\n  Fine-tuning"
                },
                "summary": "When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When using agent-task datasets to enhance agent capabilities for Large\nLanguage Models (LLMs), current methodologies often treat all tokens within a\nsample equally. However, we argue that tokens serving different roles -\nspecifically, reasoning tokens versus boilerplate tokens (e.g., those governing\noutput format) - differ significantly in importance and learning complexity,\nnecessitating their disentanglement and distinct treatment. To address this, we\npropose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token\ndiscrimination. SHAD classifies tokens by exploiting predictability differences\nobserved after shuffling input-output combinations across samples: boilerplate\ntokens, due to their repetitive nature among samples, maintain predictability,\nwhereas reasoning tokens do not. Using SHAD, we propose the\nReasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes\nreasoning tokens during fine-tuning, yielding notable performance gains over\ncommon Supervised Fine-Tuning (SFT)."
                },
                "authors": [
                    {
                        "name": "Ziang Ye"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Jianxin Ma"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14775v1",
                "updated": "2024-12-19T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "Energy and polarization based on-line interference mitigation in radio\n  interferometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy and polarization based on-line interference mitigation in radio\n  interferometry"
                },
                "summary": "Radio frequency interference (RFI) is a persistent contaminant in terrestrial\nradio astronomy. While new radio interferometers are becoming operational,\nnovel sources of RFI are also emerging. In order to strengthen the mitigation\nof RFI in modern radio interferometers, we propose an on-line RFI mitigation\nscheme that can be run in the correlator of such interferometers. We combine\nstatistics based on the energy as well as the polarization alignment of the\ncorrelated signal to develop an on-line RFI mitigation scheme that can be\napplied to a data stream produced by the correlator in real-time, especially\ntargeted at low duty-cycle or transient RFI detection. In order to improve the\ncomputational efficiency, we explore the use of both single precision and half\nprecision floating point operations in implementing the RFI mitigation\nalgorithm. This ideally suits its deployment in accelerator computing devices\nsuch as graphics processing units (GPUs) as used by the LOFAR correlator. We\nprovide results based on real data to demonstrate the efficacy of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio frequency interference (RFI) is a persistent contaminant in terrestrial\nradio astronomy. While new radio interferometers are becoming operational,\nnovel sources of RFI are also emerging. In order to strengthen the mitigation\nof RFI in modern radio interferometers, we propose an on-line RFI mitigation\nscheme that can be run in the correlator of such interferometers. We combine\nstatistics based on the energy as well as the polarization alignment of the\ncorrelated signal to develop an on-line RFI mitigation scheme that can be\napplied to a data stream produced by the correlator in real-time, especially\ntargeted at low duty-cycle or transient RFI detection. In order to improve the\ncomputational efficiency, we explore the use of both single precision and half\nprecision floating point operations in implementing the RFI mitigation\nalgorithm. This ideally suits its deployment in accelerator computing devices\nsuch as graphics processing units (GPUs) as used by the LOFAR correlator. We\nprovide results based on real data to demonstrate the efficacy of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Sarod Yatawatta"
                    },
                    {
                        "name": "Albert-Jan Boonstra"
                    },
                    {
                        "name": "Chris P. Broekema"
                    }
                ],
                "author_detail": {
                    "name": "Chris P. Broekema"
                },
                "author": "Chris P. Broekema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14771v1",
                "updated": "2024-12-19T11:55:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    55,
                    51,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:55:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    55,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Rabee Qasem"
                    },
                    {
                        "name": "Mohannad Hendi"
                    },
                    {
                        "name": "Banan Tantour"
                    }
                ],
                "author_detail": {
                    "name": "Banan Tantour"
                },
                "author": "Banan Tantour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14764v1",
                "updated": "2024-12-19T11:48:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    48,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:48:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    48,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question\n  Answering"
                },
                "summary": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically\ndesigned for evaluating repository-level question-answering capabilities in the\nfield of software engineering. CodeRepoQA encompasses five programming\nlanguages and covers a wide range of scenarios, enabling comprehensive\nevaluation of language models. To construct this dataset, we crawl data from 30\nwell-known repositories in GitHub, the largest platform for hosting and\ncollaborating on code, and carefully filter raw data. In total, CodeRepoQA is a\nmulti-turn question-answering benchmark with 585,687 entries, covering a\ndiverse array of software engineering scenarios, with an average of 6.62\ndialogue turns per entry.\n  We evaluate ten popular large language models on our dataset and provide\nin-depth analysis. We find that LLMs still have limitations in\nquestion-answering capabilities in the field of software engineering, and\nmedium-length contexts are more conducive to LLMs' performance. The entire\nbenchmark is publicly available at\nhttps://github.com/kinesiatricssxilm14/CodeRepoQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically\ndesigned for evaluating repository-level question-answering capabilities in the\nfield of software engineering. CodeRepoQA encompasses five programming\nlanguages and covers a wide range of scenarios, enabling comprehensive\nevaluation of language models. To construct this dataset, we crawl data from 30\nwell-known repositories in GitHub, the largest platform for hosting and\ncollaborating on code, and carefully filter raw data. In total, CodeRepoQA is a\nmulti-turn question-answering benchmark with 585,687 entries, covering a\ndiverse array of software engineering scenarios, with an average of 6.62\ndialogue turns per entry.\n  We evaluate ten popular large language models on our dataset and provide\nin-depth analysis. We find that LLMs still have limitations in\nquestion-answering capabilities in the field of software engineering, and\nmedium-length contexts are more conducive to LLMs' performance. The entire\nbenchmark is publicly available at\nhttps://github.com/kinesiatricssxilm14/CodeRepoQA."
                },
                "authors": [
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Qinyun Wu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14751v1",
                "updated": "2024-12-19T11:30:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    30,
                    7,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:30:07Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    30,
                    7,
                    3,
                    354,
                    0
                ],
                "title": "Query pipeline optimization for cancer patient question answering\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query pipeline optimization for cancer patient question answering\n  systems"
                },
                "summary": "Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by using query pipelines to retrieve relevant external\ninformation and grounding responses in retrieved knowledge. However, query\npipeline optimization for cancer patient question-answering (CPQA) systems\nrequires separately optimizing multiple components with domain-specific\nconsiderations. We propose a novel three-aspect optimization approach for the\nRAG query pipeline in CPQA systems, utilizing public biomedical databases like\nPubMed and PubMed Central. Our optimization includes: (1) document retrieval,\nutilizing a comparative analysis of NCBI resources and introducing Hybrid\nSemantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\nidentifying optimal pairings of dense retrievers and rerankers; and (3)\nsemantic representation, introducing Semantic Enhanced Overlap Segmentation\n(SEOS) for improved contextual understanding. On a custom-developed dataset\ntailored for cancer-related inquiries, our optimized RAG approach improved the\nanswer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\nabout 3% over a naive RAG setup. This study highlights the importance of\ndomain-specific query optimization in realizing the full potential of RAG and\nprovides a robust framework for building more accurate and reliable CPQA\nsystems, advancing the development of RAG-based biomedical systems."
                },
                "authors": [
                    {
                        "name": "Maolin He"
                    },
                    {
                        "name": "Rena Gao"
                    },
                    {
                        "name": "Mike Conway"
                    },
                    {
                        "name": "Brian E. Chapman"
                    }
                ],
                "author_detail": {
                    "name": "Brian E. Chapman"
                },
                "author": "Brian E. Chapman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14737v1",
                "updated": "2024-12-19T11:10:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    10,
                    36,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T11:10:36Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    10,
                    36,
                    3,
                    354,
                    0
                ],
                "title": "On Verbalized Confidence Scores for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Verbalized Confidence Scores for LLMs"
                },
                "summary": "The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq ."
                },
                "authors": [
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Yao-Hung Hubert Tsai"
                    },
                    {
                        "name": "Makoto Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Makoto Yamada"
                },
                "author": "Makoto Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18403v2",
                "updated": "2024-12-19T11:07:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    7,
                    9,
                    3,
                    354,
                    0
                ],
                "published": "2024-06-26T14:56:13Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    56,
                    13,
                    2,
                    178,
                    0
                ],
                "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks"
                },
                "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators."
                },
                "authors": [
                    {
                        "name": "Anna Bavaresco"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Raquel Fernndez"
                    },
                    {
                        "name": "Albert Gatt"
                    },
                    {
                        "name": "Esam Ghaleb"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Aditya K Surikuchi"
                    },
                    {
                        "name": "Ece Takmaz"
                    },
                    {
                        "name": "Alberto Testoni"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Testoni"
                },
                "author": "Alberto Testoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11242v2",
                "updated": "2024-12-19T10:33:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    33,
                    13,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-15T16:47:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    16,
                    47,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs"
                },
                "summary": "Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14714v1",
                "updated": "2024-12-19T10:25:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    25,
                    21,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T10:25:21Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    25,
                    21,
                    3,
                    354,
                    0
                ],
                "title": "Holistic Adversarially Robust Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Adversarially Robust Pruning"
                },
                "summary": "Neural networks can be drastically shrunk in size by removing redundant\nparameters. While crucial for the deployment on resource-constraint hardware,\noftentimes, compression comes with a severe drop in accuracy and lack of\nadversarial robustness. Despite recent advances, counteracting both aspects has\nonly succeeded for moderate compression rates so far. We propose a novel\nmethod, HARP, that copes with aggressive pruning significantly better than\nprior work. For this, we consider the network holistically. We learn a global\ncompression strategy that optimizes how many parameters (compression rate) and\nwhich parameters (scoring connections) to prune specific to each layer\nindividually. Our method fine-tunes an existing model with dynamic\nregularization, that follows a step-wise incremental function balancing the\ndifferent objectives. It starts by favoring robustness before shifting focus on\nreaching the target compression rate and only then handles the objectives\nequally. The learned compression strategies allow us to maintain the\npre-trained model natural accuracy and its adversarial robustness for a\nreduction by 99% of the network original size. Moreover, we observe a crucial\ninfluence of non-uniform compression across layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks can be drastically shrunk in size by removing redundant\nparameters. While crucial for the deployment on resource-constraint hardware,\noftentimes, compression comes with a severe drop in accuracy and lack of\nadversarial robustness. Despite recent advances, counteracting both aspects has\nonly succeeded for moderate compression rates so far. We propose a novel\nmethod, HARP, that copes with aggressive pruning significantly better than\nprior work. For this, we consider the network holistically. We learn a global\ncompression strategy that optimizes how many parameters (compression rate) and\nwhich parameters (scoring connections) to prune specific to each layer\nindividually. Our method fine-tunes an existing model with dynamic\nregularization, that follows a step-wise incremental function balancing the\ndifferent objectives. It starts by favoring robustness before shifting focus on\nreaching the target compression rate and only then handles the objectives\nequally. The learned compression strategies allow us to maintain the\npre-trained model natural accuracy and its adversarial robustness for a\nreduction by 99% of the network original size. Moreover, we observe a crucial\ninfluence of non-uniform compression across layers."
                },
                "authors": [
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Christian Wressnegger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wressnegger"
                },
                "author": "Christian Wressnegger",
                "arxiv_comment": "Accepted by ICLR 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14708v1",
                "updated": "2024-12-19T10:20:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    20,
                    34,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T10:20:34Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    20,
                    34,
                    3,
                    354,
                    0
                ],
                "title": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A\n  Survey"
                },
                "summary": "Smart spaces are ubiquitous computing environments that integrate diverse\nsensing and communication technologies to enhance space functionality, optimize\nenergy utilization, and improve user comfort and well-being. The integration of\nemerging AI methodologies into these environments facilitates the formation of\nAI-driven smart spaces, which further enhance functionalities of the spaces by\nenabling advanced applications such as personalized comfort settings,\ninteractive living spaces, and automatization of the space systems, all\nresulting in enhanced indoor experiences of the users. In this paper, we\npresent a systematic survey of existing research on the foundational components\nof AI-driven smart spaces, including sensor technologies, data communication\nprotocols, sensor network management and maintenance strategies, as well as the\ndata collection, processing and analytics. Given the pivotal role of AI in\nestablishing AI-powered smart spaces, we explore the opportunities and\nchallenges associated with traditional machine learning (ML) approaches, such\nas deep learning (DL), and emerging methodologies including large language\nmodels (LLMs). Finally, we provide key insights necessary for the development\nof AI-driven smart spaces, propose future research directions, and sheds light\non the path forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart spaces are ubiquitous computing environments that integrate diverse\nsensing and communication technologies to enhance space functionality, optimize\nenergy utilization, and improve user comfort and well-being. The integration of\nemerging AI methodologies into these environments facilitates the formation of\nAI-driven smart spaces, which further enhance functionalities of the spaces by\nenabling advanced applications such as personalized comfort settings,\ninteractive living spaces, and automatization of the space systems, all\nresulting in enhanced indoor experiences of the users. In this paper, we\npresent a systematic survey of existing research on the foundational components\nof AI-driven smart spaces, including sensor technologies, data communication\nprotocols, sensor network management and maintenance strategies, as well as the\ndata collection, processing and analytics. Given the pivotal role of AI in\nestablishing AI-powered smart spaces, we explore the opportunities and\nchallenges associated with traditional machine learning (ML) approaches, such\nas deep learning (DL), and emerging methodologies including large language\nmodels (LLMs). Finally, we provide key insights necessary for the development\nof AI-driven smart spaces, propose future research directions, and sheds light\non the path forward."
                },
                "authors": [
                    {
                        "name": "Aygn Varol"
                    },
                    {
                        "name": "Naser Hossein Motlagh"
                    },
                    {
                        "name": "Mirka Leino"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    },
                    {
                        "name": "Johanna Virkki"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Virkki"
                },
                "author": "Johanna Virkki",
                "arxiv_comment": "39 pages, 3 figures, 1 table, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; C.0; I.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v3",
                "updated": "2024-12-19T10:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    10,
                    11,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14680v1",
                "updated": "2024-12-19T09:32:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    32,
                    53,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:32:53Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    32,
                    53,
                    3,
                    354,
                    0
                ],
                "title": "A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space"
                },
                "summary": "Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD."
                },
                "authors": [
                    {
                        "name": "Yonghao He"
                    },
                    {
                        "name": "Hu Su"
                    },
                    {
                        "name": "Haiyong Yu"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Wei Sui"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Song Liu"
                    }
                ],
                "author_detail": {
                    "name": "Song Liu"
                },
                "author": "Song Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14675v1",
                "updated": "2024-12-19T09:29:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    29,
                    8,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:29:08Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    29,
                    8,
                    3,
                    354,
                    0
                ],
                "title": "LLMs as mediators: Can they diagnose conflicts accurately?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as mediators: Can they diagnose conflicts accurately?"
                },
                "summary": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research indicates that to be able to mediate conflict, observers of\ndisagreements between parties must be able to reliably distinguish the sources\nof their disagreement as stemming from differences in beliefs about what is\ntrue (causality) vs. differences in what they value (morality). In this paper,\nwe test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this\ntask and whether one or other type of disagreement proves particularly\nchallenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al.\n(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We\nfind that both LLMs have similar semantic understanding of the distinction\nbetween causal and moral codes as humans and can reliably distinguish between\nthem. When asked to diagnose the source of disagreement in a conversation, both\nLLMs, compared to humans, exhibit a tendency to overestimate the extent of\ncausal disagreement and underestimate the extent of moral disagreement in the\nmoral misalignment condition. This tendency is especially pronounced for GPT 4\nwhen using a proximate scale that relies on concrete language specific to an\nissue. GPT 3.5 does not perform as well as GPT4 or humans when using either the\nproximate or the distal scale. The study provides a first test of the potential\nfor using LLMs to mediate conflict by diagnosing the root of disagreements in\ncausal and evaluative codes."
                },
                "authors": [
                    {
                        "name": "zgecan Koak"
                    },
                    {
                        "name": "Phanish Puranam"
                    },
                    {
                        "name": "Afar Yegin"
                    }
                ],
                "author_detail": {
                    "name": "Afar Yegin"
                },
                "arxiv_affiliation": "Kadir Has University",
                "author": "Afar Yegin",
                "arxiv_comment": "27 pages, 2 appendices, 21 tables (incl appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14670v1",
                "updated": "2024-12-19T09:21:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    21,
                    39,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:21:39Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    21,
                    39,
                    3,
                    354,
                    0
                ],
                "title": "Analysis and Visualization of Linguistic Structures in Large Language\n  Models: Neural Representations of Verb-Particle Constructions in BERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Visualization of Linguistic Structures in Large Language\n  Models: Neural Representations of Verb-Particle Constructions in BERT"
                },
                "summary": "This study investigates the internal representations of verb-particle\ncombinations within transformer-based large language models (LLMs),\nspecifically examining how these models capture lexical and syntactic nuances\nat different neural network layers. Employing the BERT architecture, we analyse\nthe representational efficacy of its layers for various verb-particle\nconstructions such as 'agree on', 'come back', and 'give up'. Our methodology\nincludes a detailed dataset preparation from the British National Corpus,\nfollowed by extensive model training and output analysis through techniques\nlike multi-dimensional scaling (MDS) and generalized discrimination value (GDV)\ncalculations. Results show that BERT's middle layers most effectively capture\nsyntactic structures, with significant variability in representational accuracy\nacross different verb categories. These findings challenge the conventional\nuniformity assumed in neural network processing of linguistic elements and\nsuggest a complex interplay between network architecture and linguistic\nrepresentation. Our research contributes to a better understanding of how deep\nlearning models comprehend and process language, offering insights into the\npotential and limitations of current neural approaches to linguistic analysis.\nThis study not only advances our knowledge in computational linguistics but\nalso prompts further research into optimizing neural architectures for enhanced\nlinguistic precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the internal representations of verb-particle\ncombinations within transformer-based large language models (LLMs),\nspecifically examining how these models capture lexical and syntactic nuances\nat different neural network layers. Employing the BERT architecture, we analyse\nthe representational efficacy of its layers for various verb-particle\nconstructions such as 'agree on', 'come back', and 'give up'. Our methodology\nincludes a detailed dataset preparation from the British National Corpus,\nfollowed by extensive model training and output analysis through techniques\nlike multi-dimensional scaling (MDS) and generalized discrimination value (GDV)\ncalculations. Results show that BERT's middle layers most effectively capture\nsyntactic structures, with significant variability in representational accuracy\nacross different verb categories. These findings challenge the conventional\nuniformity assumed in neural network processing of linguistic elements and\nsuggest a complex interplay between network architecture and linguistic\nrepresentation. Our research contributes to a better understanding of how deep\nlearning models comprehend and process language, offering insights into the\npotential and limitations of current neural approaches to linguistic analysis.\nThis study not only advances our knowledge in computational linguistics but\nalso prompts further research into optimizing neural architectures for enhanced\nlinguistic precision."
                },
                "authors": [
                    {
                        "name": "Hassane Kissane"
                    },
                    {
                        "name": "Achim Schilling"
                    },
                    {
                        "name": "Patrick Krauss"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Krauss"
                },
                "author": "Patrick Krauss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01353v2",
                "updated": "2024-12-19T09:10:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    10,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-02T10:31:12Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    31,
                    12,
                    0,
                    337,
                    0
                ],
                "title": "Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk\n  through Social Media using Base Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk\n  through Social Media using Base Language Models"
                },
                "summary": "In recent times, more and more people are posting about their mental states\nacross various social media platforms. Leveraging this data, AI-based systems\ncan be developed that help in assessing the mental health of individuals, such\nas suicide risk. This paper is a study done on suicidal risk assessments using\nReddit data leveraging Base language models to identify patterns from social\nmedia posts. We have demonstrated that using smaller language models, i.e.,\nless than 500M parameters, can also be effective in contrast to LLMs with\ngreater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on\nsuicide risk prediction task that utilized both the labeled and unlabeled\nReddit data and tackled class imbalance by data augmentation using GPT-2 model.\nOur Su-RoBERTa model attained a 69.84% weighted F1 score during the Final\nevaluation. This paper demonstrates the effectiveness of Base language models\nfor the analysis of the risk factors related to mental health with an efficient\ncomputation pipeline",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent times, more and more people are posting about their mental states\nacross various social media platforms. Leveraging this data, AI-based systems\ncan be developed that help in assessing the mental health of individuals, such\nas suicide risk. This paper is a study done on suicidal risk assessments using\nReddit data leveraging Base language models to identify patterns from social\nmedia posts. We have demonstrated that using smaller language models, i.e.,\nless than 500M parameters, can also be effective in contrast to LLMs with\ngreater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on\nsuicide risk prediction task that utilized both the labeled and unlabeled\nReddit data and tackled class imbalance by data augmentation using GPT-2 model.\nOur Su-RoBERTa model attained a 69.84% weighted F1 score during the Final\nevaluation. This paper demonstrates the effectiveness of Base language models\nfor the analysis of the risk factors related to mental health with an efficient\ncomputation pipeline"
                },
                "authors": [
                    {
                        "name": "Chayan Tank"
                    },
                    {
                        "name": "Shaina Mehta"
                    },
                    {
                        "name": "Sarthak Pol"
                    },
                    {
                        "name": "Vinayak Katoch"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Raj Jaiswal"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "8 pages, 7 figures, Accepted at IEEE International Conference on Big\n  Data (IEEE BigData 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14660v1",
                "updated": "2024-12-19T09:10:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    10,
                    7,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:10:07Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    10,
                    7,
                    3,
                    354,
                    0
                ],
                "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}."
                },
                "authors": [
                    {
                        "name": "Zijun Chen"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14656v1",
                "updated": "2024-12-19T09:07:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    7,
                    38,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T09:07:38Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    7,
                    38,
                    3,
                    354,
                    0
                ],
                "title": "Length Controlled Generation for Black-box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length Controlled Generation for Black-box LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive instruction\nfollowing capabilities, while still struggling to accurately manage the length\nof the generated text, which is a fundamental requirement in many real-world\napplications. Existing length control methods involve fine-tuning the\nparameters of LLMs, which is inefficient and suboptimal for practical use. In\nthis paper, we propose a novel iterative sampling framework for text length\ncontrol, integrating the Metropolis-Hastings algorithm with an importance\nsampling acceleration strategy. This framework efficiently and reliably\nregulates LLMs to generate length-constrained text without modifying the\nunderlying parameters, thereby preserving the original capabilities of LLMs.\nExperimental results demonstrate that our framework achieves almost 100\\%\nsuccess rates of length control on Llama3.1 for tasks such as length-controlled\nabstractive summarization and length-constrained instruction following, with\nminimal additional computational overhead. This also highlights the significant\npotential of our method for precise length control across a broader range of\napplications, without compromising the versatility of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive instruction\nfollowing capabilities, while still struggling to accurately manage the length\nof the generated text, which is a fundamental requirement in many real-world\napplications. Existing length control methods involve fine-tuning the\nparameters of LLMs, which is inefficient and suboptimal for practical use. In\nthis paper, we propose a novel iterative sampling framework for text length\ncontrol, integrating the Metropolis-Hastings algorithm with an importance\nsampling acceleration strategy. This framework efficiently and reliably\nregulates LLMs to generate length-constrained text without modifying the\nunderlying parameters, thereby preserving the original capabilities of LLMs.\nExperimental results demonstrate that our framework achieves almost 100\\%\nsuccess rates of length control on Llama3.1 for tasks such as length-controlled\nabstractive summarization and length-constrained instruction following, with\nminimal additional computational overhead. This also highlights the significant\npotential of our method for precise length control across a broader range of\napplications, without compromising the versatility of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Kun Zhu"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14642v1",
                "updated": "2024-12-19T08:51:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T08:51:16Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"
                },
                "summary": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each task further contains three\nsubtasks, with each subtask comprising 5,000 test samples. Given the inherent\ncomplexity of open molecule generation, we have also developed an automated\nevaluation system that helps measure both the quality and the accuracy of the\ngenerated molecules. Our comprehensive benchmarking of 25 LLMs reveals the\ncurrent limitations and potential areas for improvement in text-guided molecule\ndiscovery. Furthermore, with the assistance of OpenMolIns, a specialized\ninstruction tuning dataset proposed for solving challenges raised by\nTOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even\nsurpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are\navailable through https://github.com/phenixace/TOMG-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each task further contains three\nsubtasks, with each subtask comprising 5,000 test samples. Given the inherent\ncomplexity of open molecule generation, we have also developed an automated\nevaluation system that helps measure both the quality and the accuracy of the\ngenerated molecules. Our comprehensive benchmarking of 25 LLMs reveals the\ncurrent limitations and potential areas for improvement in text-guided molecule\ndiscovery. Furthermore, with the assistance of OpenMolIns, a specialized\ninstruction tuning dataset proposed for solving challenges raised by\nTOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even\nsurpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are\navailable through https://github.com/phenixace/TOMG-Bench."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "A benchmark for text-based open molecule generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14626v1",
                "updated": "2024-12-19T08:28:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    28,
                    18,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T08:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    28,
                    18,
                    3,
                    354,
                    0
                ],
                "title": "Learning to Generate Research Idea with Dynamic Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Research Idea with Dynamic Control"
                },
                "summary": "The rapid advancements in large language models (LLMs) have demonstrated\ntheir potential to accelerate scientific discovery, particularly in automating\nthe process of research ideation. LLM-based systems have shown promise in\ngenerating hypotheses and research ideas. However, current approaches\npredominantly rely on prompting-based pre-trained models, limiting their\nability to optimize generated content effectively. Moreover, they also lack the\ncapability to deal with the complex interdependence and inherent restrictions\namong novelty, feasibility, and effectiveness, which remains challenging due to\nthe inherent trade-offs among these dimensions, such as the\ninnovation-feasibility conflict. To address these limitations, we for the first\ntime propose fine-tuning LLMs to be better idea proposers and introduce a novel\nframework that employs a two-stage approach combining Supervised Fine-Tuning\n(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model\nlearns foundational patterns from pairs of research papers and follow-up ideas.\nIn the RL stage, multi-dimensional reward modeling, guided by fine-grained\nfeedback, evaluates and optimizes the generated ideas across key metrics.\nDimensional controllers enable dynamic adjustment of generation, while a\nsentence-level decoder ensures context-aware emphasis during inference. Our\nframework provides a balanced approach to research ideation, achieving\nhigh-quality outcomes by dynamically navigating the trade-offs among novelty,\nfeasibility, and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have demonstrated\ntheir potential to accelerate scientific discovery, particularly in automating\nthe process of research ideation. LLM-based systems have shown promise in\ngenerating hypotheses and research ideas. However, current approaches\npredominantly rely on prompting-based pre-trained models, limiting their\nability to optimize generated content effectively. Moreover, they also lack the\ncapability to deal with the complex interdependence and inherent restrictions\namong novelty, feasibility, and effectiveness, which remains challenging due to\nthe inherent trade-offs among these dimensions, such as the\ninnovation-feasibility conflict. To address these limitations, we for the first\ntime propose fine-tuning LLMs to be better idea proposers and introduce a novel\nframework that employs a two-stage approach combining Supervised Fine-Tuning\n(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model\nlearns foundational patterns from pairs of research papers and follow-up ideas.\nIn the RL stage, multi-dimensional reward modeling, guided by fine-grained\nfeedback, evaluates and optimizes the generated ideas across key metrics.\nDimensional controllers enable dynamic adjustment of generation, while a\nsentence-level decoder ensures context-aware emphasis during inference. Our\nframework provides a balanced approach to research ideation, achieving\nhigh-quality outcomes by dynamically navigating the trade-offs among novelty,\nfeasibility, and effectiveness."
                },
                "authors": [
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12470v2",
                "updated": "2024-12-19T08:26:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    26,
                    32,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T15:10:56Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    10,
                    56,
                    3,
                    235,
                    0
                ],
                "title": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into recommender systems has\nled to substantial performance improvements. However, this often comes at the\ncost of diminished recommendation diversity, which can negatively impact user\nsatisfaction. To address this issue, controllable recommendation has emerged as\na promising approach, allowing users to specify their preferences and receive\nrecommendations that meet their diverse needs. Despite its potential, existing\ncontrollable recommender systems frequently rely on simplistic mechanisms, such\nas a single prompt, to regulate diversity-an approach that falls short of\ncapturing the full complexity of user preferences. In response to these\nlimitations, we propose DLCRec, a novel framework designed to enable\nfine-grained control over diversity in LLM-based recommendations. Unlike\ntraditional methods, DLCRec adopts a fine-grained task decomposition strategy,\nbreaking down the recommendation process into three sequential sub-tasks: genre\nprediction, genre filling, and item prediction. These sub-tasks are trained\nindependently and inferred sequentially according to user-defined control\nnumbers, ensuring more precise control over diversity. Furthermore, the\nscarcity and uneven distribution of diversity-related user behavior data pose\nsignificant challenges for fine-tuning. To overcome these obstacles, we\nintroduce two data augmentation techniques that enhance the model's robustness\nto noisy and out-of-distribution data. These techniques expose the model to a\nbroader range of patterns, improving its adaptability in generating\nrecommendations with varying levels of diversity. Our extensive empirical\nevaluation demonstrates that DLCRec not only provides precise control over\ndiversity but also outperforms state-of-the-art baselines across multiple\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "arxiv_comment": "Accepted by WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14617v1",
                "updated": "2024-12-19T08:06:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    6,
                    9,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T08:06:09Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    6,
                    9,
                    3,
                    354,
                    0
                ],
                "title": "How good is GPT at writing political speeches for the White House?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How good is GPT at writing political speeches for the White House?"
                },
                "summary": "Using large language models (LLMs), computers are able to generate a written\ntext in response to a us er request. As this pervasive technology can be\napplied in numerous contexts, this study analyses the written style of one LLM\ncalled GPT by comparing its generated speeches with those of the recent US\npresidents. To achieve this objective, the State of the Union (SOTU) addresses\nwritten by Reagan to Biden are contrasted to those produced by both GPT-3.5 and\nGPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma\n\"we\" and produce shorter messages with, on average, longer sentences. Moreover,\nGPT opts for an optimistic tone, opting more often for political (e.g.,\npresident, Congress), symbolic (e.g., freedom), and abstract terms (e.g.,\nfreedom). Even when imposing an author's style to GPT, the resulting speech\nremains distinct from addresses written by the target author. Finally, the two\nGPT versions present distinct characteristics, but both appear overall\ndissimilar to true presidential messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using large language models (LLMs), computers are able to generate a written\ntext in response to a us er request. As this pervasive technology can be\napplied in numerous contexts, this study analyses the written style of one LLM\ncalled GPT by comparing its generated speeches with those of the recent US\npresidents. To achieve this objective, the State of the Union (SOTU) addresses\nwritten by Reagan to Biden are contrasted to those produced by both GPT-3.5 and\nGPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma\n\"we\" and produce shorter messages with, on average, longer sentences. Moreover,\nGPT opts for an optimistic tone, opting more often for political (e.g.,\npresident, Congress), symbolic (e.g., freedom), and abstract terms (e.g.,\nfreedom). Even when imposing an author's style to GPT, the resulting speech\nremains distinct from addresses written by the target author. Finally, the two\nGPT versions present distinct characteristics, but both appear overall\ndissimilar to true presidential messages."
                },
                "authors": [
                    {
                        "name": "Jacques Savoy"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Savoy"
                },
                "author": "Jacques Savoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14614v1",
                "updated": "2024-12-19T08:04:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    4,
                    22,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T08:04:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    4,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "A Model-free Biomimetics Algorithm for Deterministic Partially\n  Observable Markov Decision Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-free Biomimetics Algorithm for Deterministic Partially\n  Observable Markov Decision Process"
                },
                "summary": "Partially Observable Markov Decision Process (POMDP) is a mathematical\nframework for modeling decision-making under uncertainty, where the agent's\nobservations are incomplete and the underlying system dynamics are\nprobabilistic. Solving the POMDP problem within the model-free paradigm is\nchallenging for agents due to the inherent difficulty in accurately identifying\nand distinguishing between states and observations. We define such a difficult\nproblem as a DETerministic Partially Observable Markov Decision Process\n(DET-POMDP) problem, which is a specific setting of POMDP. In this problem,\nstates and observations are in a many-to-one relationship. The state is\nobscured, and its relationship is less apparent to the agent. This creates\nobstacles for the agent to infer the state through observations. To effectively\naddress this problem, we convert DET-POMDP into a fully observable MDP using a\nmodel-free biomimetics algorithm called BIOMAP. BIOMAP is based on the MDP\nGraph Automaton framework to distinguish authentic environmental information\nfrom fraudulent data. Thus, it enhances the agent's ability to develop stable\npolicies against DET-POMDP. The experimental results highlight the superior\ncapabilities of BIOMAP in maintaining operational effectiveness and\nenvironmental reparability in the presence of environmental deceptions when\ncompared with existing POMDP solvers. This research opens up new avenues for\nthe deployment of reliable POMDP-based systems in fields that are particularly\nsusceptible to DET-POMDP problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observable Markov Decision Process (POMDP) is a mathematical\nframework for modeling decision-making under uncertainty, where the agent's\nobservations are incomplete and the underlying system dynamics are\nprobabilistic. Solving the POMDP problem within the model-free paradigm is\nchallenging for agents due to the inherent difficulty in accurately identifying\nand distinguishing between states and observations. We define such a difficult\nproblem as a DETerministic Partially Observable Markov Decision Process\n(DET-POMDP) problem, which is a specific setting of POMDP. In this problem,\nstates and observations are in a many-to-one relationship. The state is\nobscured, and its relationship is less apparent to the agent. This creates\nobstacles for the agent to infer the state through observations. To effectively\naddress this problem, we convert DET-POMDP into a fully observable MDP using a\nmodel-free biomimetics algorithm called BIOMAP. BIOMAP is based on the MDP\nGraph Automaton framework to distinguish authentic environmental information\nfrom fraudulent data. Thus, it enhances the agent's ability to develop stable\npolicies against DET-POMDP. The experimental results highlight the superior\ncapabilities of BIOMAP in maintaining operational effectiveness and\nenvironmental reparability in the presence of environmental deceptions when\ncompared with existing POMDP solvers. This research opens up new avenues for\nthe deployment of reliable POMDP-based systems in fields that are particularly\nsusceptible to DET-POMDP problems."
                },
                "authors": [
                    {
                        "name": "Yide Yu"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Xiaochen Yuan"
                    },
                    {
                        "name": "Dennis Wong"
                    },
                    {
                        "name": "Huijie Li"
                    },
                    {
                        "name": "Yan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yan Ma"
                },
                "author": "Yan Ma",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14611v1",
                "updated": "2024-12-19T08:00:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    0,
                    20,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T08:00:20Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    0,
                    20,
                    3,
                    354,
                    0
                ],
                "title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code\n  Stylometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code\n  Stylometry"
                },
                "summary": "With the increasing popularity of LLM-based code completers, like GitHub\nCopilot, the interest in automatically detecting AI-generated code is also\nincreasing-in particular in contexts where the use of LLMs to program is\nforbidden by policy due to security, intellectual property, or ethical\nconcerns.We introduce a novel technique for AI code stylometry, i.e., the\nability to distinguish code generated by LLMs from code written by humans,\nbased on a transformer-based encoder classifier. Differently from previous\nwork, our classifier is capable of detecting AI-written code across 10\ndifferent programming languages with a single machine learning model,\nmaintaining high average accuracy across all languages (84.1% $\\pm$\n3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open\ndataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10\npopular programming languages, labeled as either human-written or AI-generated.\nThe experimental pipeline (dataset, training code, resulting models) is the\nfirst fully reproducible one for the AI code stylometry task. Most notably our\nexperiments rely only on open LLMs, rather than on proprietary/closed ones like\nChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing popularity of LLM-based code completers, like GitHub\nCopilot, the interest in automatically detecting AI-generated code is also\nincreasing-in particular in contexts where the use of LLMs to program is\nforbidden by policy due to security, intellectual property, or ethical\nconcerns.We introduce a novel technique for AI code stylometry, i.e., the\nability to distinguish code generated by LLMs from code written by humans,\nbased on a transformer-based encoder classifier. Differently from previous\nwork, our classifier is capable of detecting AI-written code across 10\ndifferent programming languages with a single machine learning model,\nmaintaining high average accuracy across all languages (84.1% $\\pm$\n3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open\ndataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10\npopular programming languages, labeled as either human-written or AI-generated.\nThe experimental pipeline (dataset, training code, resulting models) is the\nfirst fully reproducible one for the AI code stylometry task. Most notably our\nexperiments rely only on open LLMs, rather than on proprietary/closed ones like\nChatGPT."
                },
                "authors": [
                    {
                        "name": "Andrea Gurioli"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    },
                    {
                        "name": "Stefano Zacchiroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Zacchiroli"
                },
                "arxiv_affiliation": "IP Paris, LTCI, ACES, INFRES",
                "author": "Stefano Zacchiroli",
                "arxiv_journal_ref": "IEEE International Conference on Software Analysis, Evolution and\n  Reengineering, SANER 2025, Mar 2025, Montr{\\'e}al, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14590v1",
                "updated": "2024-12-19T07:15:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    15,
                    15,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T07:15:15Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    15,
                    15,
                    3,
                    354,
                    0
                ],
                "title": "MixLLM: LLM Quantization with Global Mixed-precision between\n  Output-features and Highly-efficient System Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixLLM: LLM Quantization with Global Mixed-precision between\n  Output-features and Highly-efficient System Design"
                },
                "summary": "Quantization has become one of the most effective methodologies to compress\nLLMs into smaller size. However, the existing quantization solutions still show\nlimitations of either non-negligible accuracy drop or system inefficiency. In\nthis paper, we make a comprehensive analysis of the general quantization\nprinciples on their effect to the triangle of accuracy, memory consumption and\nsystem efficiency. We propose MixLLM that explores the new optimization space\nof mixed-precision quantization between output features based on the insight\nthat different output features matter differently in the model. MixLLM\nidentifies the output features with high salience in the global view rather\nthan within each single layer, effectively assigning the larger bit-width to\noutput features that need it most to achieve good accuracy with low memory\nconsumption. We present the sweet spot of quantization configuration of\nalgorithm-system co-design that leads to high accuracy and system efficiency.\nTo address the system challenge, we design the two-step dequantization to make\nuse of the int8 Tensor Core easily and fast data type conversion to reduce\ndequantization overhead significantly, and present the software pipeline to\noverlap the memory access, dequantization and the MatMul to the best. Extensive\nexperiments show that with only 10% more bits, the PPL increasement can be\nreduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on\naverage MMLU-Pro improves by 0.93 over the SOTA of three popular models. In\naddition to its superior accuracy, MixLLM also achieves state-of-the-art system\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has become one of the most effective methodologies to compress\nLLMs into smaller size. However, the existing quantization solutions still show\nlimitations of either non-negligible accuracy drop or system inefficiency. In\nthis paper, we make a comprehensive analysis of the general quantization\nprinciples on their effect to the triangle of accuracy, memory consumption and\nsystem efficiency. We propose MixLLM that explores the new optimization space\nof mixed-precision quantization between output features based on the insight\nthat different output features matter differently in the model. MixLLM\nidentifies the output features with high salience in the global view rather\nthan within each single layer, effectively assigning the larger bit-width to\noutput features that need it most to achieve good accuracy with low memory\nconsumption. We present the sweet spot of quantization configuration of\nalgorithm-system co-design that leads to high accuracy and system efficiency.\nTo address the system challenge, we design the two-step dequantization to make\nuse of the int8 Tensor Core easily and fast data type conversion to reduce\ndequantization overhead significantly, and present the software pipeline to\noverlap the memory access, dequantization and the MatMul to the best. Extensive\nexperiments show that with only 10% more bits, the PPL increasement can be\nreduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on\naverage MMLU-Pro improves by 0.93 over the SOTA of three popular models. In\naddition to its superior accuracy, MixLLM also achieves state-of-the-art system\nefficiency."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xiaonan Song"
                    },
                    {
                        "name": "Chuanjie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjie Liu"
                },
                "author": "Chuanjie Liu",
                "arxiv_comment": "The code will be released in the future",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14588v1",
                "updated": "2024-12-19T07:14:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    14,
                    13,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T07:14:13Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    14,
                    13,
                    3,
                    354,
                    0
                ],
                "title": "Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning"
                },
                "summary": "In legal practice, judges apply the trichotomous dogmatics of criminal law,\nsequentially assessing the elements of the offense, unlawfulness, and\nculpability to determine whether an individual's conduct constitutes a crime.\nAlthough current legal large language models (LLMs) show promising accuracy in\njudgment prediction, they lack trichotomous reasoning capabilities due to the\nabsence of an appropriate benchmark dataset, preventing them from predicting\ninnocent outcomes. As a result, every input is automatically assigned a charge,\nlimiting their practical utility in legal contexts. To bridge this gap, we\nintroduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with\nInnocent Verdicts. Adhering to the trichotomous dogmatics, we extend three\nwidely-used legal datasets through LLM-based augmentation and manual\nverification. Our experiments with state-of-the-art legal LLMs and novel\nstrategies that integrate trichotomous reasoning into zero-shot prompting and\nfine-tuning reveal: (1) current legal LLMs have significant room for\nimprovement, with even the best models achieving an F1 score of less than 0.3\non LJPIV; and (2) our strategies notably enhance both in-domain and\ncross-domain judgment prediction accuracy, especially for cases resulting in an\ninnocent verdict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In legal practice, judges apply the trichotomous dogmatics of criminal law,\nsequentially assessing the elements of the offense, unlawfulness, and\nculpability to determine whether an individual's conduct constitutes a crime.\nAlthough current legal large language models (LLMs) show promising accuracy in\njudgment prediction, they lack trichotomous reasoning capabilities due to the\nabsence of an appropriate benchmark dataset, preventing them from predicting\ninnocent outcomes. As a result, every input is automatically assigned a charge,\nlimiting their practical utility in legal contexts. To bridge this gap, we\nintroduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with\nInnocent Verdicts. Adhering to the trichotomous dogmatics, we extend three\nwidely-used legal datasets through LLM-based augmentation and manual\nverification. Our experiments with state-of-the-art legal LLMs and novel\nstrategies that integrate trichotomous reasoning into zero-shot prompting and\nfine-tuning reveal: (1) current legal LLMs have significant room for\nimprovement, with even the best models achieving an F1 score of less than 0.3\non LJPIV; and (2) our strategies notably enhance both in-domain and\ncross-domain judgment prediction accuracy, especially for cases resulting in an\ninnocent verdict."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Haoyue Yang"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14584v1",
                "updated": "2024-12-19T07:06:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    6,
                    1,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T07:06:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    6,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "Simulation-Free Hierarchical Latent Policy Planning for Proactive\n  Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Free Hierarchical Latent Policy Planning for Proactive\n  Dialogues"
                },
                "summary": "Recent advancements in proactive dialogues have garnered significant\nattention, particularly for more complex objectives (e.g. emotion support and\npersuasion). Unlike traditional task-oriented dialogues, proactive dialogues\ndemand advanced policy planning and adaptability, requiring rich scenarios and\ncomprehensive policy repositories to develop such systems. However, existing\napproaches tend to rely on Large Language Models (LLMs) for user simulation and\nonline learning, leading to biases that diverge from realistic scenarios and\nresult in suboptimal efficiency. Moreover, these methods depend on manually\ndefined, context-independent, coarse-grained policies, which not only incur\nhigh expert costs but also raise concerns regarding their completeness. In our\nwork, we highlight the potential for automatically discovering policies\ndirectly from raw, real-world dialogue records. To this end, we introduce a\nnovel dialogue policy planning framework, LDPP. It fully automates the process\nfrom mining policies in dialogue records to learning policy planning.\nSpecifically, we employ a variant of the Variational Autoencoder to discover\nfine-grained policies represented as latent vectors. After automatically\nannotating the data with these latent policy labels, we propose an Offline\nHierarchical Reinforcement Learning (RL) algorithm in the latent space to\ndevelop effective policy planning capabilities. Our experiments demonstrate\nthat LDPP outperforms existing methods on two proactive scenarios, even\nsurpassing ChatGPT with only a 1.8-billion-parameter LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in proactive dialogues have garnered significant\nattention, particularly for more complex objectives (e.g. emotion support and\npersuasion). Unlike traditional task-oriented dialogues, proactive dialogues\ndemand advanced policy planning and adaptability, requiring rich scenarios and\ncomprehensive policy repositories to develop such systems. However, existing\napproaches tend to rely on Large Language Models (LLMs) for user simulation and\nonline learning, leading to biases that diverge from realistic scenarios and\nresult in suboptimal efficiency. Moreover, these methods depend on manually\ndefined, context-independent, coarse-grained policies, which not only incur\nhigh expert costs but also raise concerns regarding their completeness. In our\nwork, we highlight the potential for automatically discovering policies\ndirectly from raw, real-world dialogue records. To this end, we introduce a\nnovel dialogue policy planning framework, LDPP. It fully automates the process\nfrom mining policies in dialogue records to learning policy planning.\nSpecifically, we employ a variant of the Variational Autoencoder to discover\nfine-grained policies represented as latent vectors. After automatically\nannotating the data with these latent policy labels, we propose an Offline\nHierarchical Reinforcement Learning (RL) algorithm in the latent space to\ndevelop effective policy planning capabilities. Our experiments demonstrate\nthat LDPP outperforms existing methods on two proactive scenarios, even\nsurpassing ChatGPT with only a 1.8-billion-parameter LLM."
                },
                "authors": [
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Lizi Liao"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Yiheng Sun"
                    },
                    {
                        "name": "Zerui Chen"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "24 pages, 5 fgiures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14581v1",
                "updated": "2024-12-19T07:01:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    1,
                    25,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T07:01:25Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    1,
                    25,
                    3,
                    354,
                    0
                ],
                "title": "CORD: Balancing COnsistency and Rank Distillation for Robust\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Balancing COnsistency and Rank Distillation for Robust\n  Retrieval-Augmented Generation"
                },
                "summary": "With the adoption of retrieval-augmented generation (RAG), large language\nmodels (LLMs) are expected to ground their generation to the retrieved\ncontexts. Yet, this is hindered by position bias of LLMs, failing to evenly\nattend to all contexts. Previous work has addressed this by synthesizing\ncontexts with perturbed positions of gold segment, creating a\nposition-diversified train set. We extend this intuition to propose consistency\nregularization with augmentation and distillation. First, we augment each\ntraining instance with its position perturbation to encourage consistent\npredictions, regardless of ordering. We also distill behaviors of this pair,\nalthough it can be counterproductive in certain RAG scenarios where the given\norder from the retriever is crucial for generation quality. We thus propose\nCORD, balancing COnsistency and Rank Distillation. CORD adaptively samples\nnoise-controlled perturbations from an interpolation space, ensuring both\nconsistency and respect for the rank prior. Empirical results show this balance\nenables CORD to outperform consistently in diverse RAG benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the adoption of retrieval-augmented generation (RAG), large language\nmodels (LLMs) are expected to ground their generation to the retrieved\ncontexts. Yet, this is hindered by position bias of LLMs, failing to evenly\nattend to all contexts. Previous work has addressed this by synthesizing\ncontexts with perturbed positions of gold segment, creating a\nposition-diversified train set. We extend this intuition to propose consistency\nregularization with augmentation and distillation. First, we augment each\ntraining instance with its position perturbation to encourage consistent\npredictions, regardless of ordering. We also distill behaviors of this pair,\nalthough it can be counterproductive in certain RAG scenarios where the given\norder from the retriever is crucial for generation quality. We thus propose\nCORD, balancing COnsistency and Rank Distillation. CORD adaptively samples\nnoise-controlled perturbations from an interpolation space, ensuring both\nconsistency and respect for the rank prior. Empirical results show this balance\nenables CORD to outperform consistently in diverse RAG benchmarks."
                },
                "authors": [
                    {
                        "name": "Youngwon Lee"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Filip Graliski"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14574v1",
                "updated": "2024-12-19T06:44:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    44,
                    59,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T06:44:59Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    44,
                    59,
                    3,
                    354,
                    0
                ],
                "title": "Sliding Windows Are Not the End: Exploring Full Ranking with\n  Long-Context Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sliding Windows Are Not the End: Exploring Full Ranking with\n  Long-Context Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown exciting performance in listwise\npassage ranking. Due to the limited input length, existing methods often adopt\nthe sliding window strategy. Such a strategy, though effective, is inefficient\nas it involves repetitive and serialized processing, which usually re-evaluates\nrelevant passages multiple times. As a result, it incurs redundant API costs,\nwhich are proportional to the number of inference tokens. The development of\nlong-context LLMs enables the full ranking of all passages within a single\ninference, avoiding redundant API costs. In this paper, we conduct a\ncomprehensive study of long-context LLMs for ranking tasks in terms of\nefficiency and effectiveness. Surprisingly, our experiments reveal that full\nranking with long-context LLMs can deliver superior performance in the\nsupervised fine-tuning setting with a huge efficiency improvement. Furthermore,\nwe identify two limitations of fine-tuning the full ranking model based on\nexisting methods: (1) sliding window strategy fails to produce a full ranking\nlist as a training label, and (2) the language modeling loss cannot emphasize\ntop-ranked passage IDs in the label. To alleviate these issues, we propose a\nnew complete listwise label construction approach and a novel importance-aware\nlearning objective for full ranking. Experiments show the superior performance\nof our method over baselines. Our codes are available at\n\\url{https://github.com/8421BCD/fullrank}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exciting performance in listwise\npassage ranking. Due to the limited input length, existing methods often adopt\nthe sliding window strategy. Such a strategy, though effective, is inefficient\nas it involves repetitive and serialized processing, which usually re-evaluates\nrelevant passages multiple times. As a result, it incurs redundant API costs,\nwhich are proportional to the number of inference tokens. The development of\nlong-context LLMs enables the full ranking of all passages within a single\ninference, avoiding redundant API costs. In this paper, we conduct a\ncomprehensive study of long-context LLMs for ranking tasks in terms of\nefficiency and effectiveness. Surprisingly, our experiments reveal that full\nranking with long-context LLMs can deliver superior performance in the\nsupervised fine-tuning setting with a huge efficiency improvement. Furthermore,\nwe identify two limitations of fine-tuning the full ranking model based on\nexisting methods: (1) sliding window strategy fails to produce a full ranking\nlist as a training label, and (2) the language modeling loss cannot emphasize\ntop-ranked passage IDs in the label. To alleviate these issues, we propose a\nnew complete listwise label construction approach and a novel importance-aware\nlearning objective for full ranking. Experiments show the superior performance\nof our method over baselines. Our codes are available at\n\\url{https://github.com/8421BCD/fullrank}."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ziliang Zhao"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19813v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19813v3",
                "updated": "2024-12-19T06:27:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    27,
                    44,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-29T09:05:10Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    9,
                    5,
                    10,
                    0,
                    211,
                    0
                ],
                "title": "Improving Retrieval Augmented Language Model with Self-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval Augmented Language Model with Self-Reasoning"
                },
                "summary": "The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-the-art models and can\nachieve comparable performance with GPT-4, while only using 2,000 training\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-the-art models and can\nachieve comparable performance with GPT-4, while only using 2,000 training\nsamples."
                },
                "authors": [
                    {
                        "name": "Yuan Xia"
                    },
                    {
                        "name": "Jingbo Zhou"
                    },
                    {
                        "name": "Zhenhui Shi"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Haifeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Huang"
                },
                "author": "Haifeng Huang",
                "arxiv_comment": "AAAI 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19813v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19813v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11532v2",
                "updated": "2024-12-19T06:19:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    19,
                    43,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-18T12:41:16Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    41,
                    16,
                    0,
                    323,
                    0
                ],
                "title": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge\n  Graph"
                },
                "summary": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the programming capabilities of large language models (LLMs)\nhave garnered significant attention. Fuzz testing, a highly effective\ntechnique, plays a key role in enhancing software reliability and detecting\nvulnerabilities. However, traditional fuzz testing tools rely on manually\ncrafted fuzz drivers, which can limit both testing efficiency and\neffectiveness. To address this challenge, we propose an automated fuzz testing\nmethod driven by a code knowledge graph and powered by an LLM-based intelligent\nagent system, referred to as CKGFuzzer. We approach fuzz driver creation as a\ncode generation task, leveraging the knowledge graph of the code repository to\nautomate the generation process within the fuzzing loop, while continuously\nrefining both the fuzz driver and input seeds. The code knowledge graph is\nconstructed through interprocedural program analysis, where each node in the\ngraph represents a code entity, such as a function or a file. The knowledge\ngraph-enhanced CKGFuzzer not only effectively resolves compilation errors in\nfuzz drivers and generates input seeds tailored to specific API usage\nscenarios, but also analyzes fuzz driver crash reports, assisting developers in\nimproving code quality. By querying the knowledge graph of the code repository\nand learning from API usage scenarios, we can better identify testing targets\nand understand the specific purpose of each fuzz driver. We evaluated our\napproach using eight open-source software projects. The experimental results\nindicate that CKGFuzzer achieved an average improvement of 8.73% in code\ncoverage compared to state-of-the-art techniques. Additionally, CKGFuzzer\nreduced the manual review workload in crash case analysis by 84.4% and\nsuccessfully detected 11 real bugs (including nine previously unreported bugs)\nacross the tested libraries."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Ting Zhou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14556v1",
                "updated": "2024-12-19T06:14:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    14,
                    20,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T06:14:20Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    14,
                    20,
                    3,
                    354,
                    0
                ],
                "title": "CitaLaw: Enhancing LLM with Citations in Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CitaLaw: Enhancing LLM with Citations in Legal Domain"
                },
                "summary": "In this paper, we propose CitaLaw, the first benchmark designed to evaluate\nLLMs' ability to produce legally sound responses with appropriate citations.\nCitaLaw features a diverse set of legal questions for both laypersons and\npractitioners, paired with a comprehensive corpus of law articles and precedent\ncases as a reference pool. This framework enables LLM-based systems to retrieve\nsupporting citations from the reference corpus and align these citations with\nthe corresponding sentences in their responses. Moreover, we introduce\nsyllogism-inspired evaluation methods to assess the legal alignment between\nretrieved references and LLM-generated responses, as well as their consistency\nwith user questions. Extensive experiments on 2 open-domain and 7\nlegal-specific LLMs demonstrate that integrating legal references substantially\nenhances response quality. Furthermore, our proposed syllogism-based evaluation\nmethod exhibits strong agreement with human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose CitaLaw, the first benchmark designed to evaluate\nLLMs' ability to produce legally sound responses with appropriate citations.\nCitaLaw features a diverse set of legal questions for both laypersons and\npractitioners, paired with a comprehensive corpus of law articles and precedent\ncases as a reference pool. This framework enables LLM-based systems to retrieve\nsupporting citations from the reference corpus and align these citations with\nthe corresponding sentences in their responses. Moreover, we introduce\nsyllogism-inspired evaluation methods to assess the legal alignment between\nretrieved references and LLM-generated responses, as well as their consistency\nwith user questions. Extensive experiments on 2 open-domain and 7\nlegal-specific LLMs demonstrate that integrating legal references substantially\nenhances response quality. Furthermore, our proposed syllogism-based evaluation\nmethod exhibits strong agreement with human judgments."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14554v1",
                "updated": "2024-12-19T06:10:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    10,
                    40,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T06:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    10,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models"
                },
                "summary": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field."
                },
                "authors": [
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07062v2",
                "updated": "2024-12-19T06:05:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    5,
                    53,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-10T00:10:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    0,
                    10,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning"
                },
                "summary": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 7.21\\% (up to 14.29\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 7.21\\% (up to 14.29\\%)."
                },
                "authors": [
                    {
                        "name": "Weihang Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zhiqiang Li"
                    },
                    {
                        "name": "Ling Gao"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10455v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10455v5",
                "updated": "2024-12-19T05:45:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    45,
                    35,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-19T23:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    23,
                    37,
                    7,
                    0,
                    232,
                    0
                ],
                "title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent\n  through Induction, Deduction, and Abduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent\n  through Induction, Deduction, and Abduction"
                },
                "summary": "While large language models (LLMs) have been thoroughly evaluated for\ndeductive and inductive reasoning, their proficiency in holistic rule learning\nin interactive environments remains less explored. We introduce RULEARN, a\nnovel benchmark to assess the rule-learning abilities of LLM agents in\ninteractive settings. In RULEARN, agents strategically interact with simulated\nenvironments to gather observations, discern patterns, and solve complex\nproblems. To enhance the rule-learning capabilities for LLM agents, we propose\nIDEA, a novel reasoning framework that integrates the process of Induction,\nDeduction, and Abduction. The IDEA agent generates initial hypotheses from\nlimited observations through abduction, devises plans to validate these\nhypotheses or leverages them to solve problems via deduction, and refines\nprevious hypotheses through induction, dynamically establishing and applying\nrules that mimic human rule-learning behaviors. Our evaluation of the IDEA\nframework, which involves five representative LLMs, demonstrates significant\nimprovements over the baseline. Furthermore, our study with human participants\nreveals notable discrepancies in rule-learning behaviors between humans and\nLLMs. We believe our benchmark will serve as a valuable and challenging\nresource, and IDEA will provide crucial insights for the development of LLM\nagents capable of human-like rule learning in real-world scenarios. Our code\nand data is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have been thoroughly evaluated for\ndeductive and inductive reasoning, their proficiency in holistic rule learning\nin interactive environments remains less explored. We introduce RULEARN, a\nnovel benchmark to assess the rule-learning abilities of LLM agents in\ninteractive settings. In RULEARN, agents strategically interact with simulated\nenvironments to gather observations, discern patterns, and solve complex\nproblems. To enhance the rule-learning capabilities for LLM agents, we propose\nIDEA, a novel reasoning framework that integrates the process of Induction,\nDeduction, and Abduction. The IDEA agent generates initial hypotheses from\nlimited observations through abduction, devises plans to validate these\nhypotheses or leverages them to solve problems via deduction, and refines\nprevious hypotheses through induction, dynamically establishing and applying\nrules that mimic human rule-learning behaviors. Our evaluation of the IDEA\nframework, which involves five representative LLMs, demonstrates significant\nimprovements over the baseline. Furthermore, our study with human participants\nreveals notable discrepancies in rule-learning behaviors between humans and\nLLMs. We believe our benchmark will serve as a valuable and challenging\nresource, and IDEA will provide crucial insights for the development of LLM\nagents capable of human-like rule learning in real-world scenarios. Our code\nand data is publicly available."
                },
                "authors": [
                    {
                        "name": "Kaiyu He"
                    },
                    {
                        "name": "Mian Zhang"
                    },
                    {
                        "name": "Shuo Yan"
                    },
                    {
                        "name": "Peilin Wu"
                    },
                    {
                        "name": "Zhiyu Zoey Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Zoey Chen"
                },
                "author": "Zhiyu Zoey Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10455v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10455v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00451v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00451v3",
                "updated": "2024-12-19T05:32:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    32,
                    59,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-01T07:11:55Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    11,
                    55,
                    1,
                    275,
                    0
                ],
                "title": "Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large\n  Language Models"
                },
                "summary": "Despite significant ongoing efforts in safety alignment, large language\nmodels (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks\nthat can induce harmful behaviors, including through the use of adversarial\nsuffixes. Building on prior research, we hypothesize that these adversarial\nsuffixes are not mere bugs but may represent features that can dominate the\nLLM's behavior. To evaluate this hypothesis, we conduct several experiments.\nFirst, we demonstrate that benign features can be effectively made to function\nas adversarial suffixes, i.e., we develop a feature extraction method to\nextract sample-agnostic features from benign dataset in the form of suffixes\nand show that these suffixes may effectively compromise safety alignment.\nSecond, we show that adversarial suffixes generated from jailbreak attacks may\ncontain meaningful features, i.e., appending the same suffix to different\nprompts results in responses exhibiting specific characteristics. Third, we\nshow that such benign-yet-safety-compromising features can be easily introduced\nthrough fine-tuning using only benign datasets. As a result, we are able to\ncompletely eliminate GPT's safety alignment in a blackbox setting through\nfinetuning with only benign data. Our code and data is available at\n\\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant ongoing efforts in safety alignment, large language\nmodels (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks\nthat can induce harmful behaviors, including through the use of adversarial\nsuffixes. Building on prior research, we hypothesize that these adversarial\nsuffixes are not mere bugs but may represent features that can dominate the\nLLM's behavior. To evaluate this hypothesis, we conduct several experiments.\nFirst, we demonstrate that benign features can be effectively made to function\nas adversarial suffixes, i.e., we develop a feature extraction method to\nextract sample-agnostic features from benign dataset in the form of suffixes\nand show that these suffixes may effectively compromise safety alignment.\nSecond, we show that adversarial suffixes generated from jailbreak attacks may\ncontain meaningful features, i.e., appending the same suffix to different\nprompts results in responses exhibiting specific characteristics. Third, we\nshow that such benign-yet-safety-compromising features can be easily introduced\nthrough fine-tuning using only benign datasets. As a result, we are able to\ncompletely eliminate GPT's safety alignment in a blackbox setting through\nfinetuning with only benign data. Our code and data is available at\n\\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00451v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00451v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16120v3",
                "updated": "2024-12-19T05:18:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    18,
                    33,
                    3,
                    354,
                    0
                ],
                "published": "2023-09-28T02:58:07Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    2,
                    58,
                    7,
                    3,
                    271,
                    0
                ],
                "title": "Fixing Large Language Models' Specification Misunderstanding for Better\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Large Language Models' Specification Misunderstanding for Better\n  Code Generation"
                },
                "summary": "Code generation is to automatically generate source code conforming to a\ngiven programming specification, which has received extensive attention\nespecially with the development of large language models (LLMs). Due to the\ninherent difficulty of code generation, the code generated by LLMs may not be\naligned with the specification. Although thought-eliciting prompting techniques\nhave been proposed to enhance the code generation performance of LLMs,\nproducing correct understanding for complicated programming problems remains\nchallenging, resulting in unsatisfactory performance. Also, some feedback-based\nprompting techniques have been proposed to fix incorrect code using error\nmessages produced by test execution. However, when the generated code deviates\nsignificantly from the ground truth, they encounter difficulties in improving\nperformance based on such coarse-grained information. In this work, we propose\na novel prompting technique, called {\\mu}FiX, to improve the code generation\nperformance of LLMs by devising both sophisticated thought-eliciting prompting\nand feedback-based prompting and making the first exploration on their synergy.\nIt first exploits test case analysis to obtain specification understanding and\nenables a self-improvement process to identify and refine the misunderstanding\nin the thought-eliciting prompting phase. {\\mu}FiX further fixes the\nspecification understanding towards the direction reducing the gap between the\nprovided understanding (from the first phase) and the actual understanding\nimplicitly utilized by LLMs for code generation in the feedback-based prompting\nphase. By improving the understanding with {\\mu}FiX, the code generation\nperformance of LLMs can be largely improved. Our evaluation on two advanced\nLLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing\nwith 15 baselines, demonstrates the effectiveness of {\\mu}FiX ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is to automatically generate source code conforming to a\ngiven programming specification, which has received extensive attention\nespecially with the development of large language models (LLMs). Due to the\ninherent difficulty of code generation, the code generated by LLMs may not be\naligned with the specification. Although thought-eliciting prompting techniques\nhave been proposed to enhance the code generation performance of LLMs,\nproducing correct understanding for complicated programming problems remains\nchallenging, resulting in unsatisfactory performance. Also, some feedback-based\nprompting techniques have been proposed to fix incorrect code using error\nmessages produced by test execution. However, when the generated code deviates\nsignificantly from the ground truth, they encounter difficulties in improving\nperformance based on such coarse-grained information. In this work, we propose\na novel prompting technique, called {\\mu}FiX, to improve the code generation\nperformance of LLMs by devising both sophisticated thought-eliciting prompting\nand feedback-based prompting and making the first exploration on their synergy.\nIt first exploits test case analysis to obtain specification understanding and\nenables a self-improvement process to identify and refine the misunderstanding\nin the thought-eliciting prompting phase. {\\mu}FiX further fixes the\nspecification understanding towards the direction reducing the gap between the\nprovided understanding (from the first phase) and the actual understanding\nimplicitly utilized by LLMs for code generation in the feedback-based prompting\nphase. By improving the understanding with {\\mu}FiX, the code generation\nperformance of LLMs can be largely improved. Our evaluation on two advanced\nLLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing\nwith 15 baselines, demonstrates the effectiveness of {\\mu}FiX ..."
                },
                "authors": [
                    {
                        "name": "Zhao Tian"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "Accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01349v2",
                "updated": "2024-12-19T05:05:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    5,
                    46,
                    3,
                    354,
                    0
                ],
                "published": "2024-03-31T22:22:53Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    22,
                    22,
                    53,
                    6,
                    91,
                    0
                ],
                "title": "Fairness in Large Language Models: A Taxonomic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness in Large Language Models: A Taxonomic Survey"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains. However, despite their promising performance in numerous\nreal-world applications, most of these algorithms lack fairness considerations.\nConsequently, they may lead to discriminatory outcomes against certain\ncommunities, particularly marginalized populations, prompting extensive study\nin fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in\ntraditional machine learning, entails exclusive backgrounds, taxonomies, and\nfulfillment techniques. To this end, this survey presents a comprehensive\noverview of recent advances in the existing literature concerning fair LLMs.\nSpecifically, a brief introduction to LLMs is provided, followed by an analysis\nof factors contributing to bias in LLMs. Additionally, the concept of fairness\nin LLMs is discussed categorically, summarizing metrics for evaluating bias in\nLLMs and existing algorithms for promoting fairness. Furthermore, resources for\nevaluating bias in LLMs, including toolkits and datasets, are summarized.\nFinally, existing research challenges and open questions are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains. However, despite their promising performance in numerous\nreal-world applications, most of these algorithms lack fairness considerations.\nConsequently, they may lead to discriminatory outcomes against certain\ncommunities, particularly marginalized populations, prompting extensive study\nin fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in\ntraditional machine learning, entails exclusive backgrounds, taxonomies, and\nfulfillment techniques. To this end, this survey presents a comprehensive\noverview of recent advances in the existing literature concerning fair LLMs.\nSpecifically, a brief introduction to LLMs is provided, followed by an analysis\nof factors contributing to bias in LLMs. Additionally, the concept of fairness\nin LLMs is discussed categorically, summarizing metrics for evaluating bias in\nLLMs and existing algorithms for promoting fairness. Furthermore, resources for\nevaluating bias in LLMs, including toolkits and datasets, are summarized.\nFinally, existing research challenges and open questions are discussed."
                },
                "authors": [
                    {
                        "name": "Zhibo Chu"
                    },
                    {
                        "name": "Zichong Wang"
                    },
                    {
                        "name": "Wenbin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Zhang"
                },
                "author": "Wenbin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08108v2",
                "updated": "2024-12-19T05:01:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    1,
                    33,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-11T05:23:34Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    23,
                    34,
                    2,
                    346,
                    0
                ],
                "title": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation"
                },
                "summary": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms."
                },
                "authors": [
                    {
                        "name": "Hee-Seon Kim"
                    },
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Changick Kim"
                    }
                ],
                "author_detail": {
                    "name": "Changick Kim"
                },
                "author": "Changick Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14528v1",
                "updated": "2024-12-19T04:51:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T04:51:06Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    51,
                    6,
                    3,
                    354,
                    0
                ],
                "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge\n  Distillation on Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge\n  Distillation on Language Models"
                },
                "summary": "Knowledge distillation (KD) has become a prevalent technique for compressing\nlarge language models (LLMs). Existing KD methods are constrained by the need\nfor identical tokenizers (i.e., vocabularies) between teacher and student\nmodels, limiting their versatility in handling LLMs of different architecture\nfamilies. In this paper, we introduce the Multi-Level Optimal Transport\n(MultiLevelOT), a novel approach that advances the optimal transport for\nuniversal cross-tokenizer knowledge distillation. Our method aligns the logit\ndistributions of the teacher and the student at both token and sequence levels\nusing diverse cost matrices, eliminating the need for dimensional or\ntoken-by-token correspondence. At the token level, MultiLevelOT integrates both\nglobal and local information by jointly optimizing all tokens within a sequence\nto enhance robustness. At the sequence level, we efficiently capture complex\ndistribution structures of logits via the Sinkhorn distance, which approximates\nthe Wasserstein distance for divergence measures. Extensive experiments on\ntasks such as extractive QA, generative QA, and summarization demonstrate that\nthe MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under\nvarious settings. Our approach is robust to different student and teacher\nmodels across model families, architectures, and parameter sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) has become a prevalent technique for compressing\nlarge language models (LLMs). Existing KD methods are constrained by the need\nfor identical tokenizers (i.e., vocabularies) between teacher and student\nmodels, limiting their versatility in handling LLMs of different architecture\nfamilies. In this paper, we introduce the Multi-Level Optimal Transport\n(MultiLevelOT), a novel approach that advances the optimal transport for\nuniversal cross-tokenizer knowledge distillation. Our method aligns the logit\ndistributions of the teacher and the student at both token and sequence levels\nusing diverse cost matrices, eliminating the need for dimensional or\ntoken-by-token correspondence. At the token level, MultiLevelOT integrates both\nglobal and local information by jointly optimizing all tokens within a sequence\nto enhance robustness. At the sequence level, we efficiently capture complex\ndistribution structures of logits via the Sinkhorn distance, which approximates\nthe Wasserstein distance for divergence measures. Extensive experiments on\ntasks such as extractive QA, generative QA, and summarization demonstrate that\nthe MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under\nvarious settings. Our approach is robust to different student and teacher\nmodels across model families, architectures, and parameter sizes."
                },
                "authors": [
                    {
                        "name": "Xiao Cui"
                    },
                    {
                        "name": "Mo Zhu"
                    },
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16646v2",
                "updated": "2024-12-19T04:50:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    50,
                    45,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-25T18:28:26Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    28,
                    26,
                    0,
                    330,
                    0
                ],
                "title": "Self-Generated Critiques Boost Reward Modeling for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Generated Critiques Boost Reward Modeling for Language Models"
                },
                "summary": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy."
                },
                "authors": [
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Zhengxing Chen"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Liang Tan"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Xuewei Wang"
                    },
                    {
                        "name": "Suchin Gururangan"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Melanie Kambadur"
                    },
                    {
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05282v2",
                "updated": "2024-12-19T04:42:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    42,
                    41,
                    3,
                    354,
                    0
                ],
                "published": "2024-07-07T06:50:22Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    6,
                    50,
                    22,
                    6,
                    189,
                    0
                ],
                "title": "UltraEdit: Instruction-based Fine-Grained Image Editing at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraEdit: Instruction-based Fine-Grained Image Editing at Scale"
                },
                "summary": "This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io."
                },
                "authors": [
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Rujie Wu"
                    },
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Peiyu Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00869v2",
                "updated": "2024-12-19T04:38:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    38,
                    59,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-01T16:15:14Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    15,
                    14,
                    6,
                    336,
                    0
                ],
                "title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to\n  Solve Proportional Analogies via Knowledge-Enhanced Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowledgePrompts: Exploring the Abilities of Large Language Models to\n  Solve Proportional Analogies via Knowledge-Enhanced Prompting"
                },
                "summary": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/"
                },
                "authors": [
                    {
                        "name": "Thilini Wijesiriwardene"
                    },
                    {
                        "name": "Ruwan Wickramarachchi"
                    },
                    {
                        "name": "Sreeram Vennam"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14516v1",
                "updated": "2024-12-19T04:31:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    31,
                    56,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T04:31:56Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    31,
                    56,
                    3,
                    354,
                    0
                ],
                "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model\n  Alignment"
                },
                "summary": "We study the problem of aligning large language models (LLMs) with human\npreference data. Contrastive preference optimization has shown promising\nresults in aligning LLMs with available preference data by optimizing the\nimplicit reward associated with the policy. However, the contrastive objective\nfocuses mainly on the relative values of implicit rewards associated with two\nresponses while ignoring their actual values, resulting in suboptimal alignment\nwith human preferences. To address this limitation, we propose calibrated\ndirect preference optimization (Cal-DPO), a simple yet effective algorithm. We\nshow that substantial improvement in alignment with the given preferences can\nbe achieved simply by calibrating the implicit reward to ensure that the\nlearned implicit rewards are comparable in scale to the ground-truth rewards.\nWe demonstrate the theoretical advantages of Cal-DPO over existing approaches.\nThe results of our experiments on a variety of standard benchmarks show that\nCal-DPO remarkably improves off-the-shelf methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of aligning large language models (LLMs) with human\npreference data. Contrastive preference optimization has shown promising\nresults in aligning LLMs with available preference data by optimizing the\nimplicit reward associated with the policy. However, the contrastive objective\nfocuses mainly on the relative values of implicit rewards associated with two\nresponses while ignoring their actual values, resulting in suboptimal alignment\nwith human preferences. To address this limitation, we propose calibrated\ndirect preference optimization (Cal-DPO), a simple yet effective algorithm. We\nshow that substantial improvement in alignment with the given preferences can\nbe achieved simply by calibrating the implicit reward to ensure that the\nlearned implicit rewards are comparable in scale to the ground-truth rewards.\nWe demonstrate the theoretical advantages of Cal-DPO over existing approaches.\nThe results of our experiments on a variety of standard benchmarks show that\nCal-DPO remarkably improves off-the-shelf methods."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "name": "Mingxiao Li"
                    },
                    {
                        "name": "Vasant G Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant G Honavar"
                },
                "author": "Vasant G Honavar",
                "arxiv_comment": "Accepted by NeurIPS 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13238v2",
                "updated": "2024-12-19T04:30:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    30,
                    24,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-17T16:45:27Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    45,
                    27,
                    1,
                    352,
                    0
                ],
                "title": "SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for\n  Autonomous Vehicles with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for\n  Autonomous Vehicles with Large Language Models"
                },
                "summary": "Recent advancements in autonomous vehicles (AVs) use Large Language Models\n(LLMs) to perform well in normal driving scenarios. However, ensuring safety in\ndynamic, high-risk environments and managing safety-critical long-tail events\nremain significant challenges. To address these issues, we propose SafeDrive, a\nknowledge- and data-driven risk-sensitive decision-making framework to enhance\nAV safety and adaptability. The proposed framework introduces a modular system\ncomprising: (1) a Risk Module for quantifying multi-factor coupled risks\ninvolving driver, vehicle, and road interactions; (2) a Memory Module for\nstoring and retrieving typical scenarios to improve adaptability; (3) a\nLLM-powered Reasoning Module for context-aware safety decision-making; and (4)\na Reflection Module for refining decisions through iterative learning. By\nintegrating knowledge-driven insights with adaptive learning mechanisms, the\nframework ensures robust decision-making under uncertain conditions. Extensive\nevaluations on real-world traffic datasets, including highways (HighD),\nintersections (InD), and roundabouts (RounD), validate the framework's ability\nto enhance decision-making safety (achieving a 100% safety rate), replicate\nhuman-like driving behaviors (with decision alignment exceeding 85%), and adapt\neffectively to unpredictable scenarios. SafeDrive establishes a novel paradigm\nfor integrating knowledge- and data-driven methods, highlighting significant\npotential to improve safety and adaptability of autonomous driving in high-risk\ntraffic scenarios. Project Page: https://mezzi33.github.io/SafeDrive/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous vehicles (AVs) use Large Language Models\n(LLMs) to perform well in normal driving scenarios. However, ensuring safety in\ndynamic, high-risk environments and managing safety-critical long-tail events\nremain significant challenges. To address these issues, we propose SafeDrive, a\nknowledge- and data-driven risk-sensitive decision-making framework to enhance\nAV safety and adaptability. The proposed framework introduces a modular system\ncomprising: (1) a Risk Module for quantifying multi-factor coupled risks\ninvolving driver, vehicle, and road interactions; (2) a Memory Module for\nstoring and retrieving typical scenarios to improve adaptability; (3) a\nLLM-powered Reasoning Module for context-aware safety decision-making; and (4)\na Reflection Module for refining decisions through iterative learning. By\nintegrating knowledge-driven insights with adaptive learning mechanisms, the\nframework ensures robust decision-making under uncertain conditions. Extensive\nevaluations on real-world traffic datasets, including highways (HighD),\nintersections (InD), and roundabouts (RounD), validate the framework's ability\nto enhance decision-making safety (achieving a 100% safety rate), replicate\nhuman-like driving behaviors (with decision alignment exceeding 85%), and adapt\neffectively to unpredictable scenarios. SafeDrive establishes a novel paradigm\nfor integrating knowledge- and data-driven methods, highlighting significant\npotential to improve safety and adaptability of autonomous driving in high-risk\ntraffic scenarios. Project Page: https://mezzi33.github.io/SafeDrive/"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhou"
                    },
                    {
                        "name": "Heye Huang"
                    },
                    {
                        "name": "Boqi Li"
                    },
                    {
                        "name": "Shiyue Zhao"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Jianqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiang Wang"
                },
                "author": "Jianqiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14510v1",
                "updated": "2024-12-19T04:18:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    18,
                    51,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T04:18:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    4,
                    18,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization"
                },
                "summary": "The emergence of Retrieval-augmented generation (RAG) has alleviated the\nissues of outdated and hallucinatory content in the generation of large\nlanguage models (LLMs), yet it still reveals numerous limitations. When a\ngeneral-purpose LLM serves as the RAG generator, it often suffers from\ninadequate response informativeness, response robustness, and citation quality.\nPast approaches to tackle these limitations, either by incorporating additional\nsteps beyond generating responses or optimizing the generator through\nsupervised fine-tuning (SFT), still failed to align with the RAG requirement\nthoroughly. Consequently, optimizing the RAG generator from multiple preference\nperspectives while maintaining its end-to-end LLM form remains a challenge. To\nbridge this gap, we propose Multiple Perspective Preference Alignment for\nRetrieval-Augmented Generation (PA-RAG), a method for optimizing the generator\nof RAG systems to align with RAG requirements comprehensively. Specifically, we\nconstruct high-quality instruction fine-tuning data and multi-perspective\npreference data by sampling varied quality responses from the generator across\ndifferent prompt documents quality scenarios. Subsequently, we optimize the\ngenerator using SFT and Direct Preference Optimization (DPO). Extensive\nexperiments conducted on four question-answer datasets across three LLMs\ndemonstrate that PA-RAG can significantly enhance the performance of RAG\ngenerators. Our code and datasets are available at\nhttps://github.com/wujwyi/PA-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Retrieval-augmented generation (RAG) has alleviated the\nissues of outdated and hallucinatory content in the generation of large\nlanguage models (LLMs), yet it still reveals numerous limitations. When a\ngeneral-purpose LLM serves as the RAG generator, it often suffers from\ninadequate response informativeness, response robustness, and citation quality.\nPast approaches to tackle these limitations, either by incorporating additional\nsteps beyond generating responses or optimizing the generator through\nsupervised fine-tuning (SFT), still failed to align with the RAG requirement\nthoroughly. Consequently, optimizing the RAG generator from multiple preference\nperspectives while maintaining its end-to-end LLM form remains a challenge. To\nbridge this gap, we propose Multiple Perspective Preference Alignment for\nRetrieval-Augmented Generation (PA-RAG), a method for optimizing the generator\nof RAG systems to align with RAG requirements comprehensively. Specifically, we\nconstruct high-quality instruction fine-tuning data and multi-perspective\npreference data by sampling varied quality responses from the generator across\ndifferent prompt documents quality scenarios. Subsequently, we optimize the\ngenerator using SFT and Direct Preference Optimization (DPO). Extensive\nexperiments conducted on four question-answer datasets across three LLMs\ndemonstrate that PA-RAG can significantly enhance the performance of RAG\ngenerators. Our code and datasets are available at\nhttps://github.com/wujwyi/PA-RAG."
                },
                "authors": [
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Ming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Gao"
                },
                "author": "Ming Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]