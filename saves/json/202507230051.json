[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos Mara Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muoz"
                    },
                    {
                        "name": "Manuel Gil Prez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anas Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Emilio Labra Gayo"
                },
                "author": "Jos Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Khn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Khn"
                },
                "author": "Martin J. Khn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13360v1",
                "updated": "2025-07-04T09:35:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    35,
                    0,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:35:00Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    35,
                    0,
                    4,
                    185,
                    0
                ],
                "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Light Enhancement via Encoder-Decoder Network with Illumination\n  Guidance"
                },
                "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig."
                },
                "authors": [
                    {
                        "name": "Le-Anh Tran"
                    },
                    {
                        "name": "Chung Nguyen Tran"
                    },
                    {
                        "name": "Ngoc-Luu Nguyen"
                    },
                    {
                        "name": "Nhan Cach Dang"
                    },
                    {
                        "name": "Jordi Carrabina"
                    },
                    {
                        "name": "David Castells-Rufas"
                    },
                    {
                        "name": "Minh Son Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Son Nguyen"
                },
                "author": "Minh Son Nguyen",
                "arxiv_comment": "6 pages, 3 figures, ICCCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.15855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15855v1",
                "updated": "2025-07-21T17:59:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:59:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025"
                },
                "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models."
                },
                "authors": [
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Lin F. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin F. Yang"
                },
                "author": "Lin F. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15854v1",
                "updated": "2025-07-21T17:59:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    25,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:59:25Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    25,
                    0,
                    202,
                    0
                ],
                "title": "Overcast mornings and clear evenings in hot Jupiter exoplanet\n  atmospheres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcast mornings and clear evenings in hot Jupiter exoplanet\n  atmospheres"
                },
                "summary": "Aerosols is an old topic in the young field of exoplanet atmospheres.\nUnderstanding what they are, how they form, and where they go has long provided\na fertile playground for theorists. For observers, however, aerosols have been\na multi-decade migraine, as their chronic presence hides atmospheric features.\nFor hot Jupiters, the large day-night temperature contrast drives inhomogeneous\nthermal structures and aerosol distribution, leading to different limb\nproperties probed by transit spectra. We present JWST NIRISS/SOSS spectra of\nmorning and evening limbs for nine gas giants with equilibrium temperatures of\n~800-1700 K. By measuring feature size of the 1.4 $\\mu$m water band for both\nlimbs, we found three planets (WASP-39 b, WASP-94 Ab, and WASP-17 b) show\nprominent ($>$5$\\sigma$) limb-limb atmospheric opacity difference with muted\nmorning and clear evening limbs. The heavily muted water features on morning\nlimbs indicate high-altitude (0.1 to 0.01 mbar) aerosols. To simultaneously\nhave clear evening limbs requires processes with timescales ($\\sim$day)\ncomparable to advection to remove these lofted grains, and we found that both\ndownwelling flow and dayside cloud evaporation could be plausible mechanisms.\nWe hypothesize an empirical boundary--termed the \"asymmetry horizon\"--in\ntemperature-gravity space that marks the transition where inhomogeneous aerosol\ncoverage begins to emerge. Heterogeneous aerosol coverage is common among hot\nJupiters. If unrecognized, limb averaging suppresses spectral features,\nmimicking high-mean-molecular-weight atmospheres, inflating inferred\nmetallicity by up to 2 dex, and underestimating limb temperatures by as much as\nhalf. Finally, we introduce the Limb Spectroscopy Metric (LSM) to predict limb\nspectral feature size based on planet parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerosols is an old topic in the young field of exoplanet atmospheres.\nUnderstanding what they are, how they form, and where they go has long provided\na fertile playground for theorists. For observers, however, aerosols have been\na multi-decade migraine, as their chronic presence hides atmospheric features.\nFor hot Jupiters, the large day-night temperature contrast drives inhomogeneous\nthermal structures and aerosol distribution, leading to different limb\nproperties probed by transit spectra. We present JWST NIRISS/SOSS spectra of\nmorning and evening limbs for nine gas giants with equilibrium temperatures of\n~800-1700 K. By measuring feature size of the 1.4 $\\mu$m water band for both\nlimbs, we found three planets (WASP-39 b, WASP-94 Ab, and WASP-17 b) show\nprominent ($>$5$\\sigma$) limb-limb atmospheric opacity difference with muted\nmorning and clear evening limbs. The heavily muted water features on morning\nlimbs indicate high-altitude (0.1 to 0.01 mbar) aerosols. To simultaneously\nhave clear evening limbs requires processes with timescales ($\\sim$day)\ncomparable to advection to remove these lofted grains, and we found that both\ndownwelling flow and dayside cloud evaporation could be plausible mechanisms.\nWe hypothesize an empirical boundary--termed the \"asymmetry horizon\"--in\ntemperature-gravity space that marks the transition where inhomogeneous aerosol\ncoverage begins to emerge. Heterogeneous aerosol coverage is common among hot\nJupiters. If unrecognized, limb averaging suppresses spectral features,\nmimicking high-mean-molecular-weight atmospheres, inflating inferred\nmetallicity by up to 2 dex, and underestimating limb temperatures by as much as\nhalf. Finally, we introduce the Limb Spectroscopy Metric (LSM) to predict limb\nspectral feature size based on planet parameters."
                },
                "authors": [
                    {
                        "name": "Guangwei Fu"
                    },
                    {
                        "name": "Sagnick Mukherjee"
                    },
                    {
                        "name": "Kevin B. Stevenson"
                    },
                    {
                        "name": "David K. Sing"
                    },
                    {
                        "name": "Reza Ashtari"
                    },
                    {
                        "name": "Nathan Mayne"
                    },
                    {
                        "name": "Joshua D. Lothringer"
                    },
                    {
                        "name": "Maria Zamyatina"
                    },
                    {
                        "name": "Stephen P. Schmidt"
                    },
                    {
                        "name": "Carlos Gascn"
                    },
                    {
                        "name": "Natalie H. Allen"
                    },
                    {
                        "name": "Katherine A. Bennett"
                    },
                    {
                        "name": "Mercedes Lpez-Morales"
                    }
                ],
                "author_detail": {
                    "name": "Mercedes Lpez-Morales"
                },
                "author": "Mercedes Lpez-Morales",
                "arxiv_comment": "Accepted to ApJL, JWST is on a hot(Jupiter) streak!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15852v2",
                "updated": "2025-07-22T10:51:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    10,
                    51,
                    42,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-21T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    2,
                    0,
                    202,
                    0
                ],
                "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction"
                },
                "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation."
                },
                "authors": [
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Songxin He"
                    },
                    {
                        "name": "Jianfan Lin"
                    },
                    {
                        "name": "Junsong Tang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "project page: https://rookiexiong7.github.io/projects/SeC/ ; code:\n  https://github.com/OpenIXCLab/SeC ; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15851v1",
                "updated": "2025-07-21T17:59:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:59:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Other Mind: How Language Models Exhibit Human Temporal Cognition"
                },
                "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io."
                },
                "authors": [
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Chubo Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "arxiv_comment": "12 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15850v1",
                "updated": "2025-07-21T17:58:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:58:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3LM: Bridging Arabic, STEM, and Code through Benchmarking"
                },
                "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."
                },
                "authors": [
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15849v1",
                "updated": "2025-07-21T17:56:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:56:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Language Mixing on Bilingual LLM Reasoning"
                },
                "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior."
                },
                "authors": [
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Jiayi Xin"
                    },
                    {
                        "name": "Miranda Muqing Miao"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15839v1",
                "updated": "2025-07-21T17:51:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    51,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    51,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs"
                },
                "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation."
                },
                "authors": [
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Sam Schafft"
                    },
                    {
                        "name": "Nicholas Hale"
                    },
                    {
                        "name": "John Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "John Alfaro"
                },
                "author": "John Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13334v2",
                "updated": "2025-07-21T17:48:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    48,
                    18,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-17T17:50:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "A Survey of Context Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Context Engineering for Large Language Models"
                },
                "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jiazhi Liu"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Tianze Xia"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Liu"
                },
                "author": "Shenghua Liu",
                "arxiv_comment": "ongoing work; 166 pages, 1411 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15836v1",
                "updated": "2025-07-21T17:47:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    47,
                    33,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:47:33Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    47,
                    33,
                    0,
                    202,
                    0
                ],
                "title": "Optimizing Canaries for Privacy Auditing with Metagradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Canaries for Privacy Auditing with Metagradient Descent"
                },
                "summary": "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD."
                },
                "authors": [
                    {
                        "name": "Matteo Boglioni"
                    },
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Andrew Ilyas"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Steven Wu"
                },
                "author": "Zhiwei Steven Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15828v1",
                "updated": "2025-07-21T17:37:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    37,
                    23,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:37:23Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    37,
                    23,
                    0,
                    202,
                    0
                ],
                "title": "Investigating the Use of LLMs for Evidence Briefings Generation in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Use of LLMs for Evidence Briefings Generation in\n  Software Engineering"
                },
                "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results."
                },
                "authors": [
                    {
                        "name": "Mauro Marcelino"
                    },
                    {
                        "name": "Marcos Alves"
                    },
                    {
                        "name": "Bianca Trinkenreich"
                    },
                    {
                        "name": "Bruno Cartaxo"
                    },
                    {
                        "name": "Srgio Soares"
                    },
                    {
                        "name": "Simone D. J. Barbosa"
                    },
                    {
                        "name": "Marcos Kalinowski"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Kalinowski"
                },
                "author": "Marcos Kalinowski",
                "arxiv_comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15826v1",
                "updated": "2025-07-21T17:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    36,
                    3,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:36:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    36,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language\n  Music Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language\n  Music Recommendation"
                },
                "summary": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks."
                },
                "authors": [
                    {
                        "name": "Alessandro B. Melchiorre"
                    },
                    {
                        "name": "Elena V. Epure"
                    },
                    {
                        "name": "Shahed Masoudian"
                    },
                    {
                        "name": "Gustavo Escobedo"
                    },
                    {
                        "name": "Anna Hausberger"
                    },
                    {
                        "name": "Manuel Moussallam"
                    },
                    {
                        "name": "Markus Schedl"
                    }
                ],
                "author_detail": {
                    "name": "Markus Schedl"
                },
                "author": "Markus Schedl",
                "arxiv_doi": "10.1145/3705328.3748020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3748020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.15826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15825v1",
                "updated": "2025-07-21T17:33:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    33,
                    15,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:33:15Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    33,
                    15,
                    0,
                    202,
                    0
                ],
                "title": "ACS: An interactive framework for conformal selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACS: An interactive framework for conformal selection"
                },
                "summary": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Yu Gui"
                    },
                    {
                        "name": "Ying Jin"
                    },
                    {
                        "name": "Yash Nair"
                    },
                    {
                        "name": "Zhimei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhimei Ren"
                },
                "author": "Zhimei Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15822v1",
                "updated": "2025-07-21T17:30:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:30:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Do AI models help produce verified bug fixes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do AI models help produce verified bug fixes?"
                },
                "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair."
                },
                "authors": [
                    {
                        "name": "Li Huang"
                    },
                    {
                        "name": "Ilgiz Mustafin"
                    },
                    {
                        "name": "Marco Piccioni"
                    },
                    {
                        "name": "Alessandro Schena"
                    },
                    {
                        "name": "Reto Weber"
                    },
                    {
                        "name": "Bertrand Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Meyer"
                },
                "author": "Bertrand Meyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15821v1",
                "updated": "2025-07-21T17:29:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    29,
                    21,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    29,
                    21,
                    0,
                    202,
                    0
                ],
                "title": "Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for\n  Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for\n  Subjective Tasks"
                },
                "summary": "LLM use in annotation is becoming widespread, and given LLMs' overall\npromising performance and speed, simply \"reviewing\" LLM annotations in\ninterpretive tasks can be tempting. In subjective annotation tasks with\nmultiple plausible answers, reviewing LLM outputs can change the label\ndistribution, impacting both the evaluation of LLM performance, and analysis\nusing these labels in a social science task downstream. We conducted a\npre-registered experiment with 410 unique annotators and over 7,000 annotations\ntesting three AI assistance conditions against controls, using two models, and\ntwo datasets. We find that presenting crowdworkers with LLM-generated\nannotation suggestions did not make them faster, but did improve their\nself-reported confidence in the task. More importantly, annotators strongly\ntook the LLM suggestions, significantly changing the label distribution\ncompared to the baseline. When these labels created with LLM assistance are\nused to evaluate LLM performance, reported model performance significantly\nincreases. We believe our work underlines the importance of understanding the\nimpact of LLM-assisted annotation on subjective, qualitative tasks, on the\ncreation of gold data for training and testing, and on the evaluation of NLP\nsystems on subjective tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM use in annotation is becoming widespread, and given LLMs' overall\npromising performance and speed, simply \"reviewing\" LLM annotations in\ninterpretive tasks can be tempting. In subjective annotation tasks with\nmultiple plausible answers, reviewing LLM outputs can change the label\ndistribution, impacting both the evaluation of LLM performance, and analysis\nusing these labels in a social science task downstream. We conducted a\npre-registered experiment with 410 unique annotators and over 7,000 annotations\ntesting three AI assistance conditions against controls, using two models, and\ntwo datasets. We find that presenting crowdworkers with LLM-generated\nannotation suggestions did not make them faster, but did improve their\nself-reported confidence in the task. More importantly, annotators strongly\ntook the LLM suggestions, significantly changing the label distribution\ncompared to the baseline. When these labels created with LLM assistance are\nused to evaluate LLM performance, reported model performance significantly\nincreases. We believe our work underlines the importance of understanding the\nimpact of LLM-assisted annotation on subjective, qualitative tasks, on the\ncreation of gold data for training and testing, and on the evaluation of NLP\nsystems on subjective tasks."
                },
                "authors": [
                    {
                        "name": "Hope Schroeder"
                    },
                    {
                        "name": "Deb Roy"
                    },
                    {
                        "name": "Jad Kabbara"
                    }
                ],
                "author_detail": {
                    "name": "Jad Kabbara"
                },
                "author": "Jad Kabbara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15819v1",
                "updated": "2025-07-21T17:25:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    25,
                    32,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:25:32Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    25,
                    32,
                    0,
                    202,
                    0
                ],
                "title": "Euclid preparation: Expected constraints on initial conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation: Expected constraints on initial conditions"
                },
                "summary": "The Euclid mission of the European Space Agency will deliver galaxy and\ncosmic shear surveys, which will be used to constrain initial conditions and\nstatistics of primordial fluctuations. We present highlights for the Euclid\nscientific capability to test initial conditions beyond LCDM with the main\nprobes, i.e. 3D galaxy clustering from the spectroscopic survey, the\ntomographic approach to 3x2pt statistics from photometric galaxy survey, and\ntheir combination. We provide Fisher forecasts from the combination of Euclid\nspectroscopic and photometric surveys for spatial curvature, running of the\nspectral index of the power spectrum of curvature perturbations, isocurvature\nperturbations, and primordial features. For the parameters of these models we\nalso provide the combination of Euclid forecasts (pessimistic and optimistic)\nwith current and future measurements of the cosmic microwave background (CMB)\nanisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide\nFisher forecasts for how the power spectrum and bispectrum from the Euclid\nspectroscopic survey will constrain the local, equilateral, and orthogonal\nshapes of primordial non-Gaussianity. We also review how Bayesian field-level\ninference of primordial non-Gaussianity can constrain local primordial\nnon-Gaussianity. We show how Euclid, with its unique combination of the main\nprobes, will provide the tightest constraints on low redshift to date. By\ntargeting a markedly different range in redshift and scale, Euclid's expected\nuncertainties are complementary to those obtained by CMB primary anisotropy,\nreturning the tightest combined constraints on the physics of the early\nUniverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Euclid mission of the European Space Agency will deliver galaxy and\ncosmic shear surveys, which will be used to constrain initial conditions and\nstatistics of primordial fluctuations. We present highlights for the Euclid\nscientific capability to test initial conditions beyond LCDM with the main\nprobes, i.e. 3D galaxy clustering from the spectroscopic survey, the\ntomographic approach to 3x2pt statistics from photometric galaxy survey, and\ntheir combination. We provide Fisher forecasts from the combination of Euclid\nspectroscopic and photometric surveys for spatial curvature, running of the\nspectral index of the power spectrum of curvature perturbations, isocurvature\nperturbations, and primordial features. For the parameters of these models we\nalso provide the combination of Euclid forecasts (pessimistic and optimistic)\nwith current and future measurements of the cosmic microwave background (CMB)\nanisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide\nFisher forecasts for how the power spectrum and bispectrum from the Euclid\nspectroscopic survey will constrain the local, equilateral, and orthogonal\nshapes of primordial non-Gaussianity. We also review how Bayesian field-level\ninference of primordial non-Gaussianity can constrain local primordial\nnon-Gaussianity. We show how Euclid, with its unique combination of the main\nprobes, will provide the tightest constraints on low redshift to date. By\ntargeting a markedly different range in redshift and scale, Euclid's expected\nuncertainties are complementary to those obtained by CMB primary anisotropy,\nreturning the tightest combined constraints on the physics of the early\nUniverse."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "A. Andrews"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "G. Alestas"
                    },
                    {
                        "name": "N. Bartolo"
                    },
                    {
                        "name": "J. R. Bermejo-Climent"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "D. Paoletti"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "A. Achcarro"
                    },
                    {
                        "name": "G. Caas-Herrera"
                    },
                    {
                        "name": "J. Jasche"
                    },
                    {
                        "name": "G. Lavaux"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "L. Amendola"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "D. Bagot"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "M. Douspis"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. M. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kmmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. J. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "C. Rosset"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "A. Zacchei"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "J. Martn-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. A. Nucita"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "S. Avila"
                    },
                    {
                        "name": "A. Balaguera-Antolinez"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "H. Bhringer"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Conseil"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "F. De Paolis"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Daz-Snchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "P. Dimauro"
                    },
                    {
                        "name": "A. Enia"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "A. Fontana"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "A. Gruppuso"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "C. Hernndez-Monteagudo"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "S. Joudaki"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "M. Lattanzi"
                    },
                    {
                        "name": "V. Le Brun"
                    },
                    {
                        "name": "J. Le Graet"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Migliaccio"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "L. Pagano"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "P. Reimberg"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "G. Rodighiero"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahln"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "F. Vernizzi"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_comment": "Abstract abridged, 25 pages, 6 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12251v2",
                "updated": "2025-07-21T17:22:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    22,
                    35,
                    0,
                    202,
                    0
                ],
                "published": "2025-06-13T21:56:52Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    56,
                    52,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Multi-Camera Tokenization with Triplanes for End-to-End\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-Camera Tokenization with Triplanes for End-to-End\n  Driving"
                },
                "summary": "Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations."
                },
                "authors": [
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Cristiano Saltori"
                    },
                    {
                        "name": "Yurong You"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Wenjie Luo"
                    },
                    {
                        "name": "Marco Pavone"
                    }
                ],
                "author_detail": {
                    "name": "Marco Pavone"
                },
                "author": "Marco Pavone",
                "arxiv_comment": "12 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15815v1",
                "updated": "2025-07-21T17:21:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    21,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:21:14Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    21,
                    14,
                    0,
                    202,
                    0
                ],
                "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra"
                },
                "summary": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations."
                },
                "authors": [
                    {
                        "name": "Seth Karten"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Zihan Ding"
                    },
                    {
                        "name": "Samuel Kleiner"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "arxiv_comment": "27 pages, 6 figures, Code:\n  https://github.com/sethkarten/LLM-Economist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15810v1",
                "updated": "2025-07-21T17:12:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    12,
                    30,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:12:30Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    12,
                    30,
                    0,
                    202,
                    0
                ],
                "title": "Empirical Likelihood Based Inference for a Divergence Measure Based on\n  Survival Extropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Likelihood Based Inference for a Divergence Measure Based on\n  Survival Extropy"
                },
                "summary": "We consider a divergence measure based on survival extropy and derive its\nnon-parametric estimators based on U-statistics, empirical\ndistribution-functions, and kernel density. Further, we construct confidence\nintervals for the divergence measure using the jackknife empirical likelihood\n(JEL) method and the normal approximation method with a jackknife\npseudo-value-based variance estimator. A comprehensive simulation study is\nconducted to compare the performance of the measure with existing divergence\nmeasures. In addition, we assess the finite-sample performance of various\nestimators for the measure. The findings highlight the effectiveness of the\ndivergence measure and its estimators in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a divergence measure based on survival extropy and derive its\nnon-parametric estimators based on U-statistics, empirical\ndistribution-functions, and kernel density. Further, we construct confidence\nintervals for the divergence measure using the jackknife empirical likelihood\n(JEL) method and the normal approximation method with a jackknife\npseudo-value-based variance estimator. A comprehensive simulation study is\nconducted to compare the performance of the measure with existing divergence\nmeasures. In addition, we assess the finite-sample performance of various\nestimators for the measure. The findings highlight the effectiveness of the\ndivergence measure and its estimators in practical applications."
                },
                "authors": [
                    {
                        "name": "Naresh Garg"
                    },
                    {
                        "name": "Isha Dewan"
                    },
                    {
                        "name": "Sudheesh Kumar Kattumannil"
                    }
                ],
                "author_detail": {
                    "name": "Sudheesh Kumar Kattumannil"
                },
                "author": "Sudheesh Kumar Kattumannil",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15788v1",
                "updated": "2025-07-21T16:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    47,
                    59,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:47:59Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    47,
                    59,
                    0,
                    202,
                    0
                ],
                "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability."
                },
                "authors": [
                    {
                        "name": "Sneheel Sarangi"
                    },
                    {
                        "name": "Hanan Salam"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Salam"
                },
                "author": "Hanan Salam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15782v1",
                "updated": "2025-07-21T16:37:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    50,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:37:50Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    50,
                    0,
                    202,
                    0
                ],
                "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object\n  Collection in Large Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved LLM and Motion Planning for Generalized Multi-Object\n  Collection in Large Scene Graphs"
                },
                "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs."
                },
                "authors": [
                    {
                        "name": "Ruochu Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Fumin Zhang"
                    },
                    {
                        "name": "Mengxue Hou"
                    }
                ],
                "author_detail": {
                    "name": "Mengxue Hou"
                },
                "author": "Mengxue Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11558v3",
                "updated": "2025-07-21T16:37:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    0,
                    0,
                    202,
                    0
                ],
                "published": "2025-06-13T08:13:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs"
                },
                "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."
                },
                "authors": [
                    {
                        "name": "Bo-Cheng Chiu"
                    },
                    {
                        "name": "Jen-Jee Chen"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Feng-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Chi Chen"
                },
                "author": "Feng-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15779v1",
                "updated": "2025-07-21T16:35:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    35,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:35:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    35,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Reservoir Computing as a Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir Computing as a Language Model"
                },
                "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance."
                },
                "authors": [
                    {
                        "name": "Felix Kster"
                    },
                    {
                        "name": "Atsushi Uchida"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Uchida"
                },
                "author": "Atsushi Uchida",
                "arxiv_comment": "8 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15778v1",
                "updated": "2025-07-21T16:34:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:34:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR."
                },
                "authors": [
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15776v1",
                "updated": "2025-07-21T16:30:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    30,
                    42,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    30,
                    42,
                    0,
                    202,
                    0
                ],
                "title": "Dissociating model architectures from inference computations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissociating model architectures from inference computations"
                },
                "summary": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures."
                },
                "authors": [
                    {
                        "name": "Noor Sajid"
                    },
                    {
                        "name": "Johan Medrano"
                    }
                ],
                "author_detail": {
                    "name": "Johan Medrano"
                },
                "author": "Johan Medrano",
                "arxiv_doi": "10.1080/17588928.2025.2532604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/17588928.2025.2532604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.15776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "3 pages, 1 figure",
                "arxiv_journal_ref": "Cognitive Neuroscience PCNS 2025",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15771v1",
                "updated": "2025-07-21T16:27:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:27:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Left Leaning Models: AI Assumptions on Economic Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left Leaning Models: AI Assumptions on Economic Policy"
                },
                "summary": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models."
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin",
                "arxiv_comment": "8 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15770v1",
                "updated": "2025-07-21T16:26:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    26,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:26:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    26,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems\n  Through LLM-based Agent Intention Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems\n  Through LLM-based Agent Intention Mining"
                },
                "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085."
                },
                "authors": [
                    {
                        "name": "Yifan Shen"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Qun Ma"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15769v1",
                "updated": "2025-07-21T16:25:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    25,
                    44,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:25:44Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    25,
                    44,
                    0,
                    202,
                    0
                ],
                "title": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave\n  Vehicular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave\n  Vehicular Networks"
                },
                "summary": "Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Ahmad M. Nazar"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Mohamed Y. Selim"
                    },
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Daji Qiao"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil",
                "arxiv_comment": "Accepted in IEEE Asilomar Conference on Signals, Systems, and\n  Computers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15768v1",
                "updated": "2025-07-21T16:25:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    25,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:25:41Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    25,
                    41,
                    0,
                    202,
                    0
                ],
                "title": "Toward an event-level analysis of hadron structure using differential\n  programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward an event-level analysis of hadron structure using differential\n  programming"
                },
                "summary": "Reconstructing the internal properties of hadrons in terms of fundamental\nquark and gluon degrees of freedom is a central goal in nuclear and particle\nphysics. This effort lies at the core of major experimental programs, such as\nthe Jefferson Lab 12 GeV program and the upcoming Electron-Ion Collider. A\nprimary challenge is the inherent inverse problem: converting large-scale\nobservational data from collision events into the fundamental quantum\ncorrelation functions (QCFs) that characterize the microscopic structure of\nhadronic systems within the theory of QCD. Recent advances in scientific\ncomputing and machine learning have opened new avenues for addressing this\nchallenge using deep learning techniques. A particularly promising direction is\nthe integration of theoretical calculations and experimental simulations into a\nunified framework capable of reconstructing QCFs directly from event-level\ninformation. In this work, we introduce a differential sampling method called\nthe local orthogonal inverse transform sampling (LOITS) algorithm. We validate\nits performance through a closure test, demonstrating the accurate\nreconstruction of a test distribution from sampled events using Generative\nAdversarial Networks. The LOITS algorithm provides a central building block for\naddressing inverse problems involving QCFs and enables end-to-end inference\npipelines within the framework of differential programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the internal properties of hadrons in terms of fundamental\nquark and gluon degrees of freedom is a central goal in nuclear and particle\nphysics. This effort lies at the core of major experimental programs, such as\nthe Jefferson Lab 12 GeV program and the upcoming Electron-Ion Collider. A\nprimary challenge is the inherent inverse problem: converting large-scale\nobservational data from collision events into the fundamental quantum\ncorrelation functions (QCFs) that characterize the microscopic structure of\nhadronic systems within the theory of QCD. Recent advances in scientific\ncomputing and machine learning have opened new avenues for addressing this\nchallenge using deep learning techniques. A particularly promising direction is\nthe integration of theoretical calculations and experimental simulations into a\nunified framework capable of reconstructing QCFs directly from event-level\ninformation. In this work, we introduce a differential sampling method called\nthe local orthogonal inverse transform sampling (LOITS) algorithm. We validate\nits performance through a closure test, demonstrating the accurate\nreconstruction of a test distribution from sampled events using Generative\nAdversarial Networks. The LOITS algorithm provides a central building block for\naddressing inverse problems involving QCFs and enables end-to-end inference\npipelines within the framework of differential programming."
                },
                "authors": [
                    {
                        "name": "Kevin Braga"
                    },
                    {
                        "name": "Markus Diefenthaler"
                    },
                    {
                        "name": "Steven Goldenberg"
                    },
                    {
                        "name": "Daniel Lersch"
                    },
                    {
                        "name": "Yaohang Li"
                    },
                    {
                        "name": "Jian-Wei Qiu"
                    },
                    {
                        "name": "Kishansingh Rajput"
                    },
                    {
                        "name": "Felix Ringer"
                    },
                    {
                        "name": "Nobuo Sato"
                    },
                    {
                        "name": "Malachi Schram"
                    }
                ],
                "author_detail": {
                    "name": "Malachi Schram"
                },
                "author": "Malachi Schram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15761v1",
                "updated": "2025-07-21T16:17:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    17,
                    25,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:17:25Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    17,
                    25,
                    0,
                    202,
                    0
                ],
                "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in\n  Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in\n  Smart Contracts"
                },
                "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Yifan Liao"
                    },
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08985v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08985v4",
                "updated": "2025-07-21T16:16:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    16,
                    56,
                    0,
                    202,
                    0
                ],
                "published": "2024-12-12T06:38:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08985v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08985v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15758v1",
                "updated": "2025-07-21T16:14:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    14,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:14:41Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    14,
                    41,
                    0,
                    202,
                    0
                ],
                "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization"
                },
                "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Yiwen Qiu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15752v1",
                "updated": "2025-07-21T16:08:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    8,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:08:19Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    8,
                    19,
                    0,
                    202,
                    0
                ],
                "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue"
                },
                "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zhu"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Yaxuan Li"
                    },
                    {
                        "name": "Syang Zhou"
                    },
                    {
                        "name": "Shijing Cai"
                    },
                    {
                        "name": "Malgorzata Lazuka"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "arxiv_comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07980v2",
                "updated": "2025-07-21T15:58:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    58,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-14T09:58:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    58,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Mapping reionization bubbles in the JWST era II: inferring the position\n  and characteristic size of individual bubbles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping reionization bubbles in the JWST era II: inferring the position\n  and characteristic size of individual bubbles"
                },
                "summary": "The James Webb Space Telescope (JWST) is discovering an increasing number of\ngalaxies well into the early stages of the Epoch of Reionization (EoR). Many of\nthese galaxies are clustered with strong Lyman alpha (Ly$\\alpha$) emission,\nmotivating the presence of surrounding cosmic HII regions that would facilitate\nLy$\\alpha$ transmission through the intergalactic medium (IGM). Detecting these\nHII \"bubbles\" would allow us to connect their growth to the properties of the\ngalaxies inside them. Here we develop a new forward-modeling framework to\nestimate the local HII region size and location from Ly$\\alpha$ spectra of\ngalaxy groups in the early stages of the EoR. Our model takes advantage of the\ncomplementary information provided by neighboring sightlines through the IGM.\nOur forward models sample the main sources of uncertainty, including: (i) the\nglobal neutral fraction; (ii) EoR morphology; (iii) emergent Ly$\\alpha$\nemission; and (iv) NIRSpec instrument noise. Depending on the availability of\ncomplementary nebular lines, $\\sim$ 0.006 $\\unicode{x2013}$ 0.01 galaxies per\ncMpc$^3$, are required to be $\\gtrsim$95\\% confident that the HII bubble\nlocation and size recovered by our method is accurate to within $\\sim$ 1\ncomoving Mpc. This corresponds roughly to tens of galaxies at\n$z\\sim7\\unicode{x2013}8$ in $\\sim$2x2 tiled pointing with JWST NIRSpec. Such a\nsample is achievable with a targeted survey with completeness down to $M_{\\rm\nUV}^{\\rm min}\\lesssim$ -19 $\\unicode{x2013}$ -17, depending on the over-density\nof the field. We test our method on 3D EoR simulations as well as misspecified\nequivalent width distributions, in both cases accurately recovering the HII\nregion surrounding targeted galaxy groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) is discovering an increasing number of\ngalaxies well into the early stages of the Epoch of Reionization (EoR). Many of\nthese galaxies are clustered with strong Lyman alpha (Ly$\\alpha$) emission,\nmotivating the presence of surrounding cosmic HII regions that would facilitate\nLy$\\alpha$ transmission through the intergalactic medium (IGM). Detecting these\nHII \"bubbles\" would allow us to connect their growth to the properties of the\ngalaxies inside them. Here we develop a new forward-modeling framework to\nestimate the local HII region size and location from Ly$\\alpha$ spectra of\ngalaxy groups in the early stages of the EoR. Our model takes advantage of the\ncomplementary information provided by neighboring sightlines through the IGM.\nOur forward models sample the main sources of uncertainty, including: (i) the\nglobal neutral fraction; (ii) EoR morphology; (iii) emergent Ly$\\alpha$\nemission; and (iv) NIRSpec instrument noise. Depending on the availability of\ncomplementary nebular lines, $\\sim$ 0.006 $\\unicode{x2013}$ 0.01 galaxies per\ncMpc$^3$, are required to be $\\gtrsim$95\\% confident that the HII bubble\nlocation and size recovered by our method is accurate to within $\\sim$ 1\ncomoving Mpc. This corresponds roughly to tens of galaxies at\n$z\\sim7\\unicode{x2013}8$ in $\\sim$2x2 tiled pointing with JWST NIRSpec. Such a\nsample is achievable with a targeted survey with completeness down to $M_{\\rm\nUV}^{\\rm min}\\lesssim$ -19 $\\unicode{x2013}$ -17, depending on the over-density\nof the field. We test our method on 3D EoR simulations as well as misspecified\nequivalent width distributions, in both cases accurately recovering the HII\nregion surrounding targeted galaxy groups."
                },
                "authors": [
                    {
                        "name": "Ivan Nikoli"
                    },
                    {
                        "name": "Andrei Mesinger"
                    },
                    {
                        "name": "Charlotte A. Mason"
                    },
                    {
                        "name": "Ting-Yi Lu"
                    },
                    {
                        "name": "Mengtao Tang"
                    },
                    {
                        "name": "David Prelogovi"
                    },
                    {
                        "name": "Samuel Gagnon-Hartman"
                    },
                    {
                        "name": "Daniel P. Stark"
                    }
                ],
                "author_detail": {
                    "name": "Daniel P. Stark"
                },
                "author": "Daniel P. Stark",
                "arxiv_comment": "14 pages, 15 figures, accepted to A&A",
                "arxiv_journal_ref": "A&A 699, A323 (July 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09754v2",
                "updated": "2025-07-21T15:48:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    48,
                    8,
                    0,
                    202,
                    0
                ],
                "published": "2024-08-19T07:30:59Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    30,
                    59,
                    0,
                    232,
                    0
                ],
                "title": "Efficient onboard multi-task AI architecture based on self-supervised\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient onboard multi-task AI architecture based on self-supervised\n  learning"
                },
                "summary": "There is growing interest towards the use of AI directly onboard satellites\nfor quick analysis and rapid response to critical events such as natural\ndisasters. This paper presents a blueprint to the mission designer for the\ndevelopment of a modular and efficient deep learning payload to address\nmultiple onboard inference tasks. In particular, we design a self-supervised\nlightweight backbone that provides features to efficient task-specific heads.\nThe latter can be developed independently and with reduced data labeling\nrequirements thanks to the frozen backbone. Experiments on three sample tasks\nof cloud segmentation, flood detection, and marine debris classification on a\n7W embedded system show competitive results with inference quality close to\nhigh-complexity state-of-the-art models and high throughput in excess of 8\nMpx/s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest towards the use of AI directly onboard satellites\nfor quick analysis and rapid response to critical events such as natural\ndisasters. This paper presents a blueprint to the mission designer for the\ndevelopment of a modular and efficient deep learning payload to address\nmultiple onboard inference tasks. In particular, we design a self-supervised\nlightweight backbone that provides features to efficient task-specific heads.\nThe latter can be developed independently and with reduced data labeling\nrequirements thanks to the frozen backbone. Experiments on three sample tasks\nof cloud segmentation, flood detection, and marine debris classification on a\n7W embedded system show competitive results with inference quality close to\nhigh-complexity state-of-the-art models and high throughput in excess of 8\nMpx/s."
                },
                "authors": [
                    {
                        "name": "Gabriele Inzerillo"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_doi": "10.1109/JSTARS.2024.3502776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSTARS.2024.3502776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.09754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06205v2",
                "updated": "2025-07-21T15:45:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    45,
                    54,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-08T16:48:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    48,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "HER-Seg: Holistically Efficient Segmentation for High-Resolution Medical\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HER-Seg: Holistically Efficient Segmentation for High-Resolution Medical\n  Images"
                },
                "summary": "High-resolution segmentation is critical for precise disease diagnosis by\nextracting fine-grained morphological details. Existing hierarchical\nencoder-decoder frameworks have demonstrated remarkable adaptability across\ndiverse medical segmentation tasks. While beneficial, they usually require the\nhuge computation and memory cost when handling large-size segmentation, which\nlimits their applications in foundation model building and real-world clinical\nscenarios. To address this limitation, we propose a holistically efficient\nframework for high-resolution medical image segmentation, called HER-Seg.\nSpecifically, we first devise a computation-efficient image encoder\n(CE-Encoder) to model long-range dependencies with linear complexity while\nmaintaining sufficient representations. In particular, we introduce the\ndual-gated linear attention (DLA) mechanism to perform cascaded token\nfiltering, selectively retaining important tokens while ignoring irrelevant\nones to enhance attention computation efficiency. Then, we introduce a\nmemory-efficient mask decoder (ME-Decoder) to eliminate the demand for the\nhierarchical structure by leveraging cross-scale segmentation decoding.\nExtensive experiments reveal that HER-Seg outperforms state-of-the-arts in\nhigh-resolution medical 2D, 3D and video segmentation tasks. In particular, our\nHER-Seg requires only 0.59GB training GPU memory and 9.39G inference FLOPs per\n1024$\\times$1024 image, demonstrating superior memory and computation\nefficiency. The code is available at https://github.com/xq141839/HER-Seg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution segmentation is critical for precise disease diagnosis by\nextracting fine-grained morphological details. Existing hierarchical\nencoder-decoder frameworks have demonstrated remarkable adaptability across\ndiverse medical segmentation tasks. While beneficial, they usually require the\nhuge computation and memory cost when handling large-size segmentation, which\nlimits their applications in foundation model building and real-world clinical\nscenarios. To address this limitation, we propose a holistically efficient\nframework for high-resolution medical image segmentation, called HER-Seg.\nSpecifically, we first devise a computation-efficient image encoder\n(CE-Encoder) to model long-range dependencies with linear complexity while\nmaintaining sufficient representations. In particular, we introduce the\ndual-gated linear attention (DLA) mechanism to perform cascaded token\nfiltering, selectively retaining important tokens while ignoring irrelevant\nones to enhance attention computation efficiency. Then, we introduce a\nmemory-efficient mask decoder (ME-Decoder) to eliminate the demand for the\nhierarchical structure by leveraging cross-scale segmentation decoding.\nExtensive experiments reveal that HER-Seg outperforms state-of-the-arts in\nhigh-resolution medical 2D, 3D and video segmentation tasks. In particular, our\nHER-Seg requires only 0.59GB training GPU memory and 9.39G inference FLOPs per\n1024$\\times$1024 image, demonstrating superior memory and computation\nefficiency. The code is available at https://github.com/xq141839/HER-Seg."
                },
                "authors": [
                    {
                        "name": "Qing Xu"
                    },
                    {
                        "name": "Zhenye Lou"
                    },
                    {
                        "name": "Chenxin Li"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xiangjian He"
                    },
                    {
                        "name": "Tesema Fiseha Berhanu"
                    },
                    {
                        "name": "Rong Qu"
                    },
                    {
                        "name": "Wenting Duan"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15736v1",
                "updated": "2025-07-21T15:43:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    43,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:43:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    43,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "Understanding Large Language Models' Ability on Interdisciplinary\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Models' Ability on Interdisciplinary\n  Research"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch."
                },
                "authors": [
                    {
                        "name": "Yuanhao Shen"
                    },
                    {
                        "name": "Daniel Xavier de Sousa"
                    },
                    {
                        "name": "Ricardo Maral"
                    },
                    {
                        "name": "Ali Asad"
                    },
                    {
                        "name": "Hongyu Guo"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15729v1",
                "updated": "2025-07-21T15:38:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    38,
                    25,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:38:25Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    38,
                    25,
                    0,
                    202,
                    0
                ],
                "title": "Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks."
                },
                "authors": [
                    {
                        "name": "Jens V. Rppel"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Tim Schreiter"
                    },
                    {
                        "name": "Martin Magnusson"
                    },
                    {
                        "name": "Achim J. Lilienthal"
                    }
                ],
                "author_detail": {
                    "name": "Achim J. Lilienthal"
                },
                "author": "Achim J. Lilienthal",
                "arxiv_comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15728v1",
                "updated": "2025-07-21T15:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    37,
                    33,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    37,
                    33,
                    0,
                    202,
                    0
                ],
                "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokensGen: Harnessing Condensed Tokens for Long Video Generation"
                },
                "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ ."
                },
                "authors": [
                    {
                        "name": "Wenqi Ouyang"
                    },
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Danni Yang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jianlou Si"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan",
                "arxiv_comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15717v1",
                "updated": "2025-07-21T15:27:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    27,
                    32,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:27:32Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    27,
                    32,
                    0,
                    202,
                    0
                ],
                "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological\n  Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological\n  Knowledge and Reasoning"
                },
                "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels."
                },
                "authors": [
                    {
                        "name": "Sahana Srinivasan"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Thaddaeus Wai Soon Lo"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Minjie Zou"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Mingjia Yang"
                    },
                    {
                        "name": "Krithi Pushpanathan"
                    },
                    {
                        "name": "Samantha Yew"
                    },
                    {
                        "name": "Wan Ting Loke"
                    },
                    {
                        "name": "Jocelyn Goh"
                    },
                    {
                        "name": "Yibing Chen"
                    },
                    {
                        "name": "Yiming Kong"
                    },
                    {
                        "name": "Emily Yuelei Fu"
                    },
                    {
                        "name": "Michelle Ongyong Hui"
                    },
                    {
                        "name": "Kristen Nwanyanwu"
                    },
                    {
                        "name": "Amisha Dave"
                    },
                    {
                        "name": "Kelvin Zhenghao Li"
                    },
                    {
                        "name": "Chen-Hsin Sun"
                    },
                    {
                        "name": "Mark Chia"
                    },
                    {
                        "name": "Gabriel Dawei Yang"
                    },
                    {
                        "name": "Wendy Meihua Wong"
                    },
                    {
                        "name": "David Ziyou Chen"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Maxwell Singer"
                    },
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "Lucian V Del Priore"
                    },
                    {
                        "name": "Jost Jonas"
                    },
                    {
                        "name": "Ron Adelman"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    }
                ],
                "author_detail": {
                    "name": "Yih-Chung Tham"
                },
                "author": "Yih-Chung Tham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15715v1",
                "updated": "2025-07-21T15:26:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    26,
                    58,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:26:58Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    26,
                    58,
                    0,
                    202,
                    0
                ],
                "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs"
                },
                "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research."
                },
                "authors": [
                    {
                        "name": "Alina Hyk"
                    },
                    {
                        "name": "Kiera McCormick"
                    },
                    {
                        "name": "Mian Zhong"
                    },
                    {
                        "name": "Ioana Ciuc"
                    },
                    {
                        "name": "Sanjib Sharma"
                    },
                    {
                        "name": "John F Wu"
                    },
                    {
                        "name": "J. E. G. Peek"
                    },
                    {
                        "name": "Kartheik G. Iyer"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17259v2",
                "updated": "2025-07-21T15:24:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    24,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-24T15:39:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "Detecting Benchmark Contamination Through Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Benchmark Contamination Through Watermarking"
                },
                "summary": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, \\eg $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, \\eg $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy."
                },
                "authors": [
                    {
                        "name": "Tom Sander"
                    },
                    {
                        "name": "Pierre Fernandez"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19545v2",
                "updated": "2025-07-21T15:24:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    24,
                    26,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-26T20:34:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    34,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training\n  for Reducing Hallucination in Product QA Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training\n  for Reducing Hallucination in Product QA Agents"
                },
                "summary": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations."
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Michael White"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Toshiaki Koike-Akino"
                    },
                    {
                        "name": "Kieran Parsons"
                    },
                    {
                        "name": "Ye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wang"
                },
                "author": "Ye Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15707v1",
                "updated": "2025-07-21T15:15:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    15,
                    30,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:15:30Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    15,
                    30,
                    0,
                    202,
                    0
                ],
                "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by\n  Different Ways Questions Are Asked?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Large Language Model Performance on Reasoning Tasks Impacted by\n  Different Ways Questions Are Asked?"
                },
                "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance."
                },
                "authors": [
                    {
                        "name": "Seok Hwan Song"
                    },
                    {
                        "name": "Mohna Chakraborty"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Wallapak Tavanapong"
                    }
                ],
                "author_detail": {
                    "name": "Wallapak Tavanapong"
                },
                "author": "Wallapak Tavanapong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13170v2",
                "updated": "2025-07-21T15:14:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    14,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-22T19:01:06Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    19,
                    1,
                    6,
                    2,
                    22,
                    0
                ],
                "title": "Non-equilibrium ionization in the multiphase circumgalactic medium --\n  impact on quasar absorption-line analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-equilibrium ionization in the multiphase circumgalactic medium --\n  impact on quasar absorption-line analyses"
                },
                "summary": "This paper presents an updated framework for studying the ionizing conditions\nand elemental abundances of photoionized, metal-enriched quasar absorption\nsystems. The standard assumption of ionization equilibrium invoked in\nabsorption line analyses requires gas to cool on longer timescales than ionic\nrecombination (t_cool >> t_rec). However, this assumption may not be valid at\nhigh metallicities due to enhanced cooling losses. This work presents a suite\nof time-dependent photoionization (TDP) models that self-consistently solve for\nthe ionization state of rapidly cooling gas irradiated by the extragalactic\nultraviolet background (UVB). The updated framework explores various revised\nUVBs from recent studies, a range of initial temperatures, and different\nelemental abundance patterns to quantify the effects of TDP on the observed ion\nfractions. A metal-enriched ($\\mathrm{[\\alpha/H]}=0.6_{-0.1}^{+0.2}$) C IV\nabsorption system at z ~ 1 previously studied using photoionization equilibrium\n(PIE) models is re-examined under the TDP framework. The main findings are as\nfollows: (1) varying prescriptions for the underlying UVB or adopting initial\ntemperatures T_0 < 1e6 K (with the starting ionization state in collisional\nequilibrium) change TDP ion fractions by up to a factor of three and ten\nrespectively, but the adopted relative elemental abundance pattern affects ion\nfractions by at most 40%; (2) the inferred gas densities are consistent between\nPIE and TDP, but under TDP solar metallicity cannot be ruled out at more than\n2-$\\sigma$ significance and a non-solar [C/$\\alpha$]$\\approx 0.25$ is robustly\nconstrained from the observed relative ion abundances. Extending the TDP\nanalyses to a larger sample of super-solar absorption components with high\nsignal-to-noise absorption spectra is needed to quantify the fraction of metal\nabsorbers originating in rapid cooling gas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an updated framework for studying the ionizing conditions\nand elemental abundances of photoionized, metal-enriched quasar absorption\nsystems. The standard assumption of ionization equilibrium invoked in\nabsorption line analyses requires gas to cool on longer timescales than ionic\nrecombination (t_cool >> t_rec). However, this assumption may not be valid at\nhigh metallicities due to enhanced cooling losses. This work presents a suite\nof time-dependent photoionization (TDP) models that self-consistently solve for\nthe ionization state of rapidly cooling gas irradiated by the extragalactic\nultraviolet background (UVB). The updated framework explores various revised\nUVBs from recent studies, a range of initial temperatures, and different\nelemental abundance patterns to quantify the effects of TDP on the observed ion\nfractions. A metal-enriched ($\\mathrm{[\\alpha/H]}=0.6_{-0.1}^{+0.2}$) C IV\nabsorption system at z ~ 1 previously studied using photoionization equilibrium\n(PIE) models is re-examined under the TDP framework. The main findings are as\nfollows: (1) varying prescriptions for the underlying UVB or adopting initial\ntemperatures T_0 < 1e6 K (with the starting ionization state in collisional\nequilibrium) change TDP ion fractions by up to a factor of three and ten\nrespectively, but the adopted relative elemental abundance pattern affects ion\nfractions by at most 40%; (2) the inferred gas densities are consistent between\nPIE and TDP, but under TDP solar metallicity cannot be ruled out at more than\n2-$\\sigma$ significance and a non-solar [C/$\\alpha$]$\\approx 0.25$ is robustly\nconstrained from the observed relative ion abundances. Extending the TDP\nanalyses to a larger sample of super-solar absorption components with high\nsignal-to-noise absorption spectra is needed to quantify the fraction of metal\nabsorbers originating in rapid cooling gas."
                },
                "authors": [
                    {
                        "name": "Suyash Kumar"
                    },
                    {
                        "name": "Hsiao-Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsiao-Wen Chen"
                },
                "author": "Hsiao-Wen Chen",
                "arxiv_doi": "10.33232/001c.142441",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33232/001c.142441",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, accepted to The Open Journal of Astrophysics with\n  revisions",
                "arxiv_journal_ref": "The Open Journal of Astrophysics, Volume 8, 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15698v1",
                "updated": "2025-07-21T15:07:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    59,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:07:59Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    59,
                    0,
                    202,
                    0
                ],
                "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models"
                },
                "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs."
                },
                "authors": [
                    {
                        "name": "Congmin Zheng"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Mengyue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Yang"
                },
                "author": "Mengyue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15697v1",
                "updated": "2025-07-21T15:07:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    51,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:07:51Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    51,
                    0,
                    202,
                    0
                ],
                "title": "Examining the Gap in the Chirp Mass Distribution of Binary Black Holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Gap in the Chirp Mass Distribution of Binary Black Holes"
                },
                "summary": "The mass distribution of binary black holes inferred from gravitational wave\nmeasurements is expected to shed light on their formation scenarios. An\nemerging structure in the mass distribution indicates the presence of multiple\npeaks around chirp masses of $8M_\\odot$, $14M_\\odot$, and $27M_\\odot$. In\nparticular, there is a lack of observations between chirp masses of 10 and 12\n$M_\\odot$. In this letter, we report that observations significantly favour the\nmodel supporting suppression of the rate in a narrow chirp mass range compared\nto the model that doesn't include suppression at a confidence greater than\n99.5\\%. Using another test, which measures the deviation between the inferred\nchirp mass distributions from the two models, we conservatively estimate a 95\\%\nconfidence in the presence of a feature. A lack of confidence has been reported\nin the presence of a gap around a comparable location in the component mass\ndistribution. The differing conclusions are due to a unique correlation between\nthe primary~(heavier of the two masses) and the secondary~(lighter of the two\nmasses) masses of binary black holes. This correlation results in increased\nclustering of measured chirp masses around specific values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass distribution of binary black holes inferred from gravitational wave\nmeasurements is expected to shed light on their formation scenarios. An\nemerging structure in the mass distribution indicates the presence of multiple\npeaks around chirp masses of $8M_\\odot$, $14M_\\odot$, and $27M_\\odot$. In\nparticular, there is a lack of observations between chirp masses of 10 and 12\n$M_\\odot$. In this letter, we report that observations significantly favour the\nmodel supporting suppression of the rate in a narrow chirp mass range compared\nto the model that doesn't include suppression at a confidence greater than\n99.5\\%. Using another test, which measures the deviation between the inferred\nchirp mass distributions from the two models, we conservatively estimate a 95\\%\nconfidence in the presence of a feature. A lack of confidence has been reported\nin the presence of a gap around a comparable location in the component mass\ndistribution. The differing conclusions are due to a unique correlation between\nthe primary~(heavier of the two masses) and the secondary~(lighter of the two\nmasses) masses of binary black holes. This correlation results in increased\nclustering of measured chirp masses around specific values."
                },
                "authors": [
                    {
                        "name": "Vaibhav Tiwari"
                    },
                    {
                        "name": "Alberto Vecchio"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Vecchio"
                },
                "author": "Alberto Vecchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15696v1",
                "updated": "2025-07-21T15:07:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:07:41Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    41,
                    0,
                    202,
                    0
                ],
                "title": "Online survival analysis with quantile regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online survival analysis with quantile regression"
                },
                "summary": "We propose an online inference method for censored quantile regression with\nstreaming data sets. A key strategy is to approximate the martingale-based\nunsmooth objective function with a quadratic loss function involving a\nwell-justified second-order expansion. This enables us to derive a new online\nconvex function based on the current data batch and summary statistics of\nhistorical data, thereby achieving online updating and occupying low storage\nspace. To estimate the regression parameters, we design a novel\nmajorize-minimize algorithm by reasonably constructing a quadratic surrogate\nobjective function, which renders a closed-form parameter update and thus\nreduces the computational burden notably. Theoretically, compared to the oracle\nestimators derived from analyzing the entire raw data once, we posit a weaker\nassumption on the quantile grid size and show that the proposed online\nestimators can maintain the same convergence rate and statistical efficiency.\nSimulation studies and an application demonstrate the satisfactory empirical\nperformance and practical utilities of the proposed online method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an online inference method for censored quantile regression with\nstreaming data sets. A key strategy is to approximate the martingale-based\nunsmooth objective function with a quadratic loss function involving a\nwell-justified second-order expansion. This enables us to derive a new online\nconvex function based on the current data batch and summary statistics of\nhistorical data, thereby achieving online updating and occupying low storage\nspace. To estimate the regression parameters, we design a novel\nmajorize-minimize algorithm by reasonably constructing a quadratic surrogate\nobjective function, which renders a closed-form parameter update and thus\nreduces the computational burden notably. Theoretically, compared to the oracle\nestimators derived from analyzing the entire raw data once, we posit a weaker\nassumption on the quantile grid size and show that the proposed online\nestimators can maintain the same convergence rate and statistical efficiency.\nSimulation studies and an application demonstrate the satisfactory empirical\nperformance and practical utilities of the proposed online method."
                },
                "authors": [
                    {
                        "name": "Yi Deng"
                    },
                    {
                        "name": "Shuwei Li"
                    },
                    {
                        "name": "Liuquan Sun"
                    },
                    {
                        "name": "Baoxue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxue Zhang"
                },
                "author": "Baoxue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00061v2",
                "updated": "2025-07-21T15:04:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    4,
                    28,
                    0,
                    202,
                    0
                ],
                "published": "2024-08-22T14:27:47Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    27,
                    47,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Natural Language Inference Performance with Knowledge Graph\n  for COVID-19 Automated Fact-Checking in Indonesian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Natural Language Inference Performance with Knowledge Graph\n  for COVID-19 Automated Fact-Checking in Indonesian Language"
                },
                "summary": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking."
                },
                "authors": [
                    {
                        "name": "Arief Purnama Muharram"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    }
                ],
                "author_detail": {
                    "name": "Ayu Purwarianti"
                },
                "author": "Ayu Purwarianti",
                "arxiv_comment": "Submitted to the Journal of ICT Research and Applications (JICTRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15686v1",
                "updated": "2025-07-21T14:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    48,
                    54,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:48:54Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    48,
                    54,
                    0,
                    202,
                    0
                ],
                "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud\n  Geometry Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud\n  Geometry Compression"
                },
                "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/."
                },
                "authors": [
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Shuting Xia"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Zhu Li"
                    },
                    {
                        "name": "Yiling Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Xu"
                },
                "author": "Yiling Xu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06565v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06565v4",
                "updated": "2025-07-21T14:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    44,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-09T05:39:56Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    39,
                    56,
                    2,
                    190,
                    0
                ],
                "title": "A Mathematical Theory of Discursive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mathematical Theory of Discursive Networks"
                },
                "summary": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability."
                },
                "authors": [
                    {
                        "name": "Juan B. Gutirrez"
                    }
                ],
                "author_detail": {
                    "name": "Juan B. Gutirrez"
                },
                "author": "Juan B. Gutirrez",
                "arxiv_comment": "39 pages, 4 figures, 4 tables, 3 algorithm, 56 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06565v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06565v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09763v2",
                "updated": "2025-07-21T14:41:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    41,
                    39,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-14T00:06:48Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    6,
                    48,
                    0,
                    104,
                    0
                ],
                "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems"
                },
                "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project Page: https://zaidkhan.me/EFAGen/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15675v1",
                "updated": "2025-07-21T14:37:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    37,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:37:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    37,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "P3: Prompts Promote Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3: Prompts Promote Prompting"
                },
                "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yuanquan Hu"
                    },
                    {
                        "name": "Fangchao Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Accepted to ACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15671v1",
                "updated": "2025-07-21T14:34:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:34:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "BugScope: Learn to Find Bugs Like Human",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugScope: Learn to Find Bugs Like Human"
                },
                "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact."
                },
                "authors": [
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Dominic Deluca"
                    },
                    {
                        "name": "Jinjie Liu"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 2 figure, 6 tables, 4 listings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15664v1",
                "updated": "2025-07-21T14:25:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    25,
                    52,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:25:52Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    25,
                    52,
                    0,
                    202,
                    0
                ],
                "title": "VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability\n  Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability\n  Repair"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential in\ncomputer-aided design (CAD), particularly for automated debugging and\nverification within electronic design automation (EDA) tools. However, Design\nfor Testability (DFT) remains a relatively underexplored area. This paper\npresents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a\nRetrieval-Augmented Generation (RAG) approach to enable LLM to revise code to\nensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity\nmeasurement model for precise retrieval of reference RTL designs for the LLM,\nand (2) an iterative code revision pipeline that allows the LLM to ensure DFT\ncompliance while maintaining synthesizability. To support VeriRAG, we introduce\nVeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG\nretrieves structurally similar RTL designs from VeriDFT, each paired with a\nrigorously validated correction, as references for code repair. With VeriRAG\nand VeriDFT, we achieve fully automated DFT correction -- resulting in a\n7.72-fold improvement in successful repair rate compared to the zero-shot\nbaseline (Fig. 5 in Section V). Ablation studies further confirm the\ncontribution of each component of the VeriRAG framework. We open-source our\ndata, models, and scripts at https://github.com/yuyangdu01/LLM4DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential in\ncomputer-aided design (CAD), particularly for automated debugging and\nverification within electronic design automation (EDA) tools. However, Design\nfor Testability (DFT) remains a relatively underexplored area. This paper\npresents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a\nRetrieval-Augmented Generation (RAG) approach to enable LLM to revise code to\nensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity\nmeasurement model for precise retrieval of reference RTL designs for the LLM,\nand (2) an iterative code revision pipeline that allows the LLM to ensure DFT\ncompliance while maintaining synthesizability. To support VeriRAG, we introduce\nVeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG\nretrieves structurally similar RTL designs from VeriDFT, each paired with a\nrigorously validated correction, as references for code repair. With VeriRAG\nand VeriDFT, we achieve fully automated DFT correction -- resulting in a\n7.72-fold improvement in successful repair rate compared to the zero-shot\nbaseline (Fig. 5 in Section V). Ablation studies further confirm the\ncontribution of each component of the VeriRAG framework. We open-source our\ndata, models, and scripts at https://github.com/yuyangdu01/LLM4DFT."
                },
                "authors": [
                    {
                        "name": "Haomin Qi"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Lihao Zhang"
                    },
                    {
                        "name": "Soung Chang Liew"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Yining Du"
                    }
                ],
                "author_detail": {
                    "name": "Yining Du"
                },
                "author": "Yining Du",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15655v1",
                "updated": "2025-07-21T14:16:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    16,
                    44,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:16:44Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    16,
                    44,
                    0,
                    202,
                    0
                ],
                "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding\n  with a Comprehensive VQA Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding\n  with a Comprehensive VQA Benchmark"
                },
                "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain."
                },
                "authors": [
                    {
                        "name": "Aniket Pal"
                    },
                    {
                        "name": "Ajoy Mondal"
                    },
                    {
                        "name": "Minesh Mathew"
                    },
                    {
                        "name": "C. V. Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C. V. Jawahar"
                },
                "author": "C. V. Jawahar",
                "arxiv_comment": "This is a minor revision of the original paper submitted to IJDAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15651v1",
                "updated": "2025-07-21T14:15:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    15,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:15:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    15,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "Data augmentation enables label-specific generation of homologous\n  protein sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation enables label-specific generation of homologous\n  protein sequences"
                },
                "summary": "Accurately annotating and controlling protein function from sequence data\nremains a major challenge, particularly within homologous families where\nannotated sequences are scarce and structural variation is minimal. We present\na two-stage approach for semi-supervised functional annotation and conditional\nsequence generation in protein families using representation learning. First,\nwe demonstrate that protein language models, pretrained on large and diverse\nsequence datasets and possibly finetuned via contrastive learning, provide\nembeddings that robustly capture fine-grained functional specificities, even\nwith limited labeled data. Second, we use the inferred annotations to train a\ngenerative probabilistic model, an annotation-aware Restricted Boltzmann\nMachine, capable of producing synthetic sequences with prescribed functional\nlabels. Across several protein families, we show that this approach achieves\nhighly accurate annotation quality and supports the generation of functionally\ncoherent sequences. Our findings underscore the power of combining\nself-supervised learning with light supervision to overcome data scarcity in\nprotein function prediction and design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately annotating and controlling protein function from sequence data\nremains a major challenge, particularly within homologous families where\nannotated sequences are scarce and structural variation is minimal. We present\na two-stage approach for semi-supervised functional annotation and conditional\nsequence generation in protein families using representation learning. First,\nwe demonstrate that protein language models, pretrained on large and diverse\nsequence datasets and possibly finetuned via contrastive learning, provide\nembeddings that robustly capture fine-grained functional specificities, even\nwith limited labeled data. Second, we use the inferred annotations to train a\ngenerative probabilistic model, an annotation-aware Restricted Boltzmann\nMachine, capable of producing synthetic sequences with prescribed functional\nlabels. Across several protein families, we show that this approach achieves\nhighly accurate annotation quality and supports the generation of functionally\ncoherent sequences. Our findings underscore the power of combining\nself-supervised learning with light supervision to overcome data scarcity in\nprotein function prediction and design."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rosset"
                    },
                    {
                        "name": "Martin Weigt"
                    },
                    {
                        "name": "Francesco Zamponi"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Zamponi"
                },
                "author": "Francesco Zamponi",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08837v2",
                "updated": "2025-07-21T14:11:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    11,
                    16,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-07T19:18:47Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    18,
                    47,
                    0,
                    188,
                    0
                ],
                "title": "A metal-poor atmosphere with a hot interior for a young sub-Neptune\n  progenitor: JWST/NIRSpec transmission spectrum of V1298 Tau b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A metal-poor atmosphere with a hot interior for a young sub-Neptune\n  progenitor: JWST/NIRSpec transmission spectrum of V1298 Tau b"
                },
                "summary": "We present the JWST/NIRSpec G395H transmission spectrum of the young (10 - 20\nMyr old) transiting planet V1298 Tau b (9.85+/-0.35 Re, Teq=670K). Combined HST\nand JWST observations reveal a haze free, H/He dominated atmosphere with a\nlarge scale height (~1500km), allowing detection of CO2 (35 sigma), H2O (30\nsigma), CO (10 sigma), CH4 (6 sigma), SO2 (4 sigma) and OCS (3.5 sigma). Our\nobservations probe several scale heights (~4.4 in the CO2 4.3 microns and ~3 in\nthe 2.7 micron water band). The planet's mass, inferred from atmospheric scale\nheight using free retrieval and grid modelling is 12+/-1 and 15+/-1.7Me\nrespectively which is significantly lower than previous radial velocity\nestimates and confirm it as a 'gas-dwarf' sub-Neptune progenitor. We find an\natmospheric super-solar metallicity (logZ=0.6^+0.4_-0.6 x solar) and a\nsub-solar C/O ratio (0.22^+0.06_-0.05). The atmospheric metallicity is low\ncompared to matured sub-Neptunes by an order of magnitude. The CH4 abundance\n([CH4]=-6.2^+0.3_-0.5) is ~7 sigma lower than equilibrium chemistry prediction.\nTo adjust for the low methane abundance, the self-consistent grids favour a\nhigh internal temperature (~500K) and vertical mixing (Kzz ~10^7-10^8 cm2/s).\nThese internal temperatures are inconsistent with predictions from evolutionary\nmodels, which expect ~100 - 200K at the current system age. We estimate a\ngas-to-core mass fraction between 0.1 - 8 %, with a core mass of 11 - 12 Me,\nconsistent with in-situ gas dwarf formation. A deep atmospheric metallicity\ngradient may explain both the high internal temperature and low observable\nmetallicity. Over time, mass loss from such an atmosphere could enhance its\nmetallicity, potentially reconciling V1298 Tau b with mature sub-Neptunes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the JWST/NIRSpec G395H transmission spectrum of the young (10 - 20\nMyr old) transiting planet V1298 Tau b (9.85+/-0.35 Re, Teq=670K). Combined HST\nand JWST observations reveal a haze free, H/He dominated atmosphere with a\nlarge scale height (~1500km), allowing detection of CO2 (35 sigma), H2O (30\nsigma), CO (10 sigma), CH4 (6 sigma), SO2 (4 sigma) and OCS (3.5 sigma). Our\nobservations probe several scale heights (~4.4 in the CO2 4.3 microns and ~3 in\nthe 2.7 micron water band). The planet's mass, inferred from atmospheric scale\nheight using free retrieval and grid modelling is 12+/-1 and 15+/-1.7Me\nrespectively which is significantly lower than previous radial velocity\nestimates and confirm it as a 'gas-dwarf' sub-Neptune progenitor. We find an\natmospheric super-solar metallicity (logZ=0.6^+0.4_-0.6 x solar) and a\nsub-solar C/O ratio (0.22^+0.06_-0.05). The atmospheric metallicity is low\ncompared to matured sub-Neptunes by an order of magnitude. The CH4 abundance\n([CH4]=-6.2^+0.3_-0.5) is ~7 sigma lower than equilibrium chemistry prediction.\nTo adjust for the low methane abundance, the self-consistent grids favour a\nhigh internal temperature (~500K) and vertical mixing (Kzz ~10^7-10^8 cm2/s).\nThese internal temperatures are inconsistent with predictions from evolutionary\nmodels, which expect ~100 - 200K at the current system age. We estimate a\ngas-to-core mass fraction between 0.1 - 8 %, with a core mass of 11 - 12 Me,\nconsistent with in-situ gas dwarf formation. A deep atmospheric metallicity\ngradient may explain both the high internal temperature and low observable\nmetallicity. Over time, mass loss from such an atmosphere could enhance its\nmetallicity, potentially reconciling V1298 Tau b with mature sub-Neptunes."
                },
                "authors": [
                    {
                        "name": "Saugata Barat"
                    },
                    {
                        "name": "Jean-Michel Dsert"
                    },
                    {
                        "name": "Sagnick Mukherjee"
                    },
                    {
                        "name": "Jayesh M. Goyal"
                    },
                    {
                        "name": "Qiao Xue"
                    },
                    {
                        "name": "Yui Kawashima"
                    },
                    {
                        "name": "Allona Vazan"
                    },
                    {
                        "name": "William Misener"
                    },
                    {
                        "name": "Hilke E. Schlichting"
                    },
                    {
                        "name": "Jonathan J. Fortney"
                    },
                    {
                        "name": "Jacob L. Bean"
                    },
                    {
                        "name": "Swaroop Avarsekar"
                    },
                    {
                        "name": "Gregory W. Henry"
                    },
                    {
                        "name": "Robin Baeyens"
                    },
                    {
                        "name": "Michael R. Line"
                    },
                    {
                        "name": "John H. Livingston"
                    },
                    {
                        "name": "Trevor David"
                    },
                    {
                        "name": "Erik A. Petigura"
                    },
                    {
                        "name": "James T. Sikora"
                    },
                    {
                        "name": "Hinna Shivkumar"
                    },
                    {
                        "name": "Adina D. Feinstein"
                    },
                    {
                        "name": "Antonija Okloi"
                    }
                ],
                "author_detail": {
                    "name": "Antonija Okloi"
                },
                "author": "Antonija Okloi",
                "arxiv_comment": "Accepted for publication in AJ, minor typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13545v2",
                "updated": "2025-07-21T14:09:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    9,
                    54,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-19T03:17:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    3,
                    17,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Know Or Not: a library for evaluating out-of-knowledge base robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Or Not: a library for evaluating out-of-knowledge base robustness"
                },
                "summary": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot."
                },
                "authors": [
                    {
                        "name": "Jessica Foo"
                    },
                    {
                        "name": "Pradyumna Shyama Prasad"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07057v2",
                "updated": "2025-07-21T14:05:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    5,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-10T21:47:49Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    47,
                    49,
                    0,
                    41,
                    0
                ],
                "title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark"
                },
                "summary": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices."
                },
                "authors": [
                    {
                        "name": "M. Ali Bayram"
                    },
                    {
                        "name": "Ali Arda Fincan"
                    },
                    {
                        "name": "Ahmet Semih Gm"
                    },
                    {
                        "name": "Sercan Karaka"
                    },
                    {
                        "name": "Banu Diri"
                    },
                    {
                        "name": "Sava Yldrm"
                    }
                ],
                "author_detail": {
                    "name": "Sava Yldrm"
                },
                "author": "Sava Yldrm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14000v2",
                "updated": "2025-07-21T14:03:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    3,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-18T15:14:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "Photonic Fabric Platform for AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic Fabric Platform for AI Accelerators"
                },
                "summary": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute."
                },
                "authors": [
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "arxiv_comment": "12 pages, 14 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08321v2",
                "updated": "2025-07-21T14:00:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    0,
                    32,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-14T18:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "Computation of FCC-ee Sensitivity to Heavy New Physics with Interactions\n  of Any Flavor Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation of FCC-ee Sensitivity to Heavy New Physics with Interactions\n  of Any Flavor Structure"
                },
                "summary": "We present a tool to compute the sensitivity of the Future Circular\nElectron--Positron Collider (FCC-ee) to the interactions of new, heavy\nparticles via publicly available extensions to the smelli and flavio computer\nprograms. We parameterize new particles' effects without any flavor assumptions\nand take into account the projected experimental and correlated theoretical\nuncertainties of various electroweak and Higgs observables at the proposed\ncollider. We illustrate a use of the tool by estimating the sensitivity of the\nFCC-ee to a $Z^\\prime$ model with flavor-specific couplings which explains\nanomalies inferred from present-day measurements and Standard Model predictions\nof observables that involve the $b \\rightarrow s \\ell^+ \\ell^-$ transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a tool to compute the sensitivity of the Future Circular\nElectron--Positron Collider (FCC-ee) to the interactions of new, heavy\nparticles via publicly available extensions to the smelli and flavio computer\nprograms. We parameterize new particles' effects without any flavor assumptions\nand take into account the projected experimental and correlated theoretical\nuncertainties of various electroweak and Higgs observables at the proposed\ncollider. We illustrate a use of the tool by estimating the sensitivity of the\nFCC-ee to a $Z^\\prime$ model with flavor-specific couplings which explains\nanomalies inferred from present-day measurements and Standard Model predictions\nof observables that involve the $b \\rightarrow s \\ell^+ \\ell^-$ transition."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Eetu Loisa"
                    }
                ],
                "author_detail": {
                    "name": "Eetu Loisa"
                },
                "author": "Eetu Loisa",
                "arxiv_comment": "7 pages, 1 figure. To access the new tools, see\n  https://github.com/eetuloisa/smelli_fcc and\n  https://github.com/eetuloisa/flavio_fcc v2 has added observables and a\n  considerable rewrite of the text in order to change the focus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15630v1",
                "updated": "2025-07-21T13:53:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    53,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:53:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    53,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "Testing Homogeneity in a heteroscedastic contaminated normal mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Homogeneity in a heteroscedastic contaminated normal mixture"
                },
                "summary": "Large-scale simultaneous hypothesis testing appears in many areas such as\nmicroarray studies, genome-wide association studies, brain imaging, disease\nmapping and astronomical surveys. A well-known inference method is to control\nthe false discovery rate. One popular approach is to model the $z$-scores\nderived from the individual $t$-tests and then use this model to control the\nfalse discovery rate. We propose a new class of contaminated normal mixtures\nfor modelling $z$-scores. We further design an EM-test for testing homogeneity\nin this class of mixture models. We show that the EM-test statistic has a\nshifted mixture of chi-squared limiting distribution. Simulation results show\nthat the proposed testing procedure has accurate type I error and significantly\nlarger power than its competitors under a variety of model specifications. A\nreal-data example is analyzed to exemplify the application of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale simultaneous hypothesis testing appears in many areas such as\nmicroarray studies, genome-wide association studies, brain imaging, disease\nmapping and astronomical surveys. A well-known inference method is to control\nthe false discovery rate. One popular approach is to model the $z$-scores\nderived from the individual $t$-tests and then use this model to control the\nfalse discovery rate. We propose a new class of contaminated normal mixtures\nfor modelling $z$-scores. We further design an EM-test for testing homogeneity\nin this class of mixture models. We show that the EM-test statistic has a\nshifted mixture of chi-squared limiting distribution. Simulation results show\nthat the proposed testing procedure has accurate type I error and significantly\nlarger power than its competitors under a variety of model specifications. A\nreal-data example is analyzed to exemplify the application of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Xiaoqing Niu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yuejiao Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuejiao Fu"
                },
                "author": "Yuejiao Fu",
                "arxiv_doi": "10.1080/02664763.2018.1552668",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/02664763.2018.1552668",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.15630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Applied Statistics, 46, 1478-1491 (2019)",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15620v1",
                "updated": "2025-07-21T13:44:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    44,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:44:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    44,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "TrajLens: Visual Analysis for Constructing Cell Developmental\n  Trajectories in Cross-Sample Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajLens: Visual Analysis for Constructing Cell Developmental\n  Trajectories in Cross-Sample Exploration"
                },
                "summary": "Constructing cell developmental trajectories is a critical task in\nsingle-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of\npotential cellular progression paths. However, current automated methods are\nlimited to establishing cell developmental trajectories within individual\nsamples, necessitating biologists to manually link cells across samples to\nconstruct complete cross-sample evolutionary trajectories that consider\ncellular spatial dynamics. This process demands substantial human effort due to\nthe complex spatial correspondence between each pair of samples. To address\nthis challenge, we first proposed a GNN-based model to predict cross-sample\ncell developmental trajectories. We then developed TrajLens, a visual analytics\nsystem that supports biologists in exploring and refining the cell\ndevelopmental trajectories based on predicted links. Specifically, we designed\nthe visualization that integrates features on cell distribution and\ndevelopmental direction across multiple samples, providing an overview of the\nspatial evolutionary patterns of cell populations along trajectories.\nAdditionally, we included contour maps superimposed on the original cell\ndistribution data, enabling biologists to explore them intuitively. To\ndemonstrate our system's performance, we conducted quantitative evaluations of\nour model with two case studies and expert interviews to validate its\nusefulness and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing cell developmental trajectories is a critical task in\nsingle-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of\npotential cellular progression paths. However, current automated methods are\nlimited to establishing cell developmental trajectories within individual\nsamples, necessitating biologists to manually link cells across samples to\nconstruct complete cross-sample evolutionary trajectories that consider\ncellular spatial dynamics. This process demands substantial human effort due to\nthe complex spatial correspondence between each pair of samples. To address\nthis challenge, we first proposed a GNN-based model to predict cross-sample\ncell developmental trajectories. We then developed TrajLens, a visual analytics\nsystem that supports biologists in exploring and refining the cell\ndevelopmental trajectories based on predicted links. Specifically, we designed\nthe visualization that integrates features on cell distribution and\ndevelopmental direction across multiple samples, providing an overview of the\nspatial evolutionary patterns of cell populations along trajectories.\nAdditionally, we included contour maps superimposed on the original cell\ndistribution data, enabling biologists to explore them intuitively. To\ndemonstrate our system's performance, we conducted quantitative evaluations of\nour model with two case studies and expert interviews to validate its\nusefulness and effectiveness."
                },
                "authors": [
                    {
                        "name": "Qipeng Wang"
                    },
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Min Zhu"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15616v1",
                "updated": "2025-07-21T13:41:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    41,
                    7,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:41:07Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    41,
                    7,
                    0,
                    202,
                    0
                ],
                "title": "On zeros and algorithms for disordered systems: mean-field spin glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On zeros and algorithms for disordered systems: mean-field spin glasses"
                },
                "summary": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for nearly all inverse\ntemperatures in the second moment regime. In particular, for the\nSherrington--Kirkpatrick model, our algorithms succeed for almost the entire\nreplica-symmetric phase. To achieve this, we study the locations of the zeros\nof the partition function. Notably, our methods are conceptually simple, and\napply equally well to the spherical case and the case of Ising spins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for nearly all inverse\ntemperatures in the second moment regime. In particular, for the\nSherrington--Kirkpatrick model, our algorithms succeed for almost the entire\nreplica-symmetric phase. To achieve this, we study the locations of the zeros\nof the partition function. Notably, our methods are conceptually simple, and\napply equally well to the spherical case and the case of Ising spins."
                },
                "authors": [
                    {
                        "name": "Ferenc Bencs"
                    },
                    {
                        "name": "Kuikui Liu"
                    },
                    {
                        "name": "Guus Regts"
                    }
                ],
                "author_detail": {
                    "name": "Guus Regts"
                },
                "author": "Guus Regts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15615v1",
                "updated": "2025-07-21T13:40:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    40,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:40:19Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    40,
                    19,
                    0,
                    202,
                    0
                ],
                "title": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP\n  Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP\n  Solving"
                },
                "summary": "Primal heuristics play a critical role in improving the efficiency of mixed\ninteger programming (MILP) solvers. As large language models (LLMs) have\ndemonstrated superior code generation abilities, recent MILP works are devoted\nto leveraging the evolutionary computation approaches with LLMs to generate\neffective primal heuristics. Although the generated heuristics have achieved\nbetter solving performance than the hand-crafted ones with little adaptability,\nthe advantage of current LLM-based methods is limited to few MILP instances in\none problem class, as they fail to capture the instance characteristics in the\nproblem class (the MILP instances generated from the same mathematical model\nare defined as a problem class). Since MILP instances often differ\nsignificantly in structure and feature distribution, the neglect of their\ncharacteristics in the evolution process results in poor generalization within\nthe same problem class. To overcome this challenge, we propose a data-algorithm\nco-evolution framework (DHEvo) that iteratively selects representative\ninstances and evolves corresponding heuristics. With the initial instance\ndistribution, we develop an LLM-based multi-agent system to generate data-code\npairs simultaneously. These data-code pairs are iteratively refined based on\ntheir fitness scores, leading to the identification of the most effective\nheuristic over the entire problem class. Extensive experiments across diverse\nMILP benchmarks demonstrate that our approach significantly outperforms both\nhuman-designed heuristics and existing LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primal heuristics play a critical role in improving the efficiency of mixed\ninteger programming (MILP) solvers. As large language models (LLMs) have\ndemonstrated superior code generation abilities, recent MILP works are devoted\nto leveraging the evolutionary computation approaches with LLMs to generate\neffective primal heuristics. Although the generated heuristics have achieved\nbetter solving performance than the hand-crafted ones with little adaptability,\nthe advantage of current LLM-based methods is limited to few MILP instances in\none problem class, as they fail to capture the instance characteristics in the\nproblem class (the MILP instances generated from the same mathematical model\nare defined as a problem class). Since MILP instances often differ\nsignificantly in structure and feature distribution, the neglect of their\ncharacteristics in the evolution process results in poor generalization within\nthe same problem class. To overcome this challenge, we propose a data-algorithm\nco-evolution framework (DHEvo) that iteratively selects representative\ninstances and evolves corresponding heuristics. With the initial instance\ndistribution, we develop an LLM-based multi-agent system to generate data-code\npairs simultaneously. These data-code pairs are iteratively refined based on\ntheir fitness scores, leading to the identification of the most effective\nheuristic over the entire problem class. Extensive experiments across diverse\nMILP benchmarks demonstrate that our approach significantly outperforms both\nhuman-designed heuristics and existing LLM-based methods."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Feifan Liu"
                    },
                    {
                        "name": "Mengjing Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15613v1",
                "updated": "2025-07-21T13:38:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    38,
                    12,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:38:12Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    38,
                    12,
                    0,
                    202,
                    0
                ],
                "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems"
                },
                "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses."
                },
                "authors": [
                    {
                        "name": "Andrii Balashov"
                    },
                    {
                        "name": "Olena Ponomarova"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15612v1",
                "updated": "2025-07-21T13:35:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    35,
                    0,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:35:00Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    35,
                    0,
                    0,
                    202,
                    0
                ],
                "title": "Inference on Nonlinear Counterfactual Functionals under a Multiplicative\n  IV Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Nonlinear Counterfactual Functionals under a Multiplicative\n  IV Model"
                },
                "summary": "Instrumental variable (IV) methods play a central role in causal inference,\nparticularly in settings where treatment assignment is confounded by unobserved\nvariables. IV methods have been extensively developed in recent years and\napplied across diverse domains, from economics to epidemiology. In this work,\nwe study the recently introduced multiplicative IV (MIV) model and demonstrate\nits utility for causal inference beyond the average treatment effect. In\nparticular, we show that it enables identification and inference for a broad\nclass of counterfactual functionals characterized by moment equations. This\nincludes, for example, inference on quantile treatment effects. We develop\nmethods for efficient and multiply robust estimation of such functionals, and\nprovide inference procedures with asymptotic validity. Experimental results\ndemonstrate that the proposed procedure performs well even with moderate sample\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental variable (IV) methods play a central role in causal inference,\nparticularly in settings where treatment assignment is confounded by unobserved\nvariables. IV methods have been extensively developed in recent years and\napplied across diverse domains, from economics to epidemiology. In this work,\nwe study the recently introduced multiplicative IV (MIV) model and demonstrate\nits utility for causal inference beyond the average treatment effect. In\nparticular, we show that it enables identification and inference for a broad\nclass of counterfactual functionals characterized by moment equations. This\nincludes, for example, inference on quantile treatment effects. We develop\nmethods for efficient and multiply robust estimation of such functionals, and\nprovide inference procedures with asymptotic validity. Experimental results\ndemonstrate that the proposed procedure performs well even with moderate sample\nsizes."
                },
                "authors": [
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Mengxin Yu"
                    },
                    {
                        "name": "Jiewen Liu"
                    },
                    {
                        "name": "Chan Park"
                    },
                    {
                        "name": "Yunshu Zhang"
                    },
                    {
                        "name": "James M. Robins"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v2",
                "updated": "2025-07-21T13:34:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    34,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15599v1",
                "updated": "2025-07-21T13:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    21,
                    29,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    21,
                    29,
                    0,
                    202,
                    0
                ],
                "title": "Applying the Chinese Wall Reverse Engineering Technique to Large\n  Language Model Code Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying the Chinese Wall Reverse Engineering Technique to Large\n  Language Model Code Editing"
                },
                "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions."
                },
                "authors": [
                    {
                        "name": "Manatsawin Hanmongkolchai"
                    }
                ],
                "author_detail": {
                    "name": "Manatsawin Hanmongkolchai"
                },
                "author": "Manatsawin Hanmongkolchai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15595v1",
                "updated": "2025-07-21T13:18:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    18,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:18:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    18,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging"
                },
                "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}."
                },
                "authors": [
                    {
                        "name": "Salah Eddine Bekhouche"
                    },
                    {
                        "name": "Gaby Maroun"
                    },
                    {
                        "name": "Fadi Dornaika"
                    },
                    {
                        "name": "Abdenour Hadid"
                    }
                ],
                "author_detail": {
                    "name": "Abdenour Hadid"
                },
                "author": "Abdenour Hadid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15586v1",
                "updated": "2025-07-21T13:03:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."
                },
                "authors": [
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Shouzheng Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "16 pages, 7 Figures, 10 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15585v1",
                "updated": "2025-07-21T13:03:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:03:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Unequal Voices: How LLMs Construct Constrained Queer Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unequal Voices: How LLMs Construct Constrained Queer Narratives"
                },
                "summary": "One way social groups are marginalized in discourse is that the narratives\ntold about them often default to a narrow, stereotyped range of topics. In\ncontrast, default groups are allowed the full complexity of human existence. We\ndescribe the constrained representations of queer people in LLM generations in\nterms of harmful representations, narrow representations, and discursive\nothering and formulate hypotheses to test for these phenomena. Our results show\nthat LLMs are significantly limited in their portrayals of queer personas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way social groups are marginalized in discourse is that the narratives\ntold about them often default to a narrow, stereotyped range of topics. In\ncontrast, default groups are allowed the full complexity of human existence. We\ndescribe the constrained representations of queer people in LLM generations in\nterms of harmful representations, narrow representations, and discursive\nothering and formulate hypotheses to test for these phenomena. Our results show\nthat LLMs are significantly limited in their portrayals of queer personas."
                },
                "authors": [
                    {
                        "name": "Atreya Ghosal"
                    },
                    {
                        "name": "Ashim Gupta"
                    },
                    {
                        "name": "Vivek Srikumar"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Srikumar"
                },
                "author": "Vivek Srikumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15581v1",
                "updated": "2025-07-21T13:01:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    1,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:01:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    1,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "Metric assessment protocol in the context of answer fluctuation on MCQ\n  tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metric assessment protocol in the context of answer fluctuation on MCQ\n  tasks"
                },
                "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol."
                },
                "authors": [
                    {
                        "name": "Ekaterina Goliakova"
                    },
                    {
                        "name": "Xavier Renard"
                    },
                    {
                        "name": "Marie-Jeanne Lesot"
                    },
                    {
                        "name": "Thibault Laugel"
                    },
                    {
                        "name": "Christophe Marsala"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17181v2",
                "updated": "2025-07-21T12:58:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    58,
                    26,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-21T14:29:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "A Study of LLMs' Preferences for Libraries and Programming Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of LLMs' Preferences for Libraries and Programming Languages"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to generate code,\ninfluencing users' choices of libraries and programming languages in critical\nreal-world projects. However, little is known about their systematic biases or\npreferences toward certain libraries and programming languages, which can\nsignificantly impact software development practices. To fill this gap, we\nperform the first empirical study of LLMs' preferences for libraries and\nprogramming languages when generating code, covering eight diverse LLMs. Our\nresults reveal that LLMs exhibit a strong tendency to overuse widely adopted\nlibraries such as NumPy; in up to 48% of cases, this usage is unnecessary and\ndeviates from the ground-truth solutions. LLMs also exhibit a significant\npreference toward Python as their default language. For high-performance\nproject initialisation tasks where Python is not the optimal language, it\nremains the dominant choice in 58% of cases, and Rust is not used a single\ntime. These results indicate that LLMs may prioritise familiarity and\npopularity over suitability and task-specific optimality. This will introduce\nsecurity vulnerabilities and technical debt, and limit exposure to newly\ndeveloped, better-suited tools and languages. Understanding and addressing\nthese biases is essential for the responsible integration of LLMs into software\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to generate code,\ninfluencing users' choices of libraries and programming languages in critical\nreal-world projects. However, little is known about their systematic biases or\npreferences toward certain libraries and programming languages, which can\nsignificantly impact software development practices. To fill this gap, we\nperform the first empirical study of LLMs' preferences for libraries and\nprogramming languages when generating code, covering eight diverse LLMs. Our\nresults reveal that LLMs exhibit a strong tendency to overuse widely adopted\nlibraries such as NumPy; in up to 48% of cases, this usage is unnecessary and\ndeviates from the ground-truth solutions. LLMs also exhibit a significant\npreference toward Python as their default language. For high-performance\nproject initialisation tasks where Python is not the optimal language, it\nremains the dominant choice in 58% of cases, and Rust is not used a single\ntime. These results indicate that LLMs may prioritise familiarity and\npopularity over suitability and task-specific optimality. This will introduce\nsecurity vulnerabilities and technical debt, and limit exposure to newly\ndeveloped, better-suited tools and languages. Understanding and addressing\nthese biases is essential for the responsible integration of LLMs into software\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Lukas Twist"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Don Syme"
                    },
                    {
                        "name": "Joost Noppen"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Detlef Nauck"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Nauck"
                },
                "author": "Detlef Nauck",
                "arxiv_comment": "13 pages, 8 tables, 2 figures. Paper was previously titled \"LLMs Love\n  Python\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12510v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12510v4",
                "updated": "2025-07-21T12:53:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    53,
                    26,
                    0,
                    202,
                    0
                ],
                "published": "2024-03-19T07:24:54Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    7,
                    24,
                    54,
                    1,
                    79,
                    0
                ],
                "title": "Generalized Consistency Trajectory Models for Image Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Consistency Trajectory Models for Image Manipulation"
                },
                "summary": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing."
                },
                "authors": [
                    {
                        "name": "Beomsu Kim"
                    },
                    {
                        "name": "Jaemin Kim"
                    },
                    {
                        "name": "Jeongsol Kim"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "ICLR 2025 (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12510v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12510v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00091v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00091v4",
                "updated": "2025-07-21T12:45:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    45,
                    28,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-30T18:02:45Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    2,
                    45,
                    2,
                    120,
                    0
                ],
                "title": "CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios"
                },
                "summary": "With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing methods, this paper\nproposes CoordField, a coordination field agent system for coordinating\nheterogeneous drone swarms in complex urban scenarios. In this system, large\nlanguage models (LLMs) is responsible for interpreting high-level human\ninstructions and converting them into executable commands for the UAV swarms,\nsuch as patrol and target tracking. Subsequently, a Coordination field\nmechanism is proposed to guide UAV motion and task selection, enabling\ndecentralized and adaptive allocation of emergent tasks. A total of 50 rounds\nof comparative testing were conducted across different models in a 2D\nsimulation space to evaluate their performance. Experimental results\ndemonstrate that the proposed system achieves superior performance in terms of\ntask coverage, response time, and adaptability to dynamic changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing methods, this paper\nproposes CoordField, a coordination field agent system for coordinating\nheterogeneous drone swarms in complex urban scenarios. In this system, large\nlanguage models (LLMs) is responsible for interpreting high-level human\ninstructions and converting them into executable commands for the UAV swarms,\nsuch as patrol and target tracking. Subsequently, a Coordination field\nmechanism is proposed to guide UAV motion and task selection, enabling\ndecentralized and adaptive allocation of emergent tasks. A total of 50 rounds\nof comparative testing were conducted across different models in a 2D\nsimulation space to evaluate their performance. Experimental results\ndemonstrate that the proposed system achieves superior performance in terms of\ntask coverage, response time, and adaptability to dynamic changes."
                },
                "authors": [
                    {
                        "name": "Tengchao Zhang"
                    },
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Patrik P. Sli"
                    },
                    {
                        "name": "Qinghua Ni"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00091v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00091v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19099v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19099v5",
                "updated": "2025-07-21T12:44:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    44,
                    10,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-25T11:28:34Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    28,
                    34,
                    6,
                    145,
                    0
                ],
                "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning"
                },
                "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."
                },
                "authors": [
                    {
                        "name": "Kun Xiang"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zirong Liu"
                    },
                    {
                        "name": "Peixin Qu"
                    },
                    {
                        "name": "Jixi He"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19099v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19099v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05445v2",
                "updated": "2025-07-21T12:42:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    42,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-08T17:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations"
                },
                "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15557v1",
                "updated": "2025-07-21T12:38:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    38,
                    7,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:38:07Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    38,
                    7,
                    0,
                    202,
                    0
                ],
                "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text\n  Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text\n  Detoxification"
                },
                "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case."
                },
                "authors": [
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15268v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15268v8",
                "updated": "2025-07-21T12:37:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    37,
                    32,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-21T17:52:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "title": "Correlation and Beyond: Positive Definite Dependence Measures for Robust\n  Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlation and Beyond: Positive Definite Dependence Measures for Robust\n  Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios"
                },
                "summary": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC\nremains valid under marginal asset distributions with notably different and\nvarying degrees of serial correlation, non-stationarity, heavy-tailedness, and\nasymmetry. NAbC provides p-values and confidence intervals at both the matrix\nlevel and the pairwise cell level, for both one and two-sample tests, with\nanalytical consistency across levels. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen, thus enabling flexible, granular,\nand realistic scenarios and stress tests. NAbC stands alone in providing all of\nthese capabilities simultaneously, and should prove to be a very useful means\nby which we can better understand and manage financial portfolios in our\nmultivariate world",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC\nremains valid under marginal asset distributions with notably different and\nvarying degrees of serial correlation, non-stationarity, heavy-tailedness, and\nasymmetry. NAbC provides p-values and confidence intervals at both the matrix\nlevel and the pairwise cell level, for both one and two-sample tests, with\nanalytical consistency across levels. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen, thus enabling flexible, granular,\nand realistic scenarios and stress tests. NAbC stands alone in providing all of\nthese capabilities simultaneously, and should prove to be a very useful means\nby which we can better understand and manage financial portfolios in our\nmultivariate world"
                },
                "authors": [
                    {
                        "name": "JD Opdyke"
                    }
                ],
                "author_detail": {
                    "name": "JD Opdyke"
                },
                "author": "JD Opdyke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15268v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15268v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15553v1",
                "updated": "2025-07-21T12:32:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    32,
                    30,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:32:30Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    32,
                    30,
                    0,
                    202,
                    0
                ],
                "title": "Efficient Routing of Inference Requests across LLM Instances in\n  Cloud-Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Routing of Inference Requests across LLM Instances in\n  Cloud-Edge Computing"
                },
                "summary": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments."
                },
                "authors": [
                    {
                        "name": "Shibo Yu"
                    },
                    {
                        "name": "Mohammad Goudarzi"
                    },
                    {
                        "name": "Adel Nadjaran Toosi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Nadjaran Toosi"
                },
                "author": "Adel Nadjaran Toosi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04510v2",
                "updated": "2025-07-21T12:31:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    31,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-08T13:56:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs.\nHowever, existing fine-tuning techniques often treat source code as plain text,\nlosing the graph-based structural information inherent in code.\n  Graph-enhanced soft prompt tuning addresses this by translating the\nstructural information into contextual cues that the LLM can understand.\nHowever, current methods are primarily designed for general graph-related tasks\nand focus more on adjacency information, they fall short in preserving the rich\nsemantic information (e.g., control/data flow) within code graphs. They also\nfail to ensure computational efficiency while capturing graph-text interactions\nin their cross-modal alignment module.\n  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection. CGP-Tuning introduces\ntype-aware embeddings to capture the rich semantic information within code\ngraphs, along with an efficient cross-modal alignment module that achieves\nlinear computational costs while incorporating graph-text interactions. It is\nevaluated on the latest DiverseVul dataset and three advanced open-source code\nLLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that\nCGP-Tuning delivers model-agnostic improvements and maintains practical\ninference speed, surpassing the best graph-enhanced soft prompt tuning baseline\nby an average of four percentage points and outperforming non-tuned zero-shot\nprompting by 15 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs.\nHowever, existing fine-tuning techniques often treat source code as plain text,\nlosing the graph-based structural information inherent in code.\n  Graph-enhanced soft prompt tuning addresses this by translating the\nstructural information into contextual cues that the LLM can understand.\nHowever, current methods are primarily designed for general graph-related tasks\nand focus more on adjacency information, they fall short in preserving the rich\nsemantic information (e.g., control/data flow) within code graphs. They also\nfail to ensure computational efficiency while capturing graph-text interactions\nin their cross-modal alignment module.\n  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection. CGP-Tuning introduces\ntype-aware embeddings to capture the rich semantic information within code\ngraphs, along with an efficient cross-modal alignment module that achieves\nlinear computational costs while incorporating graph-text interactions. It is\nevaluated on the latest DiverseVul dataset and three advanced open-source code\nLLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that\nCGP-Tuning delivers model-agnostic improvements and maintains practical\ninference speed, surpassing the best graph-enhanced soft prompt tuning baseline\nby an average of four percentage points and outperforming non-tuned zero-shot\nprompting by 15 percentage points."
                },
                "authors": [
                    {
                        "name": "Ruijun Feng"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Pietro Liguori"
                    },
                    {
                        "name": "Yulei Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yulei Sui"
                },
                "author": "Yulei Sui",
                "arxiv_comment": "Accepted by IEEE Transactions on Software Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15551v1",
                "updated": "2025-07-21T12:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:28:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders"
                },
                "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Zhifang Fan"
                    },
                    {
                        "name": "Xiaoxie Zhu"
                    },
                    {
                        "name": "Yuchen Jiang"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Haoran Ding"
                    },
                    {
                        "name": "Xinmin Wang"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Zhen Gong"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Qiwei Chen"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Zuotao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuotao Liu"
                },
                "author": "Zuotao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15550v1",
                "updated": "2025-07-21T12:28:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    10,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:28:10Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    10,
                    0,
                    202,
                    0
                ],
                "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors"
                },
                "summary": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity."
                },
                "authors": [
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Piotr Pikos"
                    },
                    {
                        "name": "Mateusz Ostaszewski"
                    },
                    {
                        "name": "Firas Laakom"
                    },
                    {
                        "name": "Jrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Schmidhuber"
                },
                "author": "Jrgen Schmidhuber",
                "arxiv_comment": "31 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00409v3",
                "updated": "2025-07-21T12:20:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    20,
                    6,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-01T12:08:38Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    12,
                    8,
                    38,
                    5,
                    32,
                    0
                ],
                "title": "Doing More with Less: A Survey on Routing Strategies for Resource\n  Optimisation in Large Language Model-Based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doing More with Less: A Survey on Routing Strategies for Resource\n  Optimisation in Large Language Model-Based Systems"
                },
                "summary": "Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems."
                },
                "authors": [
                    {
                        "name": "Clovis Varangot-Reille"
                    },
                    {
                        "name": "Christophe Bouvard"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Franois Jacquenet"
                    }
                ],
                "author_detail": {
                    "name": "Franois Jacquenet"
                },
                "author": "Franois Jacquenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12829v2",
                "updated": "2025-07-21T12:15:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-18T12:48:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    48,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan"
                },
                "summary": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Akhmed Sakip"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Bekassyl Syzdykov"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15542v1",
                "updated": "2025-07-21T12:15:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:15:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature\n  Adaptation"
                },
                "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02483v2",
                "updated": "2025-07-21T12:11:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    11,
                    50,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-03T11:07:25Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    7,
                    25,
                    3,
                    93,
                    0
                ],
                "title": "Robust direction-dependent gain-calibration of beam-modelling errors far\n  from the target field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust direction-dependent gain-calibration of beam-modelling errors far\n  from the target field"
                },
                "summary": "Many astronomical questions require deep, wide-field observations at low\nradio frequencies. Phased arrays like LOFAR and SKA-low are designed for this,\nbut have inherently unstable element gains, leading to time, frequency and\ndirection-dependent gain errors. Precise direction-dependent calibration of\nobservations is therefore key to reaching the highest possible dynamic range.\nMany tools for direction-dependent calibration utilise sky and beam models to\ninfer gains. However, these calibration tools struggle with precision\ncalibration for relatively bright (e.g. A-team) sources far from the beam\ncentre. Therefore, the point-spread-function of these sources can potentially\nobscure a faint signal of interest. We show that, and why, the assumption of a\nsmooth gain solution per station fails for realistic radio interferometers, and\nhow this affects gain-calibration results. Subsequently, we introduce an\nimprovement for smooth spectral gain constraints for direction-dependent\ngain-calibration algorithms, in which the level of regularisation is weighted\nby the expected station response to the sky model. We test this method using\ndirection-dependent calibration method DDECal and physically-motivated beam\nmodelling errors for LOFAR-HBA stations. The new method outperforms the\nstandard method for various calibration settings near nulls in the beam, and\nmatches the standard inverse-variance-weighted method's performance for the\nremainder of the data. The proposed method is especially effective for short\nbaselines, both in visibility and image space. Improved direction-dependent\ngain-calibration is critical for future high-precision SKA-low observations,\nwhere higher sensitivity, increased antenna beam complexity, and mutual\ncoupling call for better off-axis source subtraction, which may not be achieved\nthrough improved beam models alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many astronomical questions require deep, wide-field observations at low\nradio frequencies. Phased arrays like LOFAR and SKA-low are designed for this,\nbut have inherently unstable element gains, leading to time, frequency and\ndirection-dependent gain errors. Precise direction-dependent calibration of\nobservations is therefore key to reaching the highest possible dynamic range.\nMany tools for direction-dependent calibration utilise sky and beam models to\ninfer gains. However, these calibration tools struggle with precision\ncalibration for relatively bright (e.g. A-team) sources far from the beam\ncentre. Therefore, the point-spread-function of these sources can potentially\nobscure a faint signal of interest. We show that, and why, the assumption of a\nsmooth gain solution per station fails for realistic radio interferometers, and\nhow this affects gain-calibration results. Subsequently, we introduce an\nimprovement for smooth spectral gain constraints for direction-dependent\ngain-calibration algorithms, in which the level of regularisation is weighted\nby the expected station response to the sky model. We test this method using\ndirection-dependent calibration method DDECal and physically-motivated beam\nmodelling errors for LOFAR-HBA stations. The new method outperforms the\nstandard method for various calibration settings near nulls in the beam, and\nmatches the standard inverse-variance-weighted method's performance for the\nremainder of the data. The proposed method is especially effective for short\nbaselines, both in visibility and image space. Improved direction-dependent\ngain-calibration is critical for future high-precision SKA-low observations,\nwhere higher sensitivity, increased antenna beam complexity, and mutual\ncoupling call for better off-axis source subtraction, which may not be achieved\nthrough improved beam models alone."
                },
                "authors": [
                    {
                        "name": "S. A. Brackenhoff"
                    },
                    {
                        "name": "A. R. Offringa"
                    },
                    {
                        "name": "M. Mevius"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "J. K. Chege"
                    },
                    {
                        "name": "E. Ceccotti"
                    },
                    {
                        "name": "C. Hfer"
                    },
                    {
                        "name": "L. Gao"
                    },
                    {
                        "name": "S. Ghosh"
                    },
                    {
                        "name": "F. G. Mertens"
                    },
                    {
                        "name": "S. Munshi"
                    }
                ],
                "author_detail": {
                    "name": "S. Munshi"
                },
                "author": "S. Munshi",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15530v1",
                "updated": "2025-07-21T11:56:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    56,
                    11,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:56:11Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    56,
                    11,
                    0,
                    202,
                    0
                ],
                "title": "Bayesian Separation Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Separation Logic"
                },
                "summary": "Bayesian probabilistic programming languages (BPPLs) let users denote\nstatistical models as code while the interpreter infers the posterior\ndistribution. The semantics of BPPLs are usually mathematically complex and\nunable to reason about desirable properties such as expected values and\nindependence of random variables. To reason about these properties in a\nnon-Bayesian setting, probabilistic separation logics such as PSL and Lilac\ninterpret separating conjunction as probabilistic independence of random\nvariables. However, no existing separation logic can handle Bayesian updating,\nwhich is the key distinguishing feature of BPPLs.\n  To close this gap, we introduce Bayesian separation logic (BaSL), a\nprobabilistic separation logic that gives semantics to BPPL. We prove an\ninternal version of Bayes' theorem using a result in measure theory known as\nthe Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model\nprobabilistic programming concepts such as Bayesian updating, unnormalised\ndistribution, conditional distribution, soft constraint, conjugate prior and\nimproper prior while maintaining modularity via the frame rule. The model of\nBaSL is based on a novel instantiation of Kripke resource monoid via\n$\\sigma$-finite measure spaces over the Hilbert cube, and the semantics of\nHoare triple is compatible with an existing denotational semantics of BPPL\nbased on the category of $s$-finite kernels. Using BaSL, we then prove\nproperties of statistical models such as the expected value of Bayesian coin\nflip, correlation of random variables in the collider Bayesian network, and the\nposterior distributions of the burglar alarm model, a parameter estimation\nalgorithm, and the Gaussian mixture model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian probabilistic programming languages (BPPLs) let users denote\nstatistical models as code while the interpreter infers the posterior\ndistribution. The semantics of BPPLs are usually mathematically complex and\nunable to reason about desirable properties such as expected values and\nindependence of random variables. To reason about these properties in a\nnon-Bayesian setting, probabilistic separation logics such as PSL and Lilac\ninterpret separating conjunction as probabilistic independence of random\nvariables. However, no existing separation logic can handle Bayesian updating,\nwhich is the key distinguishing feature of BPPLs.\n  To close this gap, we introduce Bayesian separation logic (BaSL), a\nprobabilistic separation logic that gives semantics to BPPL. We prove an\ninternal version of Bayes' theorem using a result in measure theory known as\nthe Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model\nprobabilistic programming concepts such as Bayesian updating, unnormalised\ndistribution, conditional distribution, soft constraint, conjugate prior and\nimproper prior while maintaining modularity via the frame rule. The model of\nBaSL is based on a novel instantiation of Kripke resource monoid via\n$\\sigma$-finite measure spaces over the Hilbert cube, and the semantics of\nHoare triple is compatible with an existing denotational semantics of BPPL\nbased on the category of $s$-finite kernels. Using BaSL, we then prove\nproperties of statistical models such as the expected value of Bayesian coin\nflip, correlation of random variables in the collider Bayesian network, and the\nposterior distributions of the burglar alarm model, a parameter estimation\nalgorithm, and the Gaussian mixture model."
                },
                "authors": [
                    {
                        "name": "Shing Hin Ho"
                    },
                    {
                        "name": "Nicolas Wu"
                    },
                    {
                        "name": "Azalea Raad"
                    }
                ],
                "author_detail": {
                    "name": "Azalea Raad"
                },
                "author": "Azalea Raad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; F.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15524v1",
                "updated": "2025-07-21T11:49:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    49,
                    20,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:49:20Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    49,
                    20,
                    0,
                    202,
                    0
                ],
                "title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image\n  Segmentation"
                },
                "summary": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet."
                },
                "authors": [
                    {
                        "name": "Simon Winther Albertsen"
                    },
                    {
                        "name": "Hjalte Svaneborg Bjrnstrup"
                    },
                    {
                        "name": "Mostafa Mehdipour Ghazi"
                    }
                ],
                "author_detail": {
                    "name": "Mostafa Mehdipour Ghazi"
                },
                "author": "Mostafa Mehdipour Ghazi",
                "arxiv_comment": "EMA4MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00024v2",
                "updated": "2025-07-21T11:43:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    43,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-24T10:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    4,
                    44,
                    0,
                    55,
                    0
                ],
                "title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach\n  with LLM-based Manipulation Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach\n  with LLM-based Manipulation Checks"
                },
                "summary": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments."
                },
                "authors": [
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "ACL 2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15521v1",
                "updated": "2025-07-21T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    42,
                    3,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:42:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    42,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "LLM world models are mental: Output layer evidence of brittle world\n  model use in LLM mechanical reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM world models are mental: Output layer evidence of brittle world\n  model use in LLM mechanical reasoning"
                },
                "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Cole Robertson"
                    },
                    {
                        "name": "Philip Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Philip Wolff"
                },
                "author": "Philip Wolff",
                "arxiv_comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15518v1",
                "updated": "2025-07-21T11:36:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    36,
                    39,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:36:39Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    36,
                    39,
                    0,
                    202,
                    0
                ],
                "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics"
                },
                "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET."
                },
                "authors": [
                    {
                        "name": "Sizhou Chen"
                    },
                    {
                        "name": "Shufan Jiang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xiao-Lei Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15512v1",
                "updated": "2025-07-21T11:28:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    28,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:28:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    28,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models"
                },
                "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Yonghao Shi"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15502v1",
                "updated": "2025-07-21T11:07:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:07:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "FollowUpBot: An LLM-Based Conversational Robot for Automatic\n  Postoperative Follow-up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FollowUpBot: An LLM-Based Conversational Robot for Automatic\n  Postoperative Follow-up"
                },
                "summary": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jianing Yin"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Mingjin Zhang"
                    },
                    {
                        "name": "Weixun Gao"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Haihua Shu"
                    }
                ],
                "author_detail": {
                    "name": "Haihua Shu"
                },
                "author": "Haihua Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15501v1",
                "updated": "2025-07-21T11:07:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:07:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action\n  Execution"
                },
                "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation."
                },
                "authors": [
                    {
                        "name": "Alexandru Coca"
                    },
                    {
                        "name": "Mark Gaynor"
                    },
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Jianpeng Cheng"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Pete Boothroyd"
                    },
                    {
                        "name": "Hctor Martinez Alonso"
                    },
                    {
                        "name": "Diarmuid  Saghdha"
                    },
                    {
                        "name": "Anders Johannsen"
                    }
                ],
                "author_detail": {
                    "name": "Anders Johannsen"
                },
                "author": "Anders Johannsen",
                "arxiv_comment": "37 pages, 22 figures. To appear at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09493v3",
                "updated": "2025-07-21T10:23:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    23,
                    44,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-16T12:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating Conversational Recommender Systems via Large Language Models:\n  A User-Centric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Conversational Recommender Systems via Large Language Models:\n  A User-Centric Framework"
                },
                "summary": "Conversational recommender systems (CRSs) integrate both recommendation and\ndialogue tasks, making their evaluation uniquely challenging. Existing\napproaches primarily assess CRS performance by separately evaluating item\nrecommendation and dialogue management using rule-based metrics. However, these\nmethods fail to capture the real human experience, and they cannot draw direct\nconclusions about the system's overall performance. As conversational\nrecommender systems become increasingly vital in e-commerce, social media, and\ncustomer support, the ability to evaluate both recommendation accuracy and\ndialogue management quality using a single metric, thereby authentically\nreflecting user experience, has become the principal challenge impeding\nprogress in this field.\n  In this work, we propose a user-centric evaluation framework based on large\nlanguage models (LLMs) for CRSs, namely Conversational Recommendation Evaluator\n(CoRE). CoRE consists of two main components: (1) LLM-As-Evaluator. Firstly, we\ncomprehensively summarize 12 key factors influencing user experience in CRSs\nand directly leverage LLM as an evaluator to assign a score to each factor. (2)\nMulti-Agent Debater. Secondly, we design a multi-agent debate framework with\nfour distinct roles (common user, domain expert, linguist, and HCI expert) to\ndiscuss and synthesize the 12 evaluation factors into a unified overall\nperformance score.\n  Furthermore, we apply the proposed framework to evaluate four CRSs on two\nbenchmark datasets. The experimental results show that CoRE aligns well with\nhuman evaluation in most of the 12 factors and the overall assessment.\nEspecially, CoRE's overall evaluation scores demonstrate significantly better\nalignment with human feedback compared to existing rule-based metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems (CRSs) integrate both recommendation and\ndialogue tasks, making their evaluation uniquely challenging. Existing\napproaches primarily assess CRS performance by separately evaluating item\nrecommendation and dialogue management using rule-based metrics. However, these\nmethods fail to capture the real human experience, and they cannot draw direct\nconclusions about the system's overall performance. As conversational\nrecommender systems become increasingly vital in e-commerce, social media, and\ncustomer support, the ability to evaluate both recommendation accuracy and\ndialogue management quality using a single metric, thereby authentically\nreflecting user experience, has become the principal challenge impeding\nprogress in this field.\n  In this work, we propose a user-centric evaluation framework based on large\nlanguage models (LLMs) for CRSs, namely Conversational Recommendation Evaluator\n(CoRE). CoRE consists of two main components: (1) LLM-As-Evaluator. Firstly, we\ncomprehensively summarize 12 key factors influencing user experience in CRSs\nand directly leverage LLM as an evaluator to assign a score to each factor. (2)\nMulti-Agent Debater. Secondly, we design a multi-agent debate framework with\nfour distinct roles (common user, domain expert, linguist, and HCI expert) to\ndiscuss and synthesize the 12 evaluation factors into a unified overall\nperformance score.\n  Furthermore, we apply the proposed framework to evaluate four CRSs on two\nbenchmark datasets. The experimental results show that CoRE aligns well with\nhuman evaluation in most of the 12 factors and the overall assessment.\nEspecially, CoRE's overall evaluation scores demonstrate significantly better\nalignment with human feedback compared to existing rule-based metrics."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Qinglin Jia"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21313v2",
                "updated": "2025-07-21T10:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    20,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-27T09:45:09Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    45,
                    9,
                    3,
                    86,
                    0
                ],
                "title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HORT: Monocular Hand-held Objects Reconstruction with Transformers"
                },
                "summary": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images."
                },
                "authors": [
                    {
                        "name": "Zerui Chen"
                    },
                    {
                        "name": "Rolandos Alexandros Potamias"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Cordelia Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Cordelia Schmid"
                },
                "author": "Cordelia Schmid",
                "arxiv_comment": "Accepted by ICCV 2025. Project Page:\n  https://zerchen.github.io/projects/hort.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15465v1",
                "updated": "2025-07-21T10:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    18,
                    33,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T10:18:33Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    18,
                    33,
                    0,
                    202,
                    0
                ],
                "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts"
                },
                "summary": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models."
                },
                "authors": [
                    {
                        "name": "Sungmin Yun"
                    },
                    {
                        "name": "Seonyong Park"
                    },
                    {
                        "name": "Hwayong Nam"
                    },
                    {
                        "name": "Younjoo Lee"
                    },
                    {
                        "name": "Gunjun Lee"
                    },
                    {
                        "name": "Kwanhee Kyung"
                    },
                    {
                        "name": "Sangpyo Kim"
                    },
                    {
                        "name": "Nam Sung Kim"
                    },
                    {
                        "name": "Jongmin Kim"
                    },
                    {
                        "name": "Hyungyo Kim"
                    },
                    {
                        "name": "Juhwan Cho"
                    },
                    {
                        "name": "Seungmin Baek"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15460v2",
                "updated": "2025-07-22T09:04:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    4,
                    45,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-21T10:14:00Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    14,
                    0,
                    0,
                    202,
                    0
                ],
                "title": "Privacy-Preserving Multimodal News Recommendation through Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Multimodal News Recommendation through Federated\n  Learning"
                },
                "summary": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation."
                },
                "authors": [
                    {
                        "name": "Mehdi Khalaj"
                    },
                    {
                        "name": "Shahrzad Golestani Najafabadi"
                    },
                    {
                        "name": "Julita Vassileva"
                    }
                ],
                "author_detail": {
                    "name": "Julita Vassileva"
                },
                "author": "Julita Vassileva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.15855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15855v1",
                "updated": "2025-07-21T17:59:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:59:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025"
                },
                "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models."
                },
                "authors": [
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Lin F. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin F. Yang"
                },
                "author": "Lin F. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15851v1",
                "updated": "2025-07-21T17:59:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:59:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    59,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Other Mind: How Language Models Exhibit Human Temporal Cognition"
                },
                "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io."
                },
                "authors": [
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Chubo Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "arxiv_comment": "12 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15850v1",
                "updated": "2025-07-21T17:58:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:58:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    58,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3LM: Bridging Arabic, STEM, and Code through Benchmarking"
                },
                "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."
                },
                "authors": [
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15849v1",
                "updated": "2025-07-21T17:56:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:56:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Language Mixing on Bilingual LLM Reasoning"
                },
                "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior."
                },
                "authors": [
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Jiayi Xin"
                    },
                    {
                        "name": "Miranda Muqing Miao"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15839v1",
                "updated": "2025-07-21T17:51:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    51,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    51,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs"
                },
                "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation."
                },
                "authors": [
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Sam Schafft"
                    },
                    {
                        "name": "Nicholas Hale"
                    },
                    {
                        "name": "John Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "John Alfaro"
                },
                "author": "John Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13334v2",
                "updated": "2025-07-21T17:48:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    48,
                    18,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-17T17:50:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "A Survey of Context Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Context Engineering for Large Language Models"
                },
                "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jiazhi Liu"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Tianze Xia"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Liu"
                },
                "author": "Shenghua Liu",
                "arxiv_comment": "ongoing work; 166 pages, 1411 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15828v1",
                "updated": "2025-07-21T17:37:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    37,
                    23,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:37:23Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    37,
                    23,
                    0,
                    202,
                    0
                ],
                "title": "Investigating the Use of LLMs for Evidence Briefings Generation in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Use of LLMs for Evidence Briefings Generation in\n  Software Engineering"
                },
                "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results."
                },
                "authors": [
                    {
                        "name": "Mauro Marcelino"
                    },
                    {
                        "name": "Marcos Alves"
                    },
                    {
                        "name": "Bianca Trinkenreich"
                    },
                    {
                        "name": "Bruno Cartaxo"
                    },
                    {
                        "name": "Srgio Soares"
                    },
                    {
                        "name": "Simone D. J. Barbosa"
                    },
                    {
                        "name": "Marcos Kalinowski"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Kalinowski"
                },
                "author": "Marcos Kalinowski",
                "arxiv_comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15826v1",
                "updated": "2025-07-21T17:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    36,
                    3,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:36:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    36,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language\n  Music Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language\n  Music Recommendation"
                },
                "summary": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks."
                },
                "authors": [
                    {
                        "name": "Alessandro B. Melchiorre"
                    },
                    {
                        "name": "Elena V. Epure"
                    },
                    {
                        "name": "Shahed Masoudian"
                    },
                    {
                        "name": "Gustavo Escobedo"
                    },
                    {
                        "name": "Anna Hausberger"
                    },
                    {
                        "name": "Manuel Moussallam"
                    },
                    {
                        "name": "Markus Schedl"
                    }
                ],
                "author_detail": {
                    "name": "Markus Schedl"
                },
                "author": "Markus Schedl",
                "arxiv_doi": "10.1145/3705328.3748020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3748020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.15826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15825v1",
                "updated": "2025-07-21T17:33:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    33,
                    15,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:33:15Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    33,
                    15,
                    0,
                    202,
                    0
                ],
                "title": "ACS: An interactive framework for conformal selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACS: An interactive framework for conformal selection"
                },
                "summary": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Yu Gui"
                    },
                    {
                        "name": "Ying Jin"
                    },
                    {
                        "name": "Yash Nair"
                    },
                    {
                        "name": "Zhimei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhimei Ren"
                },
                "author": "Zhimei Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15823v1",
                "updated": "2025-07-21T17:30:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:30:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of\n  AI Models in Humanitarian Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operationalizing AI for Good: Spotlight on Deployment and Integration of\n  AI Models in Humanitarian Work"
                },
                "summary": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners."
                },
                "authors": [
                    {
                        "name": "Anton Abilov"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Hemank Lamba"
                    },
                    {
                        "name": "Elizabeth M. Olson"
                    },
                    {
                        "name": "Joel R. Tetreault"
                    },
                    {
                        "name": "Alejandro Jaimes"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Jaimes"
                },
                "author": "Alejandro Jaimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15822v1",
                "updated": "2025-07-21T17:30:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:30:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Do AI models help produce verified bug fixes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do AI models help produce verified bug fixes?"
                },
                "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair."
                },
                "authors": [
                    {
                        "name": "Li Huang"
                    },
                    {
                        "name": "Ilgiz Mustafin"
                    },
                    {
                        "name": "Marco Piccioni"
                    },
                    {
                        "name": "Alessandro Schena"
                    },
                    {
                        "name": "Reto Weber"
                    },
                    {
                        "name": "Bertrand Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Meyer"
                },
                "author": "Bertrand Meyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15821v1",
                "updated": "2025-07-21T17:29:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    29,
                    21,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:29:21Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    29,
                    21,
                    0,
                    202,
                    0
                ],
                "title": "Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for\n  Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for\n  Subjective Tasks"
                },
                "summary": "LLM use in annotation is becoming widespread, and given LLMs' overall\npromising performance and speed, simply \"reviewing\" LLM annotations in\ninterpretive tasks can be tempting. In subjective annotation tasks with\nmultiple plausible answers, reviewing LLM outputs can change the label\ndistribution, impacting both the evaluation of LLM performance, and analysis\nusing these labels in a social science task downstream. We conducted a\npre-registered experiment with 410 unique annotators and over 7,000 annotations\ntesting three AI assistance conditions against controls, using two models, and\ntwo datasets. We find that presenting crowdworkers with LLM-generated\nannotation suggestions did not make them faster, but did improve their\nself-reported confidence in the task. More importantly, annotators strongly\ntook the LLM suggestions, significantly changing the label distribution\ncompared to the baseline. When these labels created with LLM assistance are\nused to evaluate LLM performance, reported model performance significantly\nincreases. We believe our work underlines the importance of understanding the\nimpact of LLM-assisted annotation on subjective, qualitative tasks, on the\ncreation of gold data for training and testing, and on the evaluation of NLP\nsystems on subjective tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM use in annotation is becoming widespread, and given LLMs' overall\npromising performance and speed, simply \"reviewing\" LLM annotations in\ninterpretive tasks can be tempting. In subjective annotation tasks with\nmultiple plausible answers, reviewing LLM outputs can change the label\ndistribution, impacting both the evaluation of LLM performance, and analysis\nusing these labels in a social science task downstream. We conducted a\npre-registered experiment with 410 unique annotators and over 7,000 annotations\ntesting three AI assistance conditions against controls, using two models, and\ntwo datasets. We find that presenting crowdworkers with LLM-generated\nannotation suggestions did not make them faster, but did improve their\nself-reported confidence in the task. More importantly, annotators strongly\ntook the LLM suggestions, significantly changing the label distribution\ncompared to the baseline. When these labels created with LLM assistance are\nused to evaluate LLM performance, reported model performance significantly\nincreases. We believe our work underlines the importance of understanding the\nimpact of LLM-assisted annotation on subjective, qualitative tasks, on the\ncreation of gold data for training and testing, and on the evaluation of NLP\nsystems on subjective tasks."
                },
                "authors": [
                    {
                        "name": "Hope Schroeder"
                    },
                    {
                        "name": "Deb Roy"
                    },
                    {
                        "name": "Jad Kabbara"
                    }
                ],
                "author_detail": {
                    "name": "Jad Kabbara"
                },
                "author": "Jad Kabbara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15815v1",
                "updated": "2025-07-21T17:21:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    21,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T17:21:14Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    21,
                    14,
                    0,
                    202,
                    0
                ],
                "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra"
                },
                "summary": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations."
                },
                "authors": [
                    {
                        "name": "Seth Karten"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Zihan Ding"
                    },
                    {
                        "name": "Samuel Kleiner"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "arxiv_comment": "27 pages, 6 figures, Code:\n  https://github.com/sethkarten/LLM-Economist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15796v1",
                "updated": "2025-07-21T16:57:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    57,
                    6,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:57:06Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    57,
                    6,
                    0,
                    202,
                    0
                ],
                "title": "Challenges of Trustworthy Federated Learning: What's Done, Current\n  Trends and Remaining Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges of Trustworthy Federated Learning: What's Done, Current\n  Trends and Remaining Work"
                },
                "summary": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges."
                },
                "authors": [
                    {
                        "name": "Nuria Rodrguez-Barroso"
                    },
                    {
                        "name": "Mario Garca-Mrquez"
                    },
                    {
                        "name": "M. Victoria Luzn"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15788v1",
                "updated": "2025-07-21T16:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    47,
                    59,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:47:59Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    47,
                    59,
                    0,
                    202,
                    0
                ],
                "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability."
                },
                "authors": [
                    {
                        "name": "Sneheel Sarangi"
                    },
                    {
                        "name": "Hanan Salam"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Salam"
                },
                "author": "Hanan Salam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15782v1",
                "updated": "2025-07-21T16:37:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    50,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:37:50Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    50,
                    0,
                    202,
                    0
                ],
                "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object\n  Collection in Large Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved LLM and Motion Planning for Generalized Multi-Object\n  Collection in Large Scene Graphs"
                },
                "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs."
                },
                "authors": [
                    {
                        "name": "Ruochu Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Fumin Zhang"
                    },
                    {
                        "name": "Mengxue Hou"
                    }
                ],
                "author_detail": {
                    "name": "Mengxue Hou"
                },
                "author": "Mengxue Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11558v3",
                "updated": "2025-07-21T16:37:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    37,
                    0,
                    0,
                    202,
                    0
                ],
                "published": "2025-06-13T08:13:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs"
                },
                "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."
                },
                "authors": [
                    {
                        "name": "Bo-Cheng Chiu"
                    },
                    {
                        "name": "Jen-Jee Chen"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Feng-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Chi Chen"
                },
                "author": "Feng-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15779v1",
                "updated": "2025-07-21T16:35:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    35,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:35:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    35,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Reservoir Computing as a Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir Computing as a Language Model"
                },
                "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance."
                },
                "authors": [
                    {
                        "name": "Felix Kster"
                    },
                    {
                        "name": "Atsushi Uchida"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Uchida"
                },
                "author": "Atsushi Uchida",
                "arxiv_comment": "8 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15778v1",
                "updated": "2025-07-21T16:34:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:34:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR."
                },
                "authors": [
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15771v1",
                "updated": "2025-07-21T16:27:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:27:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    27,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Left Leaning Models: AI Assumptions on Economic Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left Leaning Models: AI Assumptions on Economic Policy"
                },
                "summary": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models."
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin",
                "arxiv_comment": "8 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15770v1",
                "updated": "2025-07-21T16:26:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    26,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:26:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    26,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems\n  Through LLM-based Agent Intention Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems\n  Through LLM-based Agent Intention Mining"
                },
                "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085."
                },
                "authors": [
                    {
                        "name": "Yifan Shen"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Qun Ma"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15761v1",
                "updated": "2025-07-21T16:17:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    17,
                    25,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:17:25Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    17,
                    25,
                    0,
                    202,
                    0
                ],
                "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in\n  Smart Contracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in\n  Smart Contracts"
                },
                "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Yifan Liao"
                    },
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08033v3",
                "updated": "2025-07-21T16:17:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    17,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-12T00:26:01Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    0,
                    26,
                    1,
                    2,
                    43,
                    0
                ],
                "title": "Predictive Planner for Autonomous Driving with Consistency Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Planner for Autonomous Driving with Consistency Models"
                },
                "summary": "Trajectory prediction and planning are essential for autonomous vehicles to\nnavigate safely and efficiently in dynamic environments. Traditional approaches\noften treat them separately, limiting the ability for interactive planning.\nWhile recent diffusion-based generative models have shown promise in\nmulti-agent trajectory generation, their slow sampling is less suitable for\nhigh-frequency planning tasks. In this paper, we leverage the consistency model\nto build a predictive planner that samples from a joint distribution of ego and\nsurrounding agents, conditioned on the ego vehicle's navigational goal. Trained\non real-world human driving datasets, our consistency model generates\nhigher-quality trajectories with fewer sampling steps than standard diffusion\nmodels, making it more suitable for real-time deployment. To enforce multiple\nplanning constraints simultaneously on the ego trajectory, a novel online\nguided sampling approach inspired by the Alternating Direction Method of\nMultipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset\n(WOMD), our method enables proactive behavior such as nudging and yielding, and\nalso demonstrates smoother, safer, and more efficient trajectories and\nsatisfaction of multiple constraints under a limited computational budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction and planning are essential for autonomous vehicles to\nnavigate safely and efficiently in dynamic environments. Traditional approaches\noften treat them separately, limiting the ability for interactive planning.\nWhile recent diffusion-based generative models have shown promise in\nmulti-agent trajectory generation, their slow sampling is less suitable for\nhigh-frequency planning tasks. In this paper, we leverage the consistency model\nto build a predictive planner that samples from a joint distribution of ego and\nsurrounding agents, conditioned on the ego vehicle's navigational goal. Trained\non real-world human driving datasets, our consistency model generates\nhigher-quality trajectories with fewer sampling steps than standard diffusion\nmodels, making it more suitable for real-time deployment. To enforce multiple\nplanning constraints simultaneously on the ego trajectory, a novel online\nguided sampling approach inspired by the Alternating Direction Method of\nMultipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset\n(WOMD), our method enables proactive behavior such as nudging and yielding, and\nalso demonstrates smoother, safer, and more efficient trajectories and\nsatisfaction of multiple constraints under a limited computational budget."
                },
                "authors": [
                    {
                        "name": "Anjian Li"
                    },
                    {
                        "name": "Sangjae Bae"
                    },
                    {
                        "name": "David Isele"
                    },
                    {
                        "name": "Ryne Beeson"
                    },
                    {
                        "name": "Faizan M. Tariq"
                    }
                ],
                "author_detail": {
                    "name": "Faizan M. Tariq"
                },
                "author": "Faizan M. Tariq",
                "arxiv_comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08985v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08985v4",
                "updated": "2025-07-21T16:16:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    16,
                    56,
                    0,
                    202,
                    0
                ],
                "published": "2024-12-12T06:38:40Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    38,
                    40,
                    3,
                    347,
                    0
                ],
                "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08985v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08985v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15752v1",
                "updated": "2025-07-21T16:08:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    8,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T16:08:19Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    8,
                    19,
                    0,
                    202,
                    0
                ],
                "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue"
                },
                "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zhu"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Yaxuan Li"
                    },
                    {
                        "name": "Syang Zhou"
                    },
                    {
                        "name": "Shijing Cai"
                    },
                    {
                        "name": "Malgorzata Lazuka"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "arxiv_comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15736v1",
                "updated": "2025-07-21T15:43:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    43,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:43:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    43,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "Understanding Large Language Models' Ability on Interdisciplinary\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Models' Ability on Interdisciplinary\n  Research"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch."
                },
                "authors": [
                    {
                        "name": "Yuanhao Shen"
                    },
                    {
                        "name": "Daniel Xavier de Sousa"
                    },
                    {
                        "name": "Ricardo Maral"
                    },
                    {
                        "name": "Ali Asad"
                    },
                    {
                        "name": "Hongyu Guo"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15729v1",
                "updated": "2025-07-21T15:38:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    38,
                    25,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:38:25Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    38,
                    25,
                    0,
                    202,
                    0
                ],
                "title": "Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks."
                },
                "authors": [
                    {
                        "name": "Jens V. Rppel"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Tim Schreiter"
                    },
                    {
                        "name": "Martin Magnusson"
                    },
                    {
                        "name": "Achim J. Lilienthal"
                    }
                ],
                "author_detail": {
                    "name": "Achim J. Lilienthal"
                },
                "author": "Achim J. Lilienthal",
                "arxiv_comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15717v1",
                "updated": "2025-07-21T15:27:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    27,
                    32,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:27:32Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    27,
                    32,
                    0,
                    202,
                    0
                ],
                "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological\n  Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological\n  Knowledge and Reasoning"
                },
                "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels."
                },
                "authors": [
                    {
                        "name": "Sahana Srinivasan"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Thaddaeus Wai Soon Lo"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Minjie Zou"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Mingjia Yang"
                    },
                    {
                        "name": "Krithi Pushpanathan"
                    },
                    {
                        "name": "Samantha Yew"
                    },
                    {
                        "name": "Wan Ting Loke"
                    },
                    {
                        "name": "Jocelyn Goh"
                    },
                    {
                        "name": "Yibing Chen"
                    },
                    {
                        "name": "Yiming Kong"
                    },
                    {
                        "name": "Emily Yuelei Fu"
                    },
                    {
                        "name": "Michelle Ongyong Hui"
                    },
                    {
                        "name": "Kristen Nwanyanwu"
                    },
                    {
                        "name": "Amisha Dave"
                    },
                    {
                        "name": "Kelvin Zhenghao Li"
                    },
                    {
                        "name": "Chen-Hsin Sun"
                    },
                    {
                        "name": "Mark Chia"
                    },
                    {
                        "name": "Gabriel Dawei Yang"
                    },
                    {
                        "name": "Wendy Meihua Wong"
                    },
                    {
                        "name": "David Ziyou Chen"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Maxwell Singer"
                    },
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "Lucian V Del Priore"
                    },
                    {
                        "name": "Jost Jonas"
                    },
                    {
                        "name": "Ron Adelman"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    }
                ],
                "author_detail": {
                    "name": "Yih-Chung Tham"
                },
                "author": "Yih-Chung Tham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15715v1",
                "updated": "2025-07-21T15:26:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    26,
                    58,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:26:58Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    26,
                    58,
                    0,
                    202,
                    0
                ],
                "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs"
                },
                "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research."
                },
                "authors": [
                    {
                        "name": "Alina Hyk"
                    },
                    {
                        "name": "Kiera McCormick"
                    },
                    {
                        "name": "Mian Zhong"
                    },
                    {
                        "name": "Ioana Ciuc"
                    },
                    {
                        "name": "Sanjib Sharma"
                    },
                    {
                        "name": "John F Wu"
                    },
                    {
                        "name": "J. E. G. Peek"
                    },
                    {
                        "name": "Kartheik G. Iyer"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16772v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16772v6",
                "updated": "2025-07-21T15:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    25,
                    51,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-24T01:35:32Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    35,
                    32,
                    0,
                    55,
                    0
                ],
                "title": "Model-Based Exploration in Monitored Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Exploration in Monitored Markov Decision Processes"
                },
                "summary": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable."
                },
                "authors": [
                    {
                        "name": "Alireza Kazemipour"
                    },
                    {
                        "name": "Simone Parisi"
                    },
                    {
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "name": "Michael Bowling"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bowling"
                },
                "author": "Michael Bowling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16772v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16772v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17259v2",
                "updated": "2025-07-21T15:24:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    24,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-24T15:39:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "Detecting Benchmark Contamination Through Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Benchmark Contamination Through Watermarking"
                },
                "summary": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, \\eg $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, \\eg $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy."
                },
                "authors": [
                    {
                        "name": "Tom Sander"
                    },
                    {
                        "name": "Pierre Fernandez"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19545v2",
                "updated": "2025-07-21T15:24:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    24,
                    26,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-26T20:34:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    34,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training\n  for Reducing Hallucination in Product QA Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training\n  for Reducing Hallucination in Product QA Agents"
                },
                "summary": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations."
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Michael White"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Toshiaki Koike-Akino"
                    },
                    {
                        "name": "Kieran Parsons"
                    },
                    {
                        "name": "Ye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wang"
                },
                "author": "Ye Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15709v1",
                "updated": "2025-07-21T15:17:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    17,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:17:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    17,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Face Image Quality Assessment via Self-training and Knowledge\n  Distillation"
                },
                "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git."
                },
                "authors": [
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Weixia Zhang"
                    },
                    {
                        "name": "Linhan Cao"
                    },
                    {
                        "name": "Jun Jia"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Dandan Zhu"
                    },
                    {
                        "name": "Xiongkuo Min"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "arxiv_comment": "Efficient-FIQA achieved first place in the ICCV VQualA 2025 Face\n  Image Quality Assessment Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15707v1",
                "updated": "2025-07-21T15:15:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    15,
                    30,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:15:30Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    15,
                    30,
                    0,
                    202,
                    0
                ],
                "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by\n  Different Ways Questions Are Asked?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Large Language Model Performance on Reasoning Tasks Impacted by\n  Different Ways Questions Are Asked?"
                },
                "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance."
                },
                "authors": [
                    {
                        "name": "Seok Hwan Song"
                    },
                    {
                        "name": "Mohna Chakraborty"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Wallapak Tavanapong"
                    }
                ],
                "author_detail": {
                    "name": "Wallapak Tavanapong"
                },
                "author": "Wallapak Tavanapong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15698v1",
                "updated": "2025-07-21T15:07:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    59,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T15:07:59Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    59,
                    0,
                    202,
                    0
                ],
                "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models"
                },
                "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs."
                },
                "authors": [
                    {
                        "name": "Congmin Zheng"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Mengyue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Yang"
                },
                "author": "Mengyue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15686v1",
                "updated": "2025-07-21T14:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    48,
                    54,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:48:54Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    48,
                    54,
                    0,
                    202,
                    0
                ],
                "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud\n  Geometry Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud\n  Geometry Compression"
                },
                "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/."
                },
                "authors": [
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Shuting Xia"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Zhu Li"
                    },
                    {
                        "name": "Yiling Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Xu"
                },
                "author": "Yiling Xu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06565v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06565v4",
                "updated": "2025-07-21T14:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    44,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-09T05:39:56Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    39,
                    56,
                    2,
                    190,
                    0
                ],
                "title": "A Mathematical Theory of Discursive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mathematical Theory of Discursive Networks"
                },
                "summary": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability."
                },
                "authors": [
                    {
                        "name": "Juan B. Gutirrez"
                    }
                ],
                "author_detail": {
                    "name": "Juan B. Gutirrez"
                },
                "author": "Juan B. Gutirrez",
                "arxiv_comment": "39 pages, 4 figures, 4 tables, 3 algorithm, 56 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06565v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06565v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15680v2",
                "updated": "2025-07-22T14:17:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    14,
                    17,
                    49,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-21T14:44:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    44,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "Visual-Language Model Knowledge Distillation Method for Image Quality\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-Language Model Knowledge Distillation Method for Image Quality\n  Assessment"
                },
                "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment."
                },
                "authors": [
                    {
                        "name": "Yongkang Hou"
                    },
                    {
                        "name": "Jiarun Song"
                    }
                ],
                "author_detail": {
                    "name": "Jiarun Song"
                },
                "author": "Jiarun Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09763v2",
                "updated": "2025-07-21T14:41:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    41,
                    39,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-14T00:06:48Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    6,
                    48,
                    0,
                    104,
                    0
                ],
                "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems"
                },
                "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project Page: https://zaidkhan.me/EFAGen/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15675v1",
                "updated": "2025-07-21T14:37:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    37,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:37:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    37,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "P3: Prompts Promote Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3: Prompts Promote Prompting"
                },
                "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yuanquan Hu"
                    },
                    {
                        "name": "Fangchao Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Accepted to ACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15671v1",
                "updated": "2025-07-21T14:34:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:34:01Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    34,
                    1,
                    0,
                    202,
                    0
                ],
                "title": "BugScope: Learn to Find Bugs Like Human",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BugScope: Learn to Find Bugs Like Human"
                },
                "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact."
                },
                "authors": [
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Dominic Deluca"
                    },
                    {
                        "name": "Jinjie Liu"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 2 figure, 6 tables, 4 listings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15670v1",
                "updated": "2025-07-21T14:33:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    33,
                    40,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:33:40Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    33,
                    40,
                    0,
                    202,
                    0
                ],
                "title": "Vehicular Cloud Computing: A cost-effective alternative to Edge\n  Computing in 5G networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicular Cloud Computing: A cost-effective alternative to Edge\n  Computing in 5G networks"
                },
                "summary": "Edge Computing (EC) is a computational paradigm that involves deploying\nresources such as CPUs and GPUs near end-users, enabling low-latency\napplications like augmented reality and real-time gaming. However, deploying\nand maintaining a vast network of EC nodes is costly, which can explain its\nlimited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)\nhas emerged and inspired interest among researchers and industry. VCC\nopportunistically utilizes existing and idle vehicular computational resources\nfor external task offloading. This work is the first to systematically address\nthe following question: Can VCC replace EC for low-latency applications?\nAnswering this question is highly relevant for Network Operators (NOs), as VCC\ncould eliminate costs associated with EC given that it requires no\ninfrastructural investment. Despite its potential, no systematic study has yet\nexplored the conditions under which VCC can effectively support low-latency\napplications without relying on EC. This work aims to fill that gap. Extensive\nsimulations allow for assessing the crucial scenario factors that determine\nwhen this EC-to-VCC substitution is feasible. Considered factors are load,\nvehicles mobility and density, and availability. Potential for substitution is\nassessed based on multiple criteria, such as latency, task completion success,\nand cost. Vehicle mobility is simulated in SUMO, and communication in NS3\n5G-LENA. The findings show that VCC can effectively replace EC for low-latency\napplications, except in extreme cases when the EC is still required (latency <\n16 ms).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Computing (EC) is a computational paradigm that involves deploying\nresources such as CPUs and GPUs near end-users, enabling low-latency\napplications like augmented reality and real-time gaming. However, deploying\nand maintaining a vast network of EC nodes is costly, which can explain its\nlimited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)\nhas emerged and inspired interest among researchers and industry. VCC\nopportunistically utilizes existing and idle vehicular computational resources\nfor external task offloading. This work is the first to systematically address\nthe following question: Can VCC replace EC for low-latency applications?\nAnswering this question is highly relevant for Network Operators (NOs), as VCC\ncould eliminate costs associated with EC given that it requires no\ninfrastructural investment. Despite its potential, no systematic study has yet\nexplored the conditions under which VCC can effectively support low-latency\napplications without relying on EC. This work aims to fill that gap. Extensive\nsimulations allow for assessing the crucial scenario factors that determine\nwhen this EC-to-VCC substitution is feasible. Considered factors are load,\nvehicles mobility and density, and availability. Potential for substitution is\nassessed based on multiple criteria, such as latency, task completion success,\nand cost. Vehicle mobility is simulated in SUMO, and communication in NS3\n5G-LENA. The findings show that VCC can effectively replace EC for low-latency\napplications, except in extreme cases when the EC is still required (latency <\n16 ms)."
                },
                "authors": [
                    {
                        "name": "Rosario Patan"
                    },
                    {
                        "name": "Nadjib Achir"
                    },
                    {
                        "name": "Andrea Araldo"
                    },
                    {
                        "name": "Lila Boukhatem"
                    }
                ],
                "author_detail": {
                    "name": "Lila Boukhatem"
                },
                "author": "Lila Boukhatem",
                "arxiv_doi": "10.1016/j.comnet.2025.111365",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.comnet.2025.111365",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.15670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computer Network, Elsevier, 2025 August",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15664v1",
                "updated": "2025-07-21T14:25:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    25,
                    52,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:25:52Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    25,
                    52,
                    0,
                    202,
                    0
                ],
                "title": "VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability\n  Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability\n  Repair"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential in\ncomputer-aided design (CAD), particularly for automated debugging and\nverification within electronic design automation (EDA) tools. However, Design\nfor Testability (DFT) remains a relatively underexplored area. This paper\npresents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a\nRetrieval-Augmented Generation (RAG) approach to enable LLM to revise code to\nensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity\nmeasurement model for precise retrieval of reference RTL designs for the LLM,\nand (2) an iterative code revision pipeline that allows the LLM to ensure DFT\ncompliance while maintaining synthesizability. To support VeriRAG, we introduce\nVeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG\nretrieves structurally similar RTL designs from VeriDFT, each paired with a\nrigorously validated correction, as references for code repair. With VeriRAG\nand VeriDFT, we achieve fully automated DFT correction -- resulting in a\n7.72-fold improvement in successful repair rate compared to the zero-shot\nbaseline (Fig. 5 in Section V). Ablation studies further confirm the\ncontribution of each component of the VeriRAG framework. We open-source our\ndata, models, and scripts at https://github.com/yuyangdu01/LLM4DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential in\ncomputer-aided design (CAD), particularly for automated debugging and\nverification within electronic design automation (EDA) tools. However, Design\nfor Testability (DFT) remains a relatively underexplored area. This paper\npresents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a\nRetrieval-Augmented Generation (RAG) approach to enable LLM to revise code to\nensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity\nmeasurement model for precise retrieval of reference RTL designs for the LLM,\nand (2) an iterative code revision pipeline that allows the LLM to ensure DFT\ncompliance while maintaining synthesizability. To support VeriRAG, we introduce\nVeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG\nretrieves structurally similar RTL designs from VeriDFT, each paired with a\nrigorously validated correction, as references for code repair. With VeriRAG\nand VeriDFT, we achieve fully automated DFT correction -- resulting in a\n7.72-fold improvement in successful repair rate compared to the zero-shot\nbaseline (Fig. 5 in Section V). Ablation studies further confirm the\ncontribution of each component of the VeriRAG framework. We open-source our\ndata, models, and scripts at https://github.com/yuyangdu01/LLM4DFT."
                },
                "authors": [
                    {
                        "name": "Haomin Qi"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Lihao Zhang"
                    },
                    {
                        "name": "Soung Chang Liew"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Yining Du"
                    }
                ],
                "author_detail": {
                    "name": "Yining Du"
                },
                "author": "Yining Du",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15655v1",
                "updated": "2025-07-21T14:16:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    16,
                    44,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T14:16:44Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    16,
                    44,
                    0,
                    202,
                    0
                ],
                "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding\n  with a Comprehensive VQA Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding\n  with a Comprehensive VQA Benchmark"
                },
                "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain."
                },
                "authors": [
                    {
                        "name": "Aniket Pal"
                    },
                    {
                        "name": "Ajoy Mondal"
                    },
                    {
                        "name": "Minesh Mathew"
                    },
                    {
                        "name": "C. V. Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C. V. Jawahar"
                },
                "author": "C. V. Jawahar",
                "arxiv_comment": "This is a minor revision of the original paper submitted to IJDAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13545v2",
                "updated": "2025-07-21T14:09:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    9,
                    54,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-19T03:17:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    3,
                    17,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Know Or Not: a library for evaluating out-of-knowledge base robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Or Not: a library for evaluating out-of-knowledge base robustness"
                },
                "summary": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot."
                },
                "authors": [
                    {
                        "name": "Jessica Foo"
                    },
                    {
                        "name": "Pradyumna Shyama Prasad"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07057v2",
                "updated": "2025-07-21T14:05:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    5,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-10T21:47:49Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    47,
                    49,
                    0,
                    41,
                    0
                ],
                "title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark"
                },
                "summary": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices."
                },
                "authors": [
                    {
                        "name": "M. Ali Bayram"
                    },
                    {
                        "name": "Ali Arda Fincan"
                    },
                    {
                        "name": "Ahmet Semih Gm"
                    },
                    {
                        "name": "Sercan Karaka"
                    },
                    {
                        "name": "Banu Diri"
                    },
                    {
                        "name": "Sava Yldrm"
                    }
                ],
                "author_detail": {
                    "name": "Sava Yldrm"
                },
                "author": "Sava Yldrm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14000v2",
                "updated": "2025-07-21T14:03:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    3,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-18T15:14:56Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    15,
                    14,
                    56,
                    4,
                    199,
                    0
                ],
                "title": "Photonic Fabric Platform for AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic Fabric Platform for AI Accelerators"
                },
                "summary": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute."
                },
                "authors": [
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "arxiv_comment": "12 pages, 14 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15636v1",
                "updated": "2025-07-21T13:58:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    58,
                    24,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:58:24Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    58,
                    24,
                    0,
                    202,
                    0
                ],
                "title": "Uncovering Critical Features for Deepfake Detection through the Lottery\n  Ticket Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Critical Features for Deepfake Detection through the Lottery\n  Ticket Hypothesis"
                },
                "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems."
                },
                "authors": [
                    {
                        "name": "Lisan Al Amin"
                    },
                    {
                        "name": "Md. Ismail Hossain"
                    },
                    {
                        "name": "Thanh Thi Nguyen"
                    },
                    {
                        "name": "Tasnim Jahan"
                    },
                    {
                        "name": "Mahbubul Islam"
                    },
                    {
                        "name": "Faisal Quader"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Quader"
                },
                "author": "Faisal Quader",
                "arxiv_comment": "Accepted for publication at the 2025 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15628v1",
                "updated": "2025-07-21T13:52:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    52,
                    6,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:52:06Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    52,
                    6,
                    0,
                    202,
                    0
                ],
                "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video\n  Analytics: Process Systems, Algorithms, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Efficiency Optimization Techniques for DNN-based Video\n  Analytics: Process Systems, Algorithms, and Applications"
                },
                "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics."
                },
                "authors": [
                    {
                        "name": "Shanjiang Tang"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Hsinyu Luo"
                    },
                    {
                        "name": "Chunjiang Wang"
                    },
                    {
                        "name": "Ce Yu"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Hao Fu"
                    },
                    {
                        "name": "Chao Sun"
                    },
                    {
                        "name": "and Jian Xiao"
                    }
                ],
                "author_detail": {
                    "name": "and Jian Xiao"
                },
                "author": "and Jian Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03826v2",
                "updated": "2025-07-21T13:49:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    49,
                    34,
                    0,
                    202,
                    0
                ],
                "published": "2025-06-04T10:54:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    10,
                    54,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "SigSPARQL: Signals as a First-Class Citizen When Querying Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SigSPARQL: Signals as a First-Class Citizen When Querying Knowledge\n  Graphs"
                },
                "summary": "Purpose: Cyber-Physical Systems (CPSs) integrate computation and physical\nprocesses, producing time series data from thousands of sensors. Knowledge\ngraphs can contextualize these data, yet current approaches that are applicably\nto monitoring CPS rely on observation-based approaches. This limits the ability\nto express computations on sensor data, especially when no assumptions can be\nmade about sampling synchronicity or sampling rates.\n  Methodology: We propose an approach for integrating knowledge graphs with\nsignals that model run-time sensor data as functions from time to data. To\ndemonstrate this approach, we introduce SigSPARQL, a query language that can\ncombine RDF data and signals. We assess its technical feasibility with a\nprototype and demonstrate its use in a typical CPS monitoring use case.\n  Findings: Our approach enables queries to combine graph-based knowledge with\nsignals, overcoming some key limits of observation-based methods. The developed\nprototype successfully demonstrated feasibility and applicability.\n  Value: This work presents a query-based approach for CPS monitoring that\nintegrates knowledge graphs and signals, alleviating problems of\nobservation-based approaches. By leveraging system knowledge, it enables\noperators to run a single query across different system instances within the\nsame domain. Future work will extend SigSPARQL with additional signal functions\nand evaluate it in large-scale CPS deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Cyber-Physical Systems (CPSs) integrate computation and physical\nprocesses, producing time series data from thousands of sensors. Knowledge\ngraphs can contextualize these data, yet current approaches that are applicably\nto monitoring CPS rely on observation-based approaches. This limits the ability\nto express computations on sensor data, especially when no assumptions can be\nmade about sampling synchronicity or sampling rates.\n  Methodology: We propose an approach for integrating knowledge graphs with\nsignals that model run-time sensor data as functions from time to data. To\ndemonstrate this approach, we introduce SigSPARQL, a query language that can\ncombine RDF data and signals. We assess its technical feasibility with a\nprototype and demonstrate its use in a typical CPS monitoring use case.\n  Findings: Our approach enables queries to combine graph-based knowledge with\nsignals, overcoming some key limits of observation-based methods. The developed\nprototype successfully demonstrated feasibility and applicability.\n  Value: This work presents a query-based approach for CPS monitoring that\nintegrates knowledge graphs and signals, alleviating problems of\nobservation-based approaches. By leveraging system knowledge, it enables\noperators to run a single query across different system instances within the\nsame domain. Future work will extend SigSPARQL with additional signal functions\nand evaluate it in large-scale CPS deployments."
                },
                "authors": [
                    {
                        "name": "Tobias Schwarzinger"
                    },
                    {
                        "name": "Gernot Steindl"
                    },
                    {
                        "name": "Thomas Frhwirth"
                    },
                    {
                        "name": "Thomas Preindl"
                    },
                    {
                        "name": "Konrad Diwold"
                    },
                    {
                        "name": "Katrin Ehrenmller"
                    },
                    {
                        "name": "Fajar J. Ekaputra"
                    }
                ],
                "author_detail": {
                    "name": "Fajar J. Ekaputra"
                },
                "author": "Fajar J. Ekaputra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15615v1",
                "updated": "2025-07-21T13:40:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    40,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:40:19Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    40,
                    19,
                    0,
                    202,
                    0
                ],
                "title": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP\n  Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP\n  Solving"
                },
                "summary": "Primal heuristics play a critical role in improving the efficiency of mixed\ninteger programming (MILP) solvers. As large language models (LLMs) have\ndemonstrated superior code generation abilities, recent MILP works are devoted\nto leveraging the evolutionary computation approaches with LLMs to generate\neffective primal heuristics. Although the generated heuristics have achieved\nbetter solving performance than the hand-crafted ones with little adaptability,\nthe advantage of current LLM-based methods is limited to few MILP instances in\none problem class, as they fail to capture the instance characteristics in the\nproblem class (the MILP instances generated from the same mathematical model\nare defined as a problem class). Since MILP instances often differ\nsignificantly in structure and feature distribution, the neglect of their\ncharacteristics in the evolution process results in poor generalization within\nthe same problem class. To overcome this challenge, we propose a data-algorithm\nco-evolution framework (DHEvo) that iteratively selects representative\ninstances and evolves corresponding heuristics. With the initial instance\ndistribution, we develop an LLM-based multi-agent system to generate data-code\npairs simultaneously. These data-code pairs are iteratively refined based on\ntheir fitness scores, leading to the identification of the most effective\nheuristic over the entire problem class. Extensive experiments across diverse\nMILP benchmarks demonstrate that our approach significantly outperforms both\nhuman-designed heuristics and existing LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primal heuristics play a critical role in improving the efficiency of mixed\ninteger programming (MILP) solvers. As large language models (LLMs) have\ndemonstrated superior code generation abilities, recent MILP works are devoted\nto leveraging the evolutionary computation approaches with LLMs to generate\neffective primal heuristics. Although the generated heuristics have achieved\nbetter solving performance than the hand-crafted ones with little adaptability,\nthe advantage of current LLM-based methods is limited to few MILP instances in\none problem class, as they fail to capture the instance characteristics in the\nproblem class (the MILP instances generated from the same mathematical model\nare defined as a problem class). Since MILP instances often differ\nsignificantly in structure and feature distribution, the neglect of their\ncharacteristics in the evolution process results in poor generalization within\nthe same problem class. To overcome this challenge, we propose a data-algorithm\nco-evolution framework (DHEvo) that iteratively selects representative\ninstances and evolves corresponding heuristics. With the initial instance\ndistribution, we develop an LLM-based multi-agent system to generate data-code\npairs simultaneously. These data-code pairs are iteratively refined based on\ntheir fitness scores, leading to the identification of the most effective\nheuristic over the entire problem class. Extensive experiments across diverse\nMILP benchmarks demonstrate that our approach significantly outperforms both\nhuman-designed heuristics and existing LLM-based methods."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Feifan Liu"
                    },
                    {
                        "name": "Mengjing Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Peng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liu"
                },
                "author": "Peng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15613v1",
                "updated": "2025-07-21T13:38:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    38,
                    12,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:38:12Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    38,
                    12,
                    0,
                    202,
                    0
                ],
                "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems"
                },
                "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses."
                },
                "authors": [
                    {
                        "name": "Andrii Balashov"
                    },
                    {
                        "name": "Olena Ponomarova"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v2",
                "updated": "2025-07-21T13:34:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    34,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15603v1",
                "updated": "2025-07-21T13:26:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    26,
                    2,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:26:02Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    26,
                    2,
                    0,
                    202,
                    0
                ],
                "title": "When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback\n  Alignment: A Co-Design for Neuromorphic Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback\n  Alignment: A Co-Design for Neuromorphic Edge Computing"
                },
                "summary": "Spiking Neural Networks (SNNs) are increasingly favored for deployment on\nresource-constrained edge devices due to their energy-efficient and\nevent-driven processing capabilities. However, training SNNs remains\nchallenging because of the computational intensity of traditional\nbackpropagation algorithms adapted for spike-based systems. In this paper, we\npropose a novel software-hardware co-design that introduces a hardware-friendly\ntraining algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it\non a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)\narchitecture, referred to as PipeSDFA, to accelerate SNN training.\nSoftware-wise, the computational complexity of SNN training is reduced by the\nSDFA through the elimination of sequential error propagation. Hardware-wise, a\nthree-level pipelined dataflow is designed based on IMC architecture to\nparallelize the training process. Experimental results demonstrate that the\nPipeSDFA training accelerator incurs less than 2% accuracy loss on five\ndatasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X\nreductions in training time and energy consumption, respectively compared to\nPipeLayer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are increasingly favored for deployment on\nresource-constrained edge devices due to their energy-efficient and\nevent-driven processing capabilities. However, training SNNs remains\nchallenging because of the computational intensity of traditional\nbackpropagation algorithms adapted for spike-based systems. In this paper, we\npropose a novel software-hardware co-design that introduces a hardware-friendly\ntraining algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it\non a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)\narchitecture, referred to as PipeSDFA, to accelerate SNN training.\nSoftware-wise, the computational complexity of SNN training is reduced by the\nSDFA through the elimination of sequential error propagation. Hardware-wise, a\nthree-level pipelined dataflow is designed based on IMC architecture to\nparallelize the training process. Experimental results demonstrate that the\nPipeSDFA training accelerator incurs less than 2% accuracy loss on five\ndatasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X\nreductions in training time and energy consumption, respectively compared to\nPipeLayer."
                },
                "authors": [
                    {
                        "name": "Haoxiong Ren"
                    },
                    {
                        "name": "Yangu He"
                    },
                    {
                        "name": "Kwunhang Wong"
                    },
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Ning Lin"
                    },
                    {
                        "name": "Zhongrui Wang"
                    },
                    {
                        "name": "Dashan Shang"
                    }
                ],
                "author_detail": {
                    "name": "Dashan Shang"
                },
                "author": "Dashan Shang",
                "arxiv_comment": "International Conference on Computer-Aided Design 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15601v1",
                "updated": "2025-07-21T13:24:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    24,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:24:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    24,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Optimal Batch-Size Control for Low-Latency Federated Learning with\n  Device Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Batch-Size Control for Low-Latency Federated Learning with\n  Device Heterogeneity"
                },
                "summary": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity."
                },
                "authors": [
                    {
                        "name": "Huiling Yang"
                    },
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15599v1",
                "updated": "2025-07-21T13:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    21,
                    29,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    21,
                    29,
                    0,
                    202,
                    0
                ],
                "title": "Applying the Chinese Wall Reverse Engineering Technique to Large\n  Language Model Code Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying the Chinese Wall Reverse Engineering Technique to Large\n  Language Model Code Editing"
                },
                "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions."
                },
                "authors": [
                    {
                        "name": "Manatsawin Hanmongkolchai"
                    }
                ],
                "author_detail": {
                    "name": "Manatsawin Hanmongkolchai"
                },
                "author": "Manatsawin Hanmongkolchai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15586v1",
                "updated": "2025-07-21T13:03:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."
                },
                "authors": [
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Shouzheng Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "16 pages, 7 Figures, 10 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15585v1",
                "updated": "2025-07-21T13:03:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    38,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:03:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Unequal Voices: How LLMs Construct Constrained Queer Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unequal Voices: How LLMs Construct Constrained Queer Narratives"
                },
                "summary": "One way social groups are marginalized in discourse is that the narratives\ntold about them often default to a narrow, stereotyped range of topics. In\ncontrast, default groups are allowed the full complexity of human existence. We\ndescribe the constrained representations of queer people in LLM generations in\nterms of harmful representations, narrow representations, and discursive\nothering and formulate hypotheses to test for these phenomena. Our results show\nthat LLMs are significantly limited in their portrayals of queer personas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way social groups are marginalized in discourse is that the narratives\ntold about them often default to a narrow, stereotyped range of topics. In\ncontrast, default groups are allowed the full complexity of human existence. We\ndescribe the constrained representations of queer people in LLM generations in\nterms of harmful representations, narrow representations, and discursive\nothering and formulate hypotheses to test for these phenomena. Our results show\nthat LLMs are significantly limited in their portrayals of queer personas."
                },
                "authors": [
                    {
                        "name": "Atreya Ghosal"
                    },
                    {
                        "name": "Ashim Gupta"
                    },
                    {
                        "name": "Vivek Srikumar"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Srikumar"
                },
                "author": "Vivek Srikumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15581v1",
                "updated": "2025-07-21T13:01:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    1,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T13:01:46Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    1,
                    46,
                    0,
                    202,
                    0
                ],
                "title": "Metric assessment protocol in the context of answer fluctuation on MCQ\n  tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metric assessment protocol in the context of answer fluctuation on MCQ\n  tasks"
                },
                "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol."
                },
                "authors": [
                    {
                        "name": "Ekaterina Goliakova"
                    },
                    {
                        "name": "Xavier Renard"
                    },
                    {
                        "name": "Marie-Jeanne Lesot"
                    },
                    {
                        "name": "Thibault Laugel"
                    },
                    {
                        "name": "Christophe Marsala"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17181v2",
                "updated": "2025-07-21T12:58:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    58,
                    26,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-21T14:29:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "A Study of LLMs' Preferences for Libraries and Programming Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of LLMs' Preferences for Libraries and Programming Languages"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to generate code,\ninfluencing users' choices of libraries and programming languages in critical\nreal-world projects. However, little is known about their systematic biases or\npreferences toward certain libraries and programming languages, which can\nsignificantly impact software development practices. To fill this gap, we\nperform the first empirical study of LLMs' preferences for libraries and\nprogramming languages when generating code, covering eight diverse LLMs. Our\nresults reveal that LLMs exhibit a strong tendency to overuse widely adopted\nlibraries such as NumPy; in up to 48% of cases, this usage is unnecessary and\ndeviates from the ground-truth solutions. LLMs also exhibit a significant\npreference toward Python as their default language. For high-performance\nproject initialisation tasks where Python is not the optimal language, it\nremains the dominant choice in 58% of cases, and Rust is not used a single\ntime. These results indicate that LLMs may prioritise familiarity and\npopularity over suitability and task-specific optimality. This will introduce\nsecurity vulnerabilities and technical debt, and limit exposure to newly\ndeveloped, better-suited tools and languages. Understanding and addressing\nthese biases is essential for the responsible integration of LLMs into software\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to generate code,\ninfluencing users' choices of libraries and programming languages in critical\nreal-world projects. However, little is known about their systematic biases or\npreferences toward certain libraries and programming languages, which can\nsignificantly impact software development practices. To fill this gap, we\nperform the first empirical study of LLMs' preferences for libraries and\nprogramming languages when generating code, covering eight diverse LLMs. Our\nresults reveal that LLMs exhibit a strong tendency to overuse widely adopted\nlibraries such as NumPy; in up to 48% of cases, this usage is unnecessary and\ndeviates from the ground-truth solutions. LLMs also exhibit a significant\npreference toward Python as their default language. For high-performance\nproject initialisation tasks where Python is not the optimal language, it\nremains the dominant choice in 58% of cases, and Rust is not used a single\ntime. These results indicate that LLMs may prioritise familiarity and\npopularity over suitability and task-specific optimality. This will introduce\nsecurity vulnerabilities and technical debt, and limit exposure to newly\ndeveloped, better-suited tools and languages. Understanding and addressing\nthese biases is essential for the responsible integration of LLMs into software\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Lukas Twist"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Don Syme"
                    },
                    {
                        "name": "Joost Noppen"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Detlef Nauck"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Nauck"
                },
                "author": "Detlef Nauck",
                "arxiv_comment": "13 pages, 8 tables, 2 figures. Paper was previously titled \"LLMs Love\n  Python\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00091v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00091v4",
                "updated": "2025-07-21T12:45:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    45,
                    28,
                    0,
                    202,
                    0
                ],
                "published": "2025-04-30T18:02:45Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    2,
                    45,
                    2,
                    120,
                    0
                ],
                "title": "CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios"
                },
                "summary": "With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing methods, this paper\nproposes CoordField, a coordination field agent system for coordinating\nheterogeneous drone swarms in complex urban scenarios. In this system, large\nlanguage models (LLMs) is responsible for interpreting high-level human\ninstructions and converting them into executable commands for the UAV swarms,\nsuch as patrol and target tracking. Subsequently, a Coordination field\nmechanism is proposed to guide UAV motion and task selection, enabling\ndecentralized and adaptive allocation of emergent tasks. A total of 50 rounds\nof comparative testing were conducted across different models in a 2D\nsimulation space to evaluate their performance. Experimental results\ndemonstrate that the proposed system achieves superior performance in terms of\ntask coverage, response time, and adaptability to dynamic changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing methods, this paper\nproposes CoordField, a coordination field agent system for coordinating\nheterogeneous drone swarms in complex urban scenarios. In this system, large\nlanguage models (LLMs) is responsible for interpreting high-level human\ninstructions and converting them into executable commands for the UAV swarms,\nsuch as patrol and target tracking. Subsequently, a Coordination field\nmechanism is proposed to guide UAV motion and task selection, enabling\ndecentralized and adaptive allocation of emergent tasks. A total of 50 rounds\nof comparative testing were conducted across different models in a 2D\nsimulation space to evaluate their performance. Experimental results\ndemonstrate that the proposed system achieves superior performance in terms of\ntask coverage, response time, and adaptability to dynamic changes."
                },
                "authors": [
                    {
                        "name": "Tengchao Zhang"
                    },
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Patrik P. Sli"
                    },
                    {
                        "name": "Qinghua Ni"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00091v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00091v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19099v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19099v5",
                "updated": "2025-07-21T12:44:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    44,
                    10,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-25T11:28:34Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    28,
                    34,
                    6,
                    145,
                    0
                ],
                "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning"
                },
                "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."
                },
                "authors": [
                    {
                        "name": "Kun Xiang"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zirong Liu"
                    },
                    {
                        "name": "Peixin Qu"
                    },
                    {
                        "name": "Jixi He"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19099v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19099v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05445v2",
                "updated": "2025-07-21T12:42:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    42,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-05-08T17:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations"
                },
                "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15557v1",
                "updated": "2025-07-21T12:38:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    38,
                    7,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:38:07Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    38,
                    7,
                    0,
                    202,
                    0
                ],
                "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text\n  Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text\n  Detoxification"
                },
                "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case."
                },
                "authors": [
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15553v1",
                "updated": "2025-07-21T12:32:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    32,
                    30,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:32:30Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    32,
                    30,
                    0,
                    202,
                    0
                ],
                "title": "Efficient Routing of Inference Requests across LLM Instances in\n  Cloud-Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Routing of Inference Requests across LLM Instances in\n  Cloud-Edge Computing"
                },
                "summary": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments."
                },
                "authors": [
                    {
                        "name": "Shibo Yu"
                    },
                    {
                        "name": "Mohammad Goudarzi"
                    },
                    {
                        "name": "Adel Nadjaran Toosi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Nadjaran Toosi"
                },
                "author": "Adel Nadjaran Toosi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04510v2",
                "updated": "2025-07-21T12:31:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    31,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-08T13:56:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs.\nHowever, existing fine-tuning techniques often treat source code as plain text,\nlosing the graph-based structural information inherent in code.\n  Graph-enhanced soft prompt tuning addresses this by translating the\nstructural information into contextual cues that the LLM can understand.\nHowever, current methods are primarily designed for general graph-related tasks\nand focus more on adjacency information, they fall short in preserving the rich\nsemantic information (e.g., control/data flow) within code graphs. They also\nfail to ensure computational efficiency while capturing graph-text interactions\nin their cross-modal alignment module.\n  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection. CGP-Tuning introduces\ntype-aware embeddings to capture the rich semantic information within code\ngraphs, along with an efficient cross-modal alignment module that achieves\nlinear computational costs while incorporating graph-text interactions. It is\nevaluated on the latest DiverseVul dataset and three advanced open-source code\nLLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that\nCGP-Tuning delivers model-agnostic improvements and maintains practical\ninference speed, surpassing the best graph-enhanced soft prompt tuning baseline\nby an average of four percentage points and outperforming non-tuned zero-shot\nprompting by 15 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs.\nHowever, existing fine-tuning techniques often treat source code as plain text,\nlosing the graph-based structural information inherent in code.\n  Graph-enhanced soft prompt tuning addresses this by translating the\nstructural information into contextual cues that the LLM can understand.\nHowever, current methods are primarily designed for general graph-related tasks\nand focus more on adjacency information, they fall short in preserving the rich\nsemantic information (e.g., control/data flow) within code graphs. They also\nfail to ensure computational efficiency while capturing graph-text interactions\nin their cross-modal alignment module.\n  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection. CGP-Tuning introduces\ntype-aware embeddings to capture the rich semantic information within code\ngraphs, along with an efficient cross-modal alignment module that achieves\nlinear computational costs while incorporating graph-text interactions. It is\nevaluated on the latest DiverseVul dataset and three advanced open-source code\nLLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that\nCGP-Tuning delivers model-agnostic improvements and maintains practical\ninference speed, surpassing the best graph-enhanced soft prompt tuning baseline\nby an average of four percentage points and outperforming non-tuned zero-shot\nprompting by 15 percentage points."
                },
                "authors": [
                    {
                        "name": "Ruijun Feng"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Pietro Liguori"
                    },
                    {
                        "name": "Yulei Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yulei Sui"
                },
                "author": "Yulei Sui",
                "arxiv_comment": "Accepted by IEEE Transactions on Software Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15551v1",
                "updated": "2025-07-21T12:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:28:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders"
                },
                "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Zhifang Fan"
                    },
                    {
                        "name": "Xiaoxie Zhu"
                    },
                    {
                        "name": "Yuchen Jiang"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Haoran Ding"
                    },
                    {
                        "name": "Xinmin Wang"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Zhen Gong"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Qiwei Chen"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Zuotao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuotao Liu"
                },
                "author": "Zuotao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15550v1",
                "updated": "2025-07-21T12:28:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    10,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:28:10Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    28,
                    10,
                    0,
                    202,
                    0
                ],
                "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors"
                },
                "summary": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity."
                },
                "authors": [
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Piotr Pikos"
                    },
                    {
                        "name": "Mateusz Ostaszewski"
                    },
                    {
                        "name": "Firas Laakom"
                    },
                    {
                        "name": "Jrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Schmidhuber"
                },
                "author": "Jrgen Schmidhuber",
                "arxiv_comment": "31 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00409v3",
                "updated": "2025-07-21T12:20:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    20,
                    6,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-01T12:08:38Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    12,
                    8,
                    38,
                    5,
                    32,
                    0
                ],
                "title": "Doing More with Less: A Survey on Routing Strategies for Resource\n  Optimisation in Large Language Model-Based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doing More with Less: A Survey on Routing Strategies for Resource\n  Optimisation in Large Language Model-Based Systems"
                },
                "summary": "Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems."
                },
                "authors": [
                    {
                        "name": "Clovis Varangot-Reille"
                    },
                    {
                        "name": "Christophe Bouvard"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Franois Jacquenet"
                    }
                ],
                "author_detail": {
                    "name": "Franois Jacquenet"
                },
                "author": "Franois Jacquenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12829v2",
                "updated": "2025-07-21T12:15:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    46,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-18T12:48:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    48,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan"
                },
                "summary": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Akhmed Sakip"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Bekassyl Syzdykov"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15542v1",
                "updated": "2025-07-21T12:15:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    27,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T12:15:27Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    12,
                    15,
                    27,
                    0,
                    202,
                    0
                ],
                "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature\n  Adaptation"
                },
                "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00024v2",
                "updated": "2025-07-21T11:43:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    43,
                    19,
                    0,
                    202,
                    0
                ],
                "published": "2025-02-24T10:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    4,
                    44,
                    0,
                    55,
                    0
                ],
                "title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach\n  with LLM-based Manipulation Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach\n  with LLM-based Manipulation Checks"
                },
                "summary": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments."
                },
                "authors": [
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "ACL 2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15521v1",
                "updated": "2025-07-21T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    42,
                    3,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:42:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    42,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "LLM world models are mental: Output layer evidence of brittle world\n  model use in LLM mechanical reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM world models are mental: Output layer evidence of brittle world\n  model use in LLM mechanical reasoning"
                },
                "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Cole Robertson"
                    },
                    {
                        "name": "Philip Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Philip Wolff"
                },
                "author": "Philip Wolff",
                "arxiv_comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15518v1",
                "updated": "2025-07-21T11:36:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    36,
                    39,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:36:39Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    36,
                    39,
                    0,
                    202,
                    0
                ],
                "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics"
                },
                "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET."
                },
                "authors": [
                    {
                        "name": "Sizhou Chen"
                    },
                    {
                        "name": "Shufan Jiang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xiao-Lei Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15512v1",
                "updated": "2025-07-21T11:28:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    28,
                    9,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:28:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    28,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models"
                },
                "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Yonghao Shi"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15502v1",
                "updated": "2025-07-21T11:07:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:07:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "FollowUpBot: An LLM-Based Conversational Robot for Automatic\n  Postoperative Follow-up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FollowUpBot: An LLM-Based Conversational Robot for Automatic\n  Postoperative Follow-up"
                },
                "summary": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jianing Yin"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Mingjin Zhang"
                    },
                    {
                        "name": "Weixun Gao"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Haihua Shu"
                    }
                ],
                "author_detail": {
                    "name": "Haihua Shu"
                },
                "author": "Haihua Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15501v1",
                "updated": "2025-07-21T11:07:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T11:07:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    11,
                    7,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action\n  Execution"
                },
                "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation."
                },
                "authors": [
                    {
                        "name": "Alexandru Coca"
                    },
                    {
                        "name": "Mark Gaynor"
                    },
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Jianpeng Cheng"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Pete Boothroyd"
                    },
                    {
                        "name": "Hctor Martinez Alonso"
                    },
                    {
                        "name": "Diarmuid  Saghdha"
                    },
                    {
                        "name": "Anders Johannsen"
                    }
                ],
                "author_detail": {
                    "name": "Anders Johannsen"
                },
                "author": "Anders Johannsen",
                "arxiv_comment": "37 pages, 22 figures. To appear at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09493v3",
                "updated": "2025-07-21T10:23:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    23,
                    44,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-16T12:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating Conversational Recommender Systems via Large Language Models:\n  A User-Centric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Conversational Recommender Systems via Large Language Models:\n  A User-Centric Framework"
                },
                "summary": "Conversational recommender systems (CRSs) integrate both recommendation and\ndialogue tasks, making their evaluation uniquely challenging. Existing\napproaches primarily assess CRS performance by separately evaluating item\nrecommendation and dialogue management using rule-based metrics. However, these\nmethods fail to capture the real human experience, and they cannot draw direct\nconclusions about the system's overall performance. As conversational\nrecommender systems become increasingly vital in e-commerce, social media, and\ncustomer support, the ability to evaluate both recommendation accuracy and\ndialogue management quality using a single metric, thereby authentically\nreflecting user experience, has become the principal challenge impeding\nprogress in this field.\n  In this work, we propose a user-centric evaluation framework based on large\nlanguage models (LLMs) for CRSs, namely Conversational Recommendation Evaluator\n(CoRE). CoRE consists of two main components: (1) LLM-As-Evaluator. Firstly, we\ncomprehensively summarize 12 key factors influencing user experience in CRSs\nand directly leverage LLM as an evaluator to assign a score to each factor. (2)\nMulti-Agent Debater. Secondly, we design a multi-agent debate framework with\nfour distinct roles (common user, domain expert, linguist, and HCI expert) to\ndiscuss and synthesize the 12 evaluation factors into a unified overall\nperformance score.\n  Furthermore, we apply the proposed framework to evaluate four CRSs on two\nbenchmark datasets. The experimental results show that CoRE aligns well with\nhuman evaluation in most of the 12 factors and the overall assessment.\nEspecially, CoRE's overall evaluation scores demonstrate significantly better\nalignment with human feedback compared to existing rule-based metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems (CRSs) integrate both recommendation and\ndialogue tasks, making their evaluation uniquely challenging. Existing\napproaches primarily assess CRS performance by separately evaluating item\nrecommendation and dialogue management using rule-based metrics. However, these\nmethods fail to capture the real human experience, and they cannot draw direct\nconclusions about the system's overall performance. As conversational\nrecommender systems become increasingly vital in e-commerce, social media, and\ncustomer support, the ability to evaluate both recommendation accuracy and\ndialogue management quality using a single metric, thereby authentically\nreflecting user experience, has become the principal challenge impeding\nprogress in this field.\n  In this work, we propose a user-centric evaluation framework based on large\nlanguage models (LLMs) for CRSs, namely Conversational Recommendation Evaluator\n(CoRE). CoRE consists of two main components: (1) LLM-As-Evaluator. Firstly, we\ncomprehensively summarize 12 key factors influencing user experience in CRSs\nand directly leverage LLM as an evaluator to assign a score to each factor. (2)\nMulti-Agent Debater. Secondly, we design a multi-agent debate framework with\nfour distinct roles (common user, domain expert, linguist, and HCI expert) to\ndiscuss and synthesize the 12 evaluation factors into a unified overall\nperformance score.\n  Furthermore, we apply the proposed framework to evaluate four CRSs on two\nbenchmark datasets. The experimental results show that CoRE aligns well with\nhuman evaluation in most of the 12 factors and the overall assessment.\nEspecially, CoRE's overall evaluation scores demonstrate significantly better\nalignment with human feedback compared to existing rule-based metrics."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Qinglin Jia"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15470v2",
                "updated": "2025-07-22T06:55:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    6,
                    55,
                    39,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-21T10:21:48Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    21,
                    48,
                    0,
                    202,
                    0
                ],
                "title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated\n  Learning"
                },
                "summary": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings."
                },
                "authors": [
                    {
                        "name": "Baran Can Gl"
                    },
                    {
                        "name": "Suraksha Nadig"
                    },
                    {
                        "name": "Stefanos Tziampazis"
                    },
                    {
                        "name": "Nasser Jazdi"
                    },
                    {
                        "name": "Michael Weyrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Weyrich"
                },
                "author": "Michael Weyrich",
                "arxiv_comment": "Preprint version. Accepted for publication at IEEE ICECCME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15465v1",
                "updated": "2025-07-21T10:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    18,
                    33,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T10:18:33Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    10,
                    18,
                    33,
                    0,
                    202,
                    0
                ],
                "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts"
                },
                "summary": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models."
                },
                "authors": [
                    {
                        "name": "Sungmin Yun"
                    },
                    {
                        "name": "Seonyong Park"
                    },
                    {
                        "name": "Hwayong Nam"
                    },
                    {
                        "name": "Younjoo Lee"
                    },
                    {
                        "name": "Gunjun Lee"
                    },
                    {
                        "name": "Kwanhee Kyung"
                    },
                    {
                        "name": "Sangpyo Kim"
                    },
                    {
                        "name": "Nam Sung Kim"
                    },
                    {
                        "name": "Jongmin Kim"
                    },
                    {
                        "name": "Hyungyo Kim"
                    },
                    {
                        "name": "Juhwan Cho"
                    },
                    {
                        "name": "Seungmin Baek"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15428v1",
                "updated": "2025-07-21T09:27:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    27,
                    45,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T09:27:45Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    27,
                    45,
                    0,
                    202,
                    0
                ],
                "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in\n  Embodied Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in\n  Embodied Agent"
                },
                "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning."
                },
                "authors": [
                    {
                        "name": "Jiaao Li"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15423v1",
                "updated": "2025-07-21T09:24:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    24,
                    13,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T09:24:13Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    24,
                    13,
                    0,
                    202,
                    0
                ],
                "title": "Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations"
                },
                "summary": "In the evolution towards 6G user-centric networking, the moving network (MN)\nparadigm can play an important role. In a MN, some small cell base stations\n(BS) are installed on top of vehicles, and enable a more dynamic, flexible and\nsustainable, network operation. By \"following\" the users movements and adapting\ndynamically to their requests, the MN paradigm enables a more efficient\nutilization of network resources, mitigating the need for dense small cell BS\ndeployments at the cost of an increase in resource utilization due to wireless\nbackhauling. This aspect is at least partly compensated by the shorter distance\nbetween users and BS, which allows for lower power and Line-of-Sight\ncommunications. While the MN paradigm has been investigated for some time, to\ndate, it is still unclear in which conditions the advantages of MN outweigh the\nadditional resource costs. In this paper, we propose a stochastic geometry\nframework for the characterization of the potential benefits of the MN paradigm\nas part of an HetNet in urban settings. Our approach allows the estimation of\nuser-perceived performance, accounting for wireless backhaul connectivity as\nwell as base station resource scheduling. We formulate an optimization problem\nfor determining the resource-optimal network configurations and BS scheduling\nwhich minimize the overall amount of deployed BSs in a QoS-aware manner, and\nthe minimum vehicular flow between different urban districts required to\nsupport them, and we propose an efficient stochastic heuristic to solve it. Our\nnumerical assessment suggests that the MN paradigm, coupled with appropriate\ndynamic network management strategies, significantly reduces the amount of\ndeployed network infrastructure while guaranteeing the target QoS perceived by\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolution towards 6G user-centric networking, the moving network (MN)\nparadigm can play an important role. In a MN, some small cell base stations\n(BS) are installed on top of vehicles, and enable a more dynamic, flexible and\nsustainable, network operation. By \"following\" the users movements and adapting\ndynamically to their requests, the MN paradigm enables a more efficient\nutilization of network resources, mitigating the need for dense small cell BS\ndeployments at the cost of an increase in resource utilization due to wireless\nbackhauling. This aspect is at least partly compensated by the shorter distance\nbetween users and BS, which allows for lower power and Line-of-Sight\ncommunications. While the MN paradigm has been investigated for some time, to\ndate, it is still unclear in which conditions the advantages of MN outweigh the\nadditional resource costs. In this paper, we propose a stochastic geometry\nframework for the characterization of the potential benefits of the MN paradigm\nas part of an HetNet in urban settings. Our approach allows the estimation of\nuser-perceived performance, accounting for wireless backhaul connectivity as\nwell as base station resource scheduling. We formulate an optimization problem\nfor determining the resource-optimal network configurations and BS scheduling\nwhich minimize the overall amount of deployed BSs in a QoS-aware manner, and\nthe minimum vehicular flow between different urban districts required to\nsupport them, and we propose an efficient stochastic heuristic to solve it. Our\nnumerical assessment suggests that the MN paradigm, coupled with appropriate\ndynamic network management strategies, significantly reduces the amount of\ndeployed network infrastructure while guaranteeing the target QoS perceived by\nusers."
                },
                "authors": [
                    {
                        "name": "Laura Finarelli"
                    },
                    {
                        "name": "Falko Dressler"
                    },
                    {
                        "name": "Marco Ajmone Marsan"
                    },
                    {
                        "name": "Gianluca Rizzo"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Rizzo"
                },
                "author": "Gianluca Rizzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15419v1",
                "updated": "2025-07-21T09:20:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    20,
                    43,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T09:20:43Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    20,
                    43,
                    0,
                    202,
                    0
                ],
                "title": "PhishIntentionLLM: Uncovering Phishing Website Intentions through\n  Multi-Agent Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntentionLLM: Uncovering Phishing Website Intentions through\n  Multi-Agent Retrieval-Augmented Generation"
                },
                "summary": "Phishing websites remain a major cybersecurity threat, yet existing methods\nprimarily focus on detection, while the recognition of underlying malicious\nintentions remains largely unexplored. To address this gap, we propose\nPhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework\nthat uncovers phishing intentions from website screenshots. Leveraging the\nvisual-language capabilities of large language models (LLMs), our framework\nidentifies four key phishing objectives: Credential Theft, Financial Fraud,\nMalware Distribution, and Personal Information Harvesting. We construct and\nrelease the first phishing intention ground truth dataset (~2K samples) and\nevaluate the framework using four commercial LLMs. Experimental results show\nthat PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and\nsignificantly outperforms the single-agent baseline with a ~95% improvement in\nmicro-precision. Compared to the previous work, it achieves 0.8545 precision\nfor credential theft, marking a ~4% improvement. Additionally, we generate a\nlarger dataset of ~9K samples for large-scale phishing intention profiling\nacross sectors. This work provides a scalable and interpretable solution for\nintention-aware phishing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing websites remain a major cybersecurity threat, yet existing methods\nprimarily focus on detection, while the recognition of underlying malicious\nintentions remains largely unexplored. To address this gap, we propose\nPhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework\nthat uncovers phishing intentions from website screenshots. Leveraging the\nvisual-language capabilities of large language models (LLMs), our framework\nidentifies four key phishing objectives: Credential Theft, Financial Fraud,\nMalware Distribution, and Personal Information Harvesting. We construct and\nrelease the first phishing intention ground truth dataset (~2K samples) and\nevaluate the framework using four commercial LLMs. Experimental results show\nthat PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and\nsignificantly outperforms the single-agent baseline with a ~95% improvement in\nmicro-precision. Compared to the previous work, it achieves 0.8545 precision\nfor credential theft, marking a ~4% improvement. Additionally, we generate a\nlarger dataset of ~9K samples for large-scale phishing intention profiling\nacross sectors. This work provides a scalable and interpretable solution for\nintention-aware phishing analysis."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Selvakumar Manickam"
                    },
                    {
                        "name": "Yung-wey Chong"
                    },
                    {
                        "name": "Shankar Karuppayah"
                    }
                ],
                "author_detail": {
                    "name": "Shankar Karuppayah"
                },
                "author": "Shankar Karuppayah",
                "arxiv_comment": "Accepted by EAI ICDF2C 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06211v2",
                "updated": "2025-07-21T09:16:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    16,
                    12,
                    0,
                    202,
                    0
                ],
                "published": "2024-09-10T04:34:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    34,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning"
                },
                "summary": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available."
                },
                "authors": [
                    {
                        "name": "Jaeseong Lee"
                    },
                    {
                        "name": "seung-won hwang"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Daniel F Campos"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13508v2",
                "updated": "2025-07-21T09:07:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    9,
                    7,
                    17,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-17T19:35:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    19,
                    35,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "Fake or Real: The Impostor Hunt in Texts for Space Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake or Real: The Impostor Hunt in Texts for Space Operations"
                },
                "summary": "The \"Fake or Real\" competition hosted on Kaggle\n(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the\nsecond part of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/ ). The competition idea is\nbased on two real-life AI security threats identified within the project --\ndata poisoning and overreliance in Large Language Models. The task is to\ndistinguish between the proper output from LLM and the output generated under\nmalicious modification of the LLM. As this problem was not extensively\nresearched, participants are required to develop new techniques to address this\nissue or adjust already existing ones to this problem's statement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Fake or Real\" competition hosted on Kaggle\n(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the\nsecond part of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/ ). The competition idea is\nbased on two real-life AI security threats identified within the project --\ndata poisoning and overreliance in Large Language Models. The task is to\ndistinguish between the proper output from LLM and the output generated under\nmalicious modification of the LLM. As this problem was not extensively\nresearched, participants are required to develop new techniques to address this\nissue or adjust already existing ones to this problem's statement."
                },
                "authors": [
                    {
                        "name": "Agata Kaczmarek"
                    },
                    {
                        "name": "Dawid Pudowski"
                    },
                    {
                        "name": "Piotr Wilczyski"
                    },
                    {
                        "name": "Przemysaw Biecek"
                    },
                    {
                        "name": "Krzysztof Kotowski"
                    },
                    {
                        "name": "Ramez Shendy"
                    },
                    {
                        "name": "Jakub Nalepa"
                    },
                    {
                        "name": "Artur Janicki"
                    },
                    {
                        "name": "Evridiki Ntagiou"
                    }
                ],
                "author_detail": {
                    "name": "Evridiki Ntagiou"
                },
                "author": "Evridiki Ntagiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15396v1",
                "updated": "2025-07-21T08:58:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    58,
                    31,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:58:31Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    58,
                    31,
                    0,
                    202,
                    0
                ],
                "title": "Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation"
                },
                "summary": "Hearing loss simulation models are essential for hearing aid deployment.\nHowever, existing models have high computational complexity and latency, which\nlimits real-time applications and lack direct integration with speech\nprocessing systems. To address these issues, we propose Neuro-MSBG, a\nlightweight end-to-end model with a personalized audiogram encoder for\neffective time-frequency modeling. Experiments show that Neuro-MSBG supports\nparallel inference and retains the intelligibility and perceptual quality of\nthe original MSBG, with a Spearman's rank correlation coefficient (SRCC) of\n0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for\nPerceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation\nruntime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second\ninput), further demonstrating its efficiency and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hearing loss simulation models are essential for hearing aid deployment.\nHowever, existing models have high computational complexity and latency, which\nlimits real-time applications and lack direct integration with speech\nprocessing systems. To address these issues, we propose Neuro-MSBG, a\nlightweight end-to-end model with a personalized audiogram encoder for\neffective time-frequency modeling. Experiments show that Neuro-MSBG supports\nparallel inference and retains the intelligibility and perceptual quality of\nthe original MSBG, with a Spearman's rank correlation coefficient (SRCC) of\n0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for\nPerceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation\nruntime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second\ninput), further demonstrating its efficiency and practicality."
                },
                "authors": [
                    {
                        "name": "Hui-Guan Yuan"
                    },
                    {
                        "name": "Ryandhimas E. Zezario"
                    },
                    {
                        "name": "Shafique Ahmed"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    },
                    {
                        "name": "Kai-Lung Hua"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15393v1",
                "updated": "2025-07-21T08:53:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    53,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:53:41Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    53,
                    41,
                    0,
                    202,
                    0
                ],
                "title": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails\n  with Knowledge Base Invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails\n  with Knowledge Base Invariants"
                },
                "summary": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Ruofan Liu"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Silas Yeo Shuen Yu"
                    },
                    {
                        "name": "Xiwen Teoh"
                    },
                    {
                        "name": "Zhenkai Liang"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20090v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20090v5",
                "updated": "2025-07-21T08:41:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    41,
                    47,
                    0,
                    202,
                    0
                ],
                "published": "2024-05-30T14:27:20Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    14,
                    27,
                    20,
                    3,
                    151,
                    0
                ],
                "title": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection."
                },
                "authors": [
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Jiayan Yang"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Yichi Wang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "arxiv_comment": "This paper is accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20090v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20090v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15385v1",
                "updated": "2025-07-21T08:41:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    41,
                    5,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:41:05Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    41,
                    5,
                    0,
                    202,
                    0
                ],
                "title": "Transformer-based Deep Learning Model for Joint Routing and Scheduling\n  with Varying Electric Vehicle Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Deep Learning Model for Joint Routing and Scheduling\n  with Varying Electric Vehicle Numbers"
                },
                "summary": "The growing integration of renewable energy sources in modern power systems\nhas introduced significant operational challenges due to their intermittent and\nuncertain outputs. In recent years, mobile energy storage systems (ESSs) have\nemerged as a popular flexible resource for mitigating these challenges.\nCompared to stationary ESSs, mobile ESSs offer additional spatial flexibility,\nenabling cost-effective energy delivery through the transportation network.\nHowever, the widespread deployment of mobile ESSs is often hindered by the high\ninvestment cost, which has motivated researchers to investigate utilising more\nreadily available alternatives, such as electric vehicles (EVs) as mobile\nenergy storage units instead. Hence, we explore this opportunity with a\nMIP-based day-ahead electric vehicle joint routing and scheduling problem in\nthis work. However, solving the problem in a practical setting can often be\ncomputationally intractable since the existence of binary variables makes it\ncombinatorial challenging. Therefore, we proposed to simplify the problem's\nsolution process for a MIP solver by pruning the solution search space with a\ntransformer-based deep learning (DL) model. This is done by training the model\nto rapidly predict the optimal binary solutions. In addition, unlike many\nexisting DL approaches that assume fixed problem structures, the proposed model\nis designed to accommodate problems with EV fleets of any sizes. This\nflexibility is essential since frequent re-training can introduce significant\ncomputational overhead. We evaluated the approach with simulations on the IEEE\n33-bus system coupled with the Nguyen-Dupuis transportation network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing integration of renewable energy sources in modern power systems\nhas introduced significant operational challenges due to their intermittent and\nuncertain outputs. In recent years, mobile energy storage systems (ESSs) have\nemerged as a popular flexible resource for mitigating these challenges.\nCompared to stationary ESSs, mobile ESSs offer additional spatial flexibility,\nenabling cost-effective energy delivery through the transportation network.\nHowever, the widespread deployment of mobile ESSs is often hindered by the high\ninvestment cost, which has motivated researchers to investigate utilising more\nreadily available alternatives, such as electric vehicles (EVs) as mobile\nenergy storage units instead. Hence, we explore this opportunity with a\nMIP-based day-ahead electric vehicle joint routing and scheduling problem in\nthis work. However, solving the problem in a practical setting can often be\ncomputationally intractable since the existence of binary variables makes it\ncombinatorial challenging. Therefore, we proposed to simplify the problem's\nsolution process for a MIP solver by pruning the solution search space with a\ntransformer-based deep learning (DL) model. This is done by training the model\nto rapidly predict the optimal binary solutions. In addition, unlike many\nexisting DL approaches that assume fixed problem structures, the proposed model\nis designed to accommodate problems with EV fleets of any sizes. This\nflexibility is essential since frequent re-training can introduce significant\ncomputational overhead. We evaluated the approach with simulations on the IEEE\n33-bus system coupled with the Nguyen-Dupuis transportation network."
                },
                "authors": [
                    {
                        "name": "Jun Kang Yap"
                    },
                    {
                        "name": "Vishnu Monn Baskaran"
                    },
                    {
                        "name": "Wen Shan Tan"
                    },
                    {
                        "name": "Ze Yang Ding"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "David L. Dowe"
                    }
                ],
                "author_detail": {
                    "name": "David L. Dowe"
                },
                "author": "David L. Dowe",
                "arxiv_comment": "Accepted at Industry Applications Society Annual Meeting (IAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19463v2",
                "updated": "2025-07-21T08:38:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    38,
                    53,
                    0,
                    202,
                    0
                ],
                "published": "2024-11-29T04:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    25,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Understanding the Design Decisions of Retrieval-Augmented Generation\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Design Decisions of Retrieval-Augmented Generation\n  Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a critical technique for\nenhancing large language model (LLM) capabilities. However, practitioners face\nsignificant challenges when making RAG deployment decisions. While existing\nresearch prioritizes algorithmic innovations, a systematic gap persists in\nunderstanding fundamental engineering trade-offs that determine RAG success. We\npresent the first comprehensive study of three universal RAG deployment\ndecisions: whether to deploy RAG, how much information to retrieve, and how to\nintegrate retrieved knowledge effectively. Through systematic experiments\nacross three LLMs and six datasets spanning question answering and code\ngeneration tasks, we reveal critical insights: (1) RAG deployment must be\nhighly selective, with variable recall thresholds and failure modes affecting\nup to 12.6\\% of samples even with perfect documents. (2) Optimal retrieval\nvolume exhibits task-dependent behavior QA tasks show universal patterns (5-10\ndocuments optimal) while code generation requires scenario-specific\noptimization. (3) Knowledge integration effectiveness depends on task and model\ncharacteristics, with code generation benefiting significantly from prompting\nmethods while question answering shows minimal improvement. These findings\ndemonstrate that universal RAG strategies prove inadequate. Effective RAG\nsystems require context-aware design decisions based on task characteristics\nand model capabilities. Our analysis provides evidence-based guidance for\npractitioners and establishes foundational insights for principled RAG\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a critical technique for\nenhancing large language model (LLM) capabilities. However, practitioners face\nsignificant challenges when making RAG deployment decisions. While existing\nresearch prioritizes algorithmic innovations, a systematic gap persists in\nunderstanding fundamental engineering trade-offs that determine RAG success. We\npresent the first comprehensive study of three universal RAG deployment\ndecisions: whether to deploy RAG, how much information to retrieve, and how to\nintegrate retrieved knowledge effectively. Through systematic experiments\nacross three LLMs and six datasets spanning question answering and code\ngeneration tasks, we reveal critical insights: (1) RAG deployment must be\nhighly selective, with variable recall thresholds and failure modes affecting\nup to 12.6\\% of samples even with perfect documents. (2) Optimal retrieval\nvolume exhibits task-dependent behavior QA tasks show universal patterns (5-10\ndocuments optimal) while code generation requires scenario-specific\noptimization. (3) Knowledge integration effectiveness depends on task and model\ncharacteristics, with code generation benefiting significantly from prompting\nmethods while question answering shows minimal improvement. These findings\ndemonstrate that universal RAG strategies prove inadequate. Effective RAG\nsystems require context-aware design decisions based on task characteristics\nand model capabilities. Our analysis provides evidence-based guidance for\npractitioners and establishes foundational insights for principled RAG\ndeployment."
                },
                "authors": [
                    {
                        "name": "Shengming Zhao"
                    },
                    {
                        "name": "Yuchen Shao"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Jiayang Song"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15378v1",
                "updated": "2025-07-21T08:34:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    34,
                    20,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:34:20Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    34,
                    20,
                    0,
                    202,
                    0
                ],
                "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for\n  Competitive Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlgoSimBench: Identifying Algorithmically Similar Problems for\n  Competitive Programming"
                },
                "summary": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com"
                },
                "authors": [
                    {
                        "name": "Jierui Li"
                    },
                    {
                        "name": "Raymond Mooney"
                    }
                ],
                "author_detail": {
                    "name": "Raymond Mooney"
                },
                "author": "Raymond Mooney",
                "arxiv_comment": "19 pages, pre-print only",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06717v2",
                "updated": "2025-07-21T08:23:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    23,
                    7,
                    0,
                    202,
                    0
                ],
                "published": "2024-08-13T08:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    22,
                    1,
                    1,
                    226,
                    0
                ],
                "title": "Proficient Graph Neural Network Design by Accumulating Knowledge on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proficient Graph Neural Network Design by Accumulating Knowledge on\n  Large Language Models"
                },
                "summary": "High-level automation is increasingly critical in AI, driven by rapid\nadvances in large language models (LLMs) and AI agents. However, LLMs, despite\ntheir general reasoning power, struggle significantly in specialized,\ndata-sensitive tasks such as designing Graph Neural Networks (GNNs). This\ndifficulty arises from (1) the inherent knowledge gaps in modeling the\nintricate, varying relationships between graph properties and suitable\narchitectures and (2) the external noise from misleading descriptive inputs,\noften resulting in generic or even misleading model suggestions. Achieving\nproficiency in designing data-aware models -- defined as the meta-level\ncapability to systematically accumulate, interpret, and apply data-specific\ndesign knowledge -- remains challenging for existing automated approaches, due\nto their inefficient construction and application of meta-knowledge. To achieve\nthe meta-level proficiency, we propose DesiGNN, a knowledge-centered framework\nthat systematically converts past model design experiences into structured,\nfine-grained knowledge priors well fitted to meta-learning with LLMs. To\naccount for the inherent variability and external noise, DesiGNN aligns\nempirical property filtering from extensive benchmarks with adaptive\nelicitation of literature insights via LLMs. By constructing a solid\nmeta-knowledge between unseen graph understanding and known effective\narchitecture patterns, DesiGNN can deliver top-5.77% initial model proposals\nfor unseen datasets within seconds, and achieve consistently superior\nperformance with minimal search costs against baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level automation is increasingly critical in AI, driven by rapid\nadvances in large language models (LLMs) and AI agents. However, LLMs, despite\ntheir general reasoning power, struggle significantly in specialized,\ndata-sensitive tasks such as designing Graph Neural Networks (GNNs). This\ndifficulty arises from (1) the inherent knowledge gaps in modeling the\nintricate, varying relationships between graph properties and suitable\narchitectures and (2) the external noise from misleading descriptive inputs,\noften resulting in generic or even misleading model suggestions. Achieving\nproficiency in designing data-aware models -- defined as the meta-level\ncapability to systematically accumulate, interpret, and apply data-specific\ndesign knowledge -- remains challenging for existing automated approaches, due\nto their inefficient construction and application of meta-knowledge. To achieve\nthe meta-level proficiency, we propose DesiGNN, a knowledge-centered framework\nthat systematically converts past model design experiences into structured,\nfine-grained knowledge priors well fitted to meta-learning with LLMs. To\naccount for the inherent variability and external noise, DesiGNN aligns\nempirical property filtering from extensive benchmarks with adaptive\nelicitation of literature insights via LLMs. By constructing a solid\nmeta-knowledge between unseen graph understanding and known effective\narchitecture patterns, DesiGNN can deliver top-5.77% initial model proposals\nfor unseen datasets within seconds, and achieve consistently superior\nperformance with minimal search costs against baselines."
                },
                "authors": [
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Hanmo Liu"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Zhili Wang"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15361v1",
                "updated": "2025-07-21T08:15:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    15,
                    17,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:15:17Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    15,
                    17,
                    0,
                    202,
                    0
                ],
                "title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion\n  Biomedical Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion\n  Biomedical Segmentation"
                },
                "summary": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings."
                },
                "authors": [
                    {
                        "name": "Muhammad Aqeel"
                    },
                    {
                        "name": "Maham Nazir"
                    },
                    {
                        "name": "Zanxi Ruan"
                    },
                    {
                        "name": "Francesco Setti"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Setti"
                },
                "author": "Francesco Setti",
                "arxiv_comment": "Accepted to CVGMMI Workshop at ICIAP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07053v3",
                "updated": "2025-07-21T08:12:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    12,
                    47,
                    0,
                    202,
                    0
                ],
                "published": "2024-04-10T14:44:48Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    44,
                    48,
                    2,
                    101,
                    0
                ],
                "title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and\n  Interpretation"
                },
                "summary": "Metaphors are a ubiquitous but often overlooked part of everyday language. As\na complex cognitive-linguistic phenomenon, they provide a valuable means to\nevaluate whether language models can capture deeper aspects of meaning,\nincluding semantic, pragmatic, and cultural context. In this work, we present\nMeta4XNLI, the first parallel dataset for Natural Language Inference (NLI)\nnewly annotated for metaphor detection and interpretation in both English and\nSpanish. Meta4XNLI facilitates the comparison of encoder- and decoder-based\nmodels in detecting and understanding metaphorical language in multilingual and\ncross-lingual settings. Our results show that fine-tuned encoders outperform\ndecoders-only LLMs in metaphor detection. Metaphor interpretation is evaluated\nvia the NLI framework with comparable performance of masked and autoregressive\nmodels, which notably decreases when the inference is affected by metaphorical\nlanguage. Our study also finds that translation plays an important role in the\npreservation or loss of metaphors across languages, introducing shifts that\nmight impact metaphor occurrence and model performance. These findings\nunderscore the importance of resources like Meta4XNLI for advancing the\nanalysis of the capabilities of language models and improving our understanding\nof metaphor processing across languages. Furthermore, the dataset offers\npreviously unavailable opportunities to investigate metaphor interpretation,\ncross-lingual metaphor transferability, and the impact of translation on the\ndevelopment of multilingual annotated resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphors are a ubiquitous but often overlooked part of everyday language. As\na complex cognitive-linguistic phenomenon, they provide a valuable means to\nevaluate whether language models can capture deeper aspects of meaning,\nincluding semantic, pragmatic, and cultural context. In this work, we present\nMeta4XNLI, the first parallel dataset for Natural Language Inference (NLI)\nnewly annotated for metaphor detection and interpretation in both English and\nSpanish. Meta4XNLI facilitates the comparison of encoder- and decoder-based\nmodels in detecting and understanding metaphorical language in multilingual and\ncross-lingual settings. Our results show that fine-tuned encoders outperform\ndecoders-only LLMs in metaphor detection. Metaphor interpretation is evaluated\nvia the NLI framework with comparable performance of masked and autoregressive\nmodels, which notably decreases when the inference is affected by metaphorical\nlanguage. Our study also finds that translation plays an important role in the\npreservation or loss of metaphors across languages, introducing shifts that\nmight impact metaphor occurrence and model performance. These findings\nunderscore the importance of resources like Meta4XNLI for advancing the\nanalysis of the capabilities of language models and improving our understanding\nof metaphor processing across languages. Furthermore, the dataset offers\npreviously unavailable opportunities to investigate metaphor interpretation,\ncross-lingual metaphor transferability, and the impact of translation on the\ndevelopment of multilingual annotated resources."
                },
                "authors": [
                    {
                        "name": "Elisa Sanchez-Bayona"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15357v1",
                "updated": "2025-07-21T08:09:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    9,
                    11,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:09:11Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    9,
                    11,
                    0,
                    202,
                    0
                ],
                "title": "Metaphor and Large Language Models: When Surface Features Matter More\n  than Deep Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor and Large Language Models: When Surface Features Matter More\n  than Deep Understanding"
                },
                "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Elisa Sanchez-Bayona"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03040v2",
                "updated": "2025-07-21T08:01:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    1,
                    59,
                    0,
                    202,
                    0
                ],
                "published": "2025-01-06T14:27:41Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    27,
                    41,
                    0,
                    6,
                    0
                ],
                "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense."
                },
                "authors": [
                    {
                        "name": "Duygu Sezen Islakoglu"
                    },
                    {
                        "name": "Jan-Christoph Kalo"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christoph Kalo"
                },
                "author": "Jan-Christoph Kalo",
                "arxiv_comment": "Accepted to ACL 2025. Results on a larger test set. 13 pages, 2\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15349v1",
                "updated": "2025-07-21T08:01:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    1,
                    43,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:01:43Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    1,
                    43,
                    0,
                    202,
                    0
                ],
                "title": "Scaling Decentralized Learning with FLock",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Decentralized Learning with FLock"
                },
                "summary": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data."
                },
                "authors": [
                    {
                        "name": "Zehua Cheng"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15346v1",
                "updated": "2025-07-21T08:01:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    1,
                    8,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T08:01:08Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    8,
                    1,
                    8,
                    0,
                    202,
                    0
                ],
                "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection"
                },
                "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection."
                },
                "authors": [
                    {
                        "name": "Muhammad Aqeel"
                    },
                    {
                        "name": "Kidus Dagnaw Bellete"
                    },
                    {
                        "name": "Francesco Setti"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Setti"
                },
                "author": "Francesco Setti",
                "arxiv_comment": "Accepted to ICIAP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15343v1",
                "updated": "2025-07-21T07:58:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    58,
                    3,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T07:58:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    58,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StackTrans: From Large Language Model to Large Pushdown Automata Model"
                },
                "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability."
                },
                "authors": [
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Huangzhao Zhang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "currently under development",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15340v1",
                "updated": "2025-07-21T07:53:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    53,
                    49,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-21T07:53:49Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    53,
                    49,
                    0,
                    202,
                    0
                ],
                "title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT\n  Segmentation, Radiomics, Classification, and Prognosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT\n  Segmentation, Radiomics, Classification, and Prognosis"
                },
                "summary": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows."
                },
                "authors": [
                    {
                        "name": "Marc Boubnovski Martell"
                    },
                    {
                        "name": "Kristofer Linton-Reid"
                    },
                    {
                        "name": "Mitchell Chen"
                    },
                    {
                        "name": "Sumeet Hindocha"
                    },
                    {
                        "name": "Benjamin Hunter"
                    },
                    {
                        "name": "Marco A. Calzado"
                    },
                    {
                        "name": "Richard Lee"
                    },
                    {
                        "name": "Joram M. Posma"
                    },
                    {
                        "name": "Eric O. Aboagye"
                    }
                ],
                "author_detail": {
                    "name": "Eric O. Aboagye"
                },
                "author": "Eric O. Aboagye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]